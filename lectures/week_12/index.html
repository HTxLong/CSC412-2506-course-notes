



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="https://johngiorgi.github.io/CSC412-2506-course-notes/. /lectures/week_12/">
      
      
        <meta name="author" content="John Giorgi">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.2.0">
    
    
      
        <title>Week 12 - CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/application.572ca0f0.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../../assets/javascripts/modernizr.8c900955.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="../../styles/extra.css">
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#week-12-general-adversarial-networks-gans" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://johngiorgi.github.io/CSC412-2506-course-notes/. " title="CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning
              </span>
              <span class="md-header-nav__topic">
                Week 12
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      JohnGiorgi/CSC412-2506-course-notes
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://johngiorgi.github.io/CSC412-2506-course-notes/. " title="CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      JohnGiorgi/CSC412-2506-course-notes
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../.." title="About" class="md-nav__link">
      About
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      Lectures
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Lectures
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../week_1/" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_2/" title="Week 2" class="md-nav__link">
      Week 2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_3/" title="Week 3" class="md-nav__link">
      Week 3
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_4/" title="Week 4" class="md-nav__link">
      Week 4
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_5/" title="Week 5" class="md-nav__link">
      Week 5
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_6/" title="Week 6" class="md-nav__link">
      Week 6
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_8/" title="Week 8" class="md-nav__link">
      Week 8
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_9/" title="Week 9" class="md-nav__link">
      Week 9
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_10/" title="Week 10" class="md-nav__link">
      Week 10
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_11/" title="Week 11" class="md-nav__link">
      Week 11
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Week 12
      </label>
    
    <a href="./" title="Week 12" class="md-nav__link md-nav__link--active">
      Week 12
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#assigned-reading" title="Assigned Reading" class="md-nav__link">
    Assigned Reading
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generative-models-a-review" title="Generative Models, a Review" class="md-nav__link">
    Generative Models, a Review
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#prototypical-generative-model" title="Prototypical Generative Model" class="md-nav__link">
    Prototypical Generative Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maximum-likelihood-estimate" title="Maximum Likelihood Estimate" class="md-nav__link">
    Maximum Likelihood Estimate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#explicit-density-models" title="Explicit Density Models" class="md-nav__link">
    Explicit Density Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generative-model-goal" title="Generative Model Goal" class="md-nav__link">
    Generative Model Goal
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#implicit-density-models" title="Implicit Density Models" class="md-nav__link">
    Implicit Density Models
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#general-adversarial-network-gan-approach" title="General Adversarial Network (GAN) Approach" class="md-nav__link">
    General Adversarial Network (GAN) Approach
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#adversarial-game-the-specifics" title="Adversarial Game, the Specifics" class="md-nav__link">
    Adversarial Game, the Specifics
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#training-procedure" title="Training Procedure" class="md-nav__link">
    Training Procedure
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cost-functions" title="Cost Functions" class="md-nav__link">
    Cost Functions
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-discriminators-cost" title="The Discriminator’s Cost" class="md-nav__link">
    The Discriminator’s Cost
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#optimal-discriminator-strategy" title="Optimal Discriminator Strategy" class="md-nav__link">
    Optimal Discriminator Strategy
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimal-generator-strategy" title="Optimal Generator Strategy" class="md-nav__link">
    Optimal Generator Strategy
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_13/" title="Week 13" class="md-nav__link">
      Week 13
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Tutorials
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Tutorials
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/week_1/" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../sample_midterm/" title="Sample Midterm" class="md-nav__link">
      Sample Midterm
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../sample_final/" title="Sample Final" class="md-nav__link">
      Sample Final
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#assigned-reading" title="Assigned Reading" class="md-nav__link">
    Assigned Reading
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generative-models-a-review" title="Generative Models, a Review" class="md-nav__link">
    Generative Models, a Review
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#prototypical-generative-model" title="Prototypical Generative Model" class="md-nav__link">
    Prototypical Generative Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maximum-likelihood-estimate" title="Maximum Likelihood Estimate" class="md-nav__link">
    Maximum Likelihood Estimate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#explicit-density-models" title="Explicit Density Models" class="md-nav__link">
    Explicit Density Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generative-model-goal" title="Generative Model Goal" class="md-nav__link">
    Generative Model Goal
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#implicit-density-models" title="Implicit Density Models" class="md-nav__link">
    Implicit Density Models
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#general-adversarial-network-gan-approach" title="General Adversarial Network (GAN) Approach" class="md-nav__link">
    General Adversarial Network (GAN) Approach
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#adversarial-game-the-specifics" title="Adversarial Game, the Specifics" class="md-nav__link">
    Adversarial Game, the Specifics
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#training-procedure" title="Training Procedure" class="md-nav__link">
    Training Procedure
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cost-functions" title="Cost Functions" class="md-nav__link">
    Cost Functions
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-discriminators-cost" title="The Discriminator’s Cost" class="md-nav__link">
    The Discriminator’s Cost
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#optimal-discriminator-strategy" title="Optimal Discriminator Strategy" class="md-nav__link">
    Optimal Discriminator Strategy
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimal-generator-strategy" title="Optimal Generator Strategy" class="md-nav__link">
    Optimal Generator Strategy
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/edit/master/docs/lectures/week_12.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="week-12-general-adversarial-networks-gans">Week 12: General Adversarial Networks (GANs)<a class="headerlink" href="#week-12-general-adversarial-networks-gans" title="Permanent link">&para;</a></h1>
<h3 id="assigned-reading">Assigned Reading<a class="headerlink" href="#assigned-reading" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://arxiv.org/pdf/1701.00160.pdf">NIPS 2016 Tutorial: Generative Adversarial Networks; Goodfellow (2016)</a></li>
<li><a href="https://arxiv.org/abs/1406.2661">Generative adversarial networks; Goodfellow et al. (2014)</a></li>
</ul>
<p>In the first half of this lecture, we will review the <a href="https://en.wikipedia.org/wiki/Generative_model">generative models</a> that we have discussed so far.</p>
<h2 id="generative-models-a-review">Generative Models, a Review<a class="headerlink" href="#generative-models-a-review" title="Permanent link">&para;</a></h2>
<p>Generative models make the assumption that your data was generated from some distribution</p>
<div>
<div class="MathJax_Preview">
\{x_i\}_{i=1}^N \sim P_{data}
</div>
<script type="math/tex; mode=display">
\{x_i\}_{i=1}^N \sim P_{data}
</script>
</div>
<p>This is the distribution we will attempt to learn. More specifically, we want to learn a model, <span><span class="MathJax_Preview">P_{model}</span><script type="math/tex">P_{model}</script></span>, that represents an estimate of <span><span class="MathJax_Preview">P_{data}</span><script type="math/tex">P_{data}</script></span>. If we have done our job correctly, then samples from our model</p>
<div>
<div class="MathJax_Preview">
\tilde x_i \sim P_{model}
</div>
<script type="math/tex; mode=display">
\tilde x_i \sim P_{model}
</script>
</div>
<p>should look like samples from our data. The density <span><span class="MathJax_Preview">P_{model}</span><script type="math/tex">P_{model}</script></span> can be</p>
<ol>
<li>explicitly defined</li>
<li>approximated by random sampling from our model.</li>
</ol>
<p>So far, we have only looked at models of the first type, sometimes referred to as <strong>explicit density models</strong>.</p>
<p>As an example of the simplest possible model of the second type, sometimes referred to as <strong>implicit density models</strong>, imagine we had a sampler:</p>
<div>
<div class="MathJax_Preview">
j = \text{randint}(1:N) \\
\text{return} \ x_j
</div>
<script type="math/tex; mode=display">
j = \text{randint}(1:N) \\
\text{return} \ x_j
</script>
</div>
<p>This perhaps is the most trivial generative model we could define. Clearly, this model has no <em>"understanding"</em> of the distribution that generated our data. But what does it mean to <em>"understand"</em> a data distribution?</p>
<ul>
<li>the ability to generate new data</li>
<li>the ability to smoothly interpolate between data points <span><span class="MathJax_Preview">x_i \rightarrow x_j</span><script type="math/tex">x_i \rightarrow x_j</script></span></li>
<li>the ability to generate samples representing <em>"underlying features of variation"</em></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>For the last point, think of the example of generating images. We might expect a sampled images from a <em>good</em> model to contain variation in the lighting, or camera angle.</p>
</div>
<p>We have <a href="../week_11">previously discussed auto-encoders</a>, which achieve this <em>"understanding"</em> of our data via compression.</p>
<p>Of course, this begs the question: Why do we want our models to be able to <em>"understand"</em> a data distribution in the first place?</p>
<ul>
<li>Samples from high dimensional distributions are useful and serve as an excellent test of our ability to represent and manipulate high-dimensional probability distributions (import objects in a wide variety of applied math and engineering domains).</li>
<li>Model-based RL makes use of generative models.</li>
<li>Generative models can be trained with missing data and can provide predictions on inputs that are missing data (e.g. semi-supervised learning)</li>
<li>Generative modals enable machine learning to work with multi-modal outputs (for many tasks, a single input may correspond to many different correct answers, each of which is acceptable).</li>
<li>Finally, many tasks intrinsically require realistic generation of samples from some distribution, e.g.<ul>
<li>single image super-resolution</li>
<li>art creation</li>
<li>image-to-image translation</li>
</ul>
</li>
</ul>
<h3 id="prototypical-generative-model">Prototypical Generative Model<a class="headerlink" href="#prototypical-generative-model" title="Permanent link">&para;</a></h3>
<p>The prototypical generative model follows the following procedure:</p>
<ol>
<li>Sample some noise, <span><span class="MathJax_Preview">z \sim p(z)</span><script type="math/tex">z \sim p(z)</script></span></li>
<li>Pass this noise through a model, <span><span class="MathJax_Preview">G</span><script type="math/tex">G</script></span></li>
<li>Get a sample, <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> from <span><span class="MathJax_Preview">G(z)</span><script type="math/tex">G(z)</script></span></li>
</ol>
<p><img alt="" src="../../img/lecture_12_1.png" /></p>
<p>In VAEs, we saw that</p>
<div>
<div class="MathJax_Preview">
x \sim p_\theta(x | z)
</div>
<script type="math/tex; mode=display">
x \sim p_\theta(x | z)
</script>
</div>
<h3 id="maximum-likelihood-estimate">Maximum Likelihood Estimate<a class="headerlink" href="#maximum-likelihood-estimate" title="Permanent link">&para;</a></h3>
<p>Maximum likelihood estimate is <a href="../week_2/#maximum-likelihood-estimation">how we have trained all the generative models we have seen thus far</a>.</p>
<p>The basic idea is to:</p>
<ol>
<li>Define a model which provides an estimate of a probability distribution parameterized by parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span></li>
<li>Define a likelihood function that represents the probability of our data and train our parameters to maximize this likelihood</li>
</ol>
<p>e.g. for the i.i.d case:</p>
<div>
<div class="MathJax_Preview">
\prod_{i=1}^N P_{model} (x^{(i)}, \theta) \\
\Rightarrow \hat \theta_{MLE} = \underset{\theta}{\operatorname{argmax}} \sum \log P_{model} (x^{(i)} ; \theta)
</div>
<script type="math/tex; mode=display">
\prod_{i=1}^N P_{model} (x^{(i)}, \theta) \\
\Rightarrow \hat \theta_{MLE} = \underset{\theta}{\operatorname{argmax}} \sum \log P_{model} (x^{(i)} ; \theta)
</script>
</div>
<p>which is equivalent to the minimizing the <span><span class="MathJax_Preview">D_{KL}</span><script type="math/tex">D_{KL}</script></span> between <span><span class="MathJax_Preview">P_{data}(x)</span><script type="math/tex">P_{data}(x)</script></span> and <span><span class="MathJax_Preview">P_{model}(x ; \theta)</span><script type="math/tex">P_{model}(x ; \theta)</script></span></p>
<div>
<div class="MathJax_Preview">
\underset{\theta}{\operatorname{argmin}} D_{KL}(P_{data}(x) || P_{model}(x ; \theta))
</div>
<script type="math/tex; mode=display">
\underset{\theta}{\operatorname{argmin}} D_{KL}(P_{data}(x) || P_{model}(x ; \theta))
</script>
</div>
<p>If we were able to do this precisely, and if <span><span class="MathJax_Preview">P_{data}</span><script type="math/tex">P_{data}</script></span> lies within the family of distributions <span><span class="MathJax_Preview">P_{model}(x ; \theta)</span><script type="math/tex">P_{model}(x ; \theta)</script></span>, then the model would recover <span><span class="MathJax_Preview">P_{data}</span><script type="math/tex">P_{data}</script></span> exactly. In practice, we do not have access to <span><span class="MathJax_Preview">P_{data}</span><script type="math/tex">P_{data}</script></span> itself, but only to a training set consisting of <span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span> samples from <span><span class="MathJax_Preview">P_{data}</span><script type="math/tex">P_{data}</script></span>. We uses these to define <span><span class="MathJax_Preview">\hat P_{data}</span><script type="math/tex">\hat P_{data}</script></span>, an empirical distribution that places mass only on exactly those <span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span> points, approximating <span><span class="MathJax_Preview">P_{data}</span><script type="math/tex">P_{data}</script></span>. Minimizing the <span><span class="MathJax_Preview">D_{KL}</span><script type="math/tex">D_{KL}</script></span> between <span><span class="MathJax_Preview">\hat P_{data}</span><script type="math/tex">\hat P_{data}</script></span> and <span><span class="MathJax_Preview">P_{model}</span><script type="math/tex">P_{model}</script></span> is exactly equivalent to maximizing the log-likelihood of the training set.</p>
<h3 id="explicit-density-models">Explicit Density Models<a class="headerlink" href="#explicit-density-models" title="Permanent link">&para;</a></h3>
<p>In explicit density models (i.e., the models we are discussing and have been discussing the entire course) define an explicit density function <span><span class="MathJax_Preview">P_{model}(x ; \theta)</span><script type="math/tex">P_{model}(x ; \theta)</script></span>. For these models, maximization of the likelihood function is straightforward: we simply plug the models definition of the density function into the expression for likelihood and follow the gradient uphill.</p>
<p>The main difficulty present in explicit density models is designing a model that can capture all of the complexity of the data to be generated while still maintaining computational tractability.</p>
<ol>
<li>careful construction of models whose structure guarantees their tractability</li>
<li>models that admit tractable approximations to the likelihood and its gradients</li>
</ol>
<p><em>Tractable</em> examples include:</p>
<ul>
<li>Fully Visible Belief Nets (96, 98)</li>
<li>WaveNet (2016)</li>
</ul>
<p>In the <em>intractable</em> case, we use variational (e.g. <a href="../week_11/#variational-autoencoders-vaes">VAEs</a>) or <a href="../week_8/#metropolis-hastings-method">MCMC approximations</a> to get at <span><span class="MathJax_Preview">\log p_\theta(x)</span><script type="math/tex">\log p_\theta(x)</script></span>.</p>
<h3 id="generative-model-goal">Generative Model Goal<a class="headerlink" href="#generative-model-goal" title="Permanent link">&para;</a></h3>
<p>The goal of generative models, in short, is to produce samples similar to <span><span class="MathJax_Preview">P_{data}</span><script type="math/tex">P_{data}</script></span>. But do we really need maximum likelihood estimations to achieve our goals?</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Everything up until this point was considered review. Everything that follows is new content.</p>
</div>
<h2 id="implicit-density-models">Implicit Density Models<a class="headerlink" href="#implicit-density-models" title="Permanent link">&para;</a></h2>
<p>In implicit density models, we do not define <span><span class="MathJax_Preview">P_{model}(x; \theta)</span><script type="math/tex">P_{model}(x; \theta)</script></span> explicitly. Instead, we interact with <span><span class="MathJax_Preview">P_{model}</span><script type="math/tex">P_{model}</script></span> through samples.</p>
<p>A taxonomic tree of generative models is shown below:</p>
<p><img alt="" src="../../img/lecture_12_2.png" /></p>
<div class="admonition cite">
<p class="admonition-title">Cite</p>
<p><a href="https://arxiv.org/pdf/1701.00160.pdf">NIPS 2016 Tutorial: Generative Adversarial Networks; Goodfellow (2016)</a>.</p>
</div>
<p><em>But how do we train it?</em> No density means <em>no likelihood evaluation</em>. We will look at a specific instance of implicit density models known as Generative Adversarial Networks.</p>
<h2 id="general-adversarial-network-gan-approach">General Adversarial Network (GAN) Approach<a class="headerlink" href="#general-adversarial-network-gan-approach" title="Permanent link">&para;</a></h2>
<p>In the general adversarial approach, we <em>do not</em> have likelihoods, only samples. The idea is based on an adversarial game and pulls heavily from <a href="https://en.wikipedia.org/wiki/Game_theory">game theory</a>.</p>
<p>The basic idea of GANs is to set up a game between two players:</p>
<ul>
<li>One of them is called the <strong>generator</strong> (<span><span class="MathJax_Preview">G_{\theta_G}</span><script type="math/tex">G_{\theta_G}</script></span>). The generator creates samples that are intended to come from the same distribution as the training data.</li>
<li>The other player is the <strong>discriminator</strong> (<span><span class="MathJax_Preview">D_{\theta_D}</span><script type="math/tex">D_{\theta_D}</script></span>). The discriminator examines samples to determine whether they are real or fake.</li>
</ul>
<p>The discriminator learns using traditional supervised learning techniques, dividing inputs into two classes (real or fake). The generator is trained to fool the discriminator. Both are almost always modeled as neural networks (and are therefore differentiable w.r.t their outputs). Once training is complete, <em>we throw away the discriminator</em>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>We can think of the generator as being like a counterfeiter, trying to make fake money, and the discriminator as being like police, trying to allow legitimate money and catch counterfeit money. To succeed in this game, the counterfeiter must learn to make money that is indistinguishable from genuine money, and the generator network must learn to create samples that are drawn from the same distribution as the training data.</p>
</div>
<p>Formally, GANs are structured as a probabilistic model containing latent variables <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> and observed variable <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span></p>
<p><img alt="" src="../../img/lecture_12_3.png" /></p>
<div class="admonition cite">
<p class="admonition-title">Cite</p>
<p><a href="https://arxiv.org/pdf/1701.00160.pdf">NIPS 2016 Tutorial: Generative Adversarial Networks; Goodfellow (2016)</a>.</p>
</div>
<p>to sample from the model:</p>
<ul>
<li>first sample <span><span class="MathJax_Preview">z \sim p(z)</span><script type="math/tex">z \sim p(z)</script></span> ; where <span><span class="MathJax_Preview">p(z)</span><script type="math/tex">p(z)</script></span> is some prior distribution</li>
<li>then <span><span class="MathJax_Preview">x = G(z) \sim P_{model}</span><script type="math/tex">x = G(z) \sim P_{model}</script></span></li>
</ul>
<p>notice that we <em>never explicitly define a distribution</em> <span><span class="MathJax_Preview">P_{model}(x; \theta)</span><script type="math/tex">P_{model}(x; \theta)</script></span>. Contrast this with VAEs, where to sample from the model:</p>
<ul>
<li>first sample <span><span class="MathJax_Preview">z \sim q_\phi(z | x)</span><script type="math/tex">z \sim q_\phi(z | x)</script></span> for some input <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span></li>
<li>then using the encoder, compute <span><span class="MathJax_Preview">\theta = f(z)</span><script type="math/tex">\theta = f(z)</script></span></li>
<li>finally, using the decoder, sample <span><span class="MathJax_Preview">x \sim p_\theta(x | z)</span><script type="math/tex">x \sim p_\theta(x | z)</script></span></li>
</ul>
<p>notice that we <em>explicitly define a distribution</em> (<span><span class="MathJax_Preview">p_\theta(x | z)</span><script type="math/tex">p_\theta(x | z)</script></span>) and sample from it.</p>
<h3 id="adversarial-game-the-specifics">Adversarial Game, the Specifics<a class="headerlink" href="#adversarial-game-the-specifics" title="Permanent link">&para;</a></h3>
<p>In the adversarial game, both players have cost functions that are defined in terms of both players’ parameters</p>
<ul>
<li>The <strong>discriminator</strong> wishes to minimize <span><span class="MathJax_Preview">J^{(D)}(\theta_D, \theta_G)</span><script type="math/tex">J^{(D)}(\theta_D, \theta_G)</script></span> and must do so while controlling only <span><span class="MathJax_Preview">\theta_D</span><script type="math/tex">\theta_D</script></span>.</li>
<li>The <strong>generator</strong> wishes to minimize <span><span class="MathJax_Preview">J^{(G)}(\theta_D, \theta_G)</span><script type="math/tex">J^{(G)}(\theta_D, \theta_G)</script></span> and must do so while controlling only <span><span class="MathJax_Preview">\theta_G</span><script type="math/tex">\theta_G</script></span>.</li>
</ul>
<p>Because each player’s cost depends on the other player’s parameters, but each player cannot control the other player’s parameters, this scenario is most straightforward to describe as a game rather than as an optimization problem</p>
<h4 id="training-procedure">Training Procedure<a class="headerlink" href="#training-procedure" title="Permanent link">&para;</a></h4>
<p>The training process consists of simultaneous <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">SGD</a>. On each step, two minibatches are sampled: a minibatch of <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> values from the dataset and a minibatch of <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> values drawn from the model’s prior over latent variables, e.g.,</p>
<ol>
<li><span><span class="MathJax_Preview">x \sim P_{data}</span><script type="math/tex">x \sim P_{data}</script></span></li>
<li><span><span class="MathJax_Preview">z \sim p(z) \Rightarrow x \sim P_{G(z)}</span><script type="math/tex">z \sim p(z) \Rightarrow x \sim P_{G(z)}</script></span></li>
</ol>
<p>Then two gradient steps are made simultaneously: one updating <span><span class="MathJax_Preview">\theta_D</span><script type="math/tex">\theta_D</script></span> to reduce <span><span class="MathJax_Preview">J^{(D)}</span><script type="math/tex">J^{(D)}</script></span> and one updating <span><span class="MathJax_Preview">\theta_G</span><script type="math/tex">\theta_G</script></span> to reduce <span><span class="MathJax_Preview">J^{(G)}</span><script type="math/tex">J^{(G)}</script></span>.</p>
<h3 id="cost-functions">Cost Functions<a class="headerlink" href="#cost-functions" title="Permanent link">&para;</a></h3>
<p>Several different cost functions may be used within the GANs framework. All of the different games designed for GANs so far use the same cost for the discriminator, <span><span class="MathJax_Preview">J^{(D)}</span><script type="math/tex">J^{(D)}</script></span>. They differ only in terms of the cost used for the generator, <span><span class="MathJax_Preview">J^{(G)}</span><script type="math/tex">J^{(G)}</script></span>.</p>
<h4 id="the-discriminators-cost">The Discriminator’s Cost<a class="headerlink" href="#the-discriminators-cost" title="Permanent link">&para;</a></h4>
<p>The cost used for the discriminator is (almost) always:</p>
<div>
<div class="MathJax_Preview">
J^{(D)}(\theta_D, \theta_G) = -\mathbb E_{x \sim P_{data}}[\log D(x)] - \mathbb E_{z \sim p(z)}[\log(1 - D(G(z)))]
</div>
<script type="math/tex; mode=display">
J^{(D)}(\theta_D, \theta_G) = -\mathbb E_{x \sim P_{data}}[\log D(x)] - \mathbb E_{z \sim p(z)}[\log(1 - D(G(z)))]
</script>
</div>
<p>This is just the standard cross-entropy cost that is minimized when training a standard binary classifier with a sigmoid output. The only difference is that the classifier is trained on two mini-batches of data; one coming from the dataset, where the label is 1 for all examples, and one coming from the generator, where the label is 0 for all examples.</p>
<h5 id="optimal-discriminator-strategy">Optimal Discriminator Strategy<a class="headerlink" href="#optimal-discriminator-strategy" title="Permanent link">&para;</a></h5>
<p>Our goal is to minimize</p>
<div>
<div class="MathJax_Preview">
J^{(D)}(\theta_D, \theta_G) = -\mathbb E_{x \sim P_{data}}[\log D(x)] - \mathbb E_{z \sim p(z)}[\log(1 - D(G(z)))]
</div>
<script type="math/tex; mode=display">
J^{(D)}(\theta_D, \theta_G) = -\mathbb E_{x \sim P_{data}}[\log D(x)] - \mathbb E_{z \sim p(z)}[\log(1 - D(G(z)))]
</script>
</div>
<p>in function space, specifying <span><span class="MathJax_Preview">D(x)</span><script type="math/tex">D(x)</script></span> directly. We begin by assuming that both <span><span class="MathJax_Preview">P_{data}</span><script type="math/tex">P_{data}</script></span> and <span><span class="MathJax_Preview">P_{model}</span><script type="math/tex">P_{model}</script></span> are nonzero everywhere.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If we do not make this assumption, then some points are never visited during training, and have undefined behavior.</p>
</div>
<p><img alt="" src="../../img/lecture_12_4.png" /></p>
<p>To minimize <span><span class="MathJax_Preview">J^{(D)}</span><script type="math/tex">J^{(D)}</script></span> with respect to <span><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>, we can write down the functional
derivatives with respect to a single entry <span><span class="MathJax_Preview">D^{(x)}</span><script type="math/tex">D^{(x)}</script></span>, and set them equal to zero:</p>
<div>
<div class="MathJax_Preview">
\frac{\delta}{\delta D(x)}J^{(D)} = 0
</div>
<script type="math/tex; mode=display">
\frac{\delta}{\delta D(x)}J^{(D)} = 0
</script>
</div>
<p>By solving this equation, we obtain</p>
<div>
<div class="MathJax_Preview">
D^\star(x)  = \frac{P_{data}(x)}{P_{data}(x) + P_{model}(x)}
</div>
<script type="math/tex; mode=display">
D^\star(x)  = \frac{P_{data}(x)}{P_{data}(x) + P_{model}(x)}
</script>
</div>
<p>The discriminator (dashed blue line) estimates the ratio between the data density (black dots) and the sum of the data and model densities. Wherever the output of the discriminator is large, the model density is too low, and wherever the output of the discriminator is small, the model density is too high. Estimating this ratio is the key approximation mechanism used by GANs</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We can't actually compute <span><span class="MathJax_Preview">D^\star(x)</span><script type="math/tex">D^\star(x)</script></span> directly but it illustrates how the discriminator achieves optimal behavior.</p>
</div>
<h5 id="optimal-generator-strategy">Optimal Generator Strategy<a class="headerlink" href="#optimal-generator-strategy" title="Permanent link">&para;</a></h5>
<p>So far we have specified the cost function for only the discriminator. A complete specification of the game requires that we specify a cost function also for the generator.</p>
<p>The simplest version of the game is a <strong>zero-sum game</strong>, in which the sum of all player’s costs is always zero. In this version of the game,</p>
<div>
<div class="MathJax_Preview">
J(^{(G)}) = -J^{(D)}
</div>
<script type="math/tex; mode=display">
J(^{(G)}) = -J^{(D)}
</script>
</div>
<p>Because <span><span class="MathJax_Preview">J^{(G)}</span><script type="math/tex">J^{(G)}</script></span> is tied directly to <span><span class="MathJax_Preview">J^{(D)}</span><script type="math/tex">J^{(D)}</script></span>, we can summarize the entire game
with a value function specifying the discriminator’s payoff:</p>
<div>
<div class="MathJax_Preview">
V(\theta^{(D)}, \theta^{(G)}) = -J^{(D)}(\theta^{(D)}, \theta^{(G)})
</div>
<script type="math/tex; mode=display">
V(\theta^{(D)}, \theta^{(G)}) = -J^{(D)}(\theta^{(D)}, \theta^{(G)})
</script>
</div>
<p>Zero-sum games are also called <strong>minimax</strong> games because their solution involves minimization in an outer loop and maximization in an inner loop:</p>
<div>
<div class="MathJax_Preview">
\theta^{(G)\star} = \underset{\theta^{(G)}}{\operatorname{argmin}} \underset{\theta^{(D)}}{\operatorname{argmax}} V(\theta^{(D)}, \theta^{(G)})
</div>
<script type="math/tex; mode=display">
\theta^{(G)\star} = \underset{\theta^{(G)}}{\operatorname{argmin}} \underset{\theta^{(D)}}{\operatorname{argmax}} V(\theta^{(D)}, \theta^{(G)})
</script>
</div>
<p>Where we want to maximize <span><span class="MathJax_Preview">\theta_D</span><script type="math/tex">\theta_D</script></span> such that</p>
<ul>
<li><span><span class="MathJax_Preview">D_{\theta_D}(x) = 1</span><script type="math/tex">D_{\theta_D}(x) = 1</script></span></li>
<li><span><span class="MathJax_Preview">D_{\theta_D}(G(z)) = 0</span><script type="math/tex">D_{\theta_D}(G(z)) = 0</script></span></li>
</ul>
<p>and minimize <span><span class="MathJax_Preview">\theta_G</span><script type="math/tex">\theta_G</script></span> such that</p>
<ul>
<li><span><span class="MathJax_Preview">D_{\theta_D}(G_{\theta_G}(z)) \rightarrow 1</span><script type="math/tex">D_{\theta_D}(G_{\theta_G}(z)) \rightarrow 1</script></span></li>
</ul>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../week_11/" title="Week 11" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Week 11
              </span>
            </div>
          </a>
        
        
          <a href="../week_13/" title="Week 13" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Week 13
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
  <div class="md-footer-social">
    <link rel="stylesheet" href="../../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/JohnGiorgi" class="md-footer-social__link fa fa-github"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/application.b41f3d20.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"../.."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
    
      
    
  </body>
</html>