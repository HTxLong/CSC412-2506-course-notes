



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="https://johngiorgi.github.io/CSC412-2506-course-notes/. /lectures/week_1/">
      
      
        <meta name="author" content="John Giorgi">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.2.0">
    
    
      
        <title>Week 1 - CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/application.572ca0f0.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../../assets/javascripts/modernizr.8c900955.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="../../styles/extra.css">
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#week-1-introduction" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://johngiorgi.github.io/CSC412-2506-course-notes/. " title="CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning
              </span>
              <span class="md-header-nav__topic">
                Week 1
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      JohnGiorgi/CSC412-2506-course-notes
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://johngiorgi.github.io/CSC412-2506-course-notes/. " title="CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      JohnGiorgi/CSC412-2506-course-notes
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../.." title="About" class="md-nav__link">
      About
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      Lectures
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Lectures
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Week 1
      </label>
    
    <a href="./" title="Week 1" class="md-nav__link md-nav__link--active">
      Week 1
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#assigned-reading" title="Assigned Reading" class="md-nav__link">
    Assigned Reading
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overview" title="Overview" class="md-nav__link">
    Overview
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#textbook-and-resources" title="Textbook and Resources" class="md-nav__link">
    Textbook and Resources
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#assignments" title="Assignments" class="md-nav__link">
    Assignments
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#related-courses" title="Related Courses" class="md-nav__link">
    Related Courses
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stats-vs-machine-learning" title="Stats vs Machine Learning" class="md-nav__link">
    Stats vs Machine Learning
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#types-of-learning" title="Types of Learning" class="md-nav__link">
    Types of Learning
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#finding-structure-in-data" title="Finding Structure in Data" class="md-nav__link">
    Finding Structure in Data
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#matrix-factorization" title="Matrix Factorization" class="md-nav__link">
    Matrix Factorization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multiple-kinds-of-data-in-one-model" title="Multiple Kinds of Data in One Model" class="md-nav__link">
    Multiple Kinds of Data in One Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#latent-representations" title="Latent Representations" class="md-nav__link">
    Latent Representations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#latent-representations-of-structured-data" title="Latent Representations of Structured Data" class="md-nav__link">
    Latent Representations of Structured Data
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#course-themes" title="Course Themes" class="md-nav__link">
    Course Themes
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#computation" title="Computation" class="md-nav__link">
    Computation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ml-as-a-bag-of-tricks" title="ML as a Bag of Tricks" class="md-nav__link">
    ML as a Bag of Tricks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regularization-as-a-bag-of-tricks" title="Regularization as a Bag of Tricks" class="md-nav__link">
    Regularization as a Bag of Tricks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-language-of-models" title="A Language of Models" class="md-nav__link">
    A Language of Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#advantages-of-probabilistic-latent-variable-models" title="Advantages of probabilistic latent-variable models" class="md-nav__link">
    Advantages of probabilistic latent-variable models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#disadvantages-of-probabilistic-latent-variable-models" title="Disadvantages of Probabilistic Latent-variable Models" class="md-nav__link">
    Disadvantages of Probabilistic Latent-variable Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probabilistic-graphical-models-vs-neural-networks" title="Probabilistic Graphical Models Vs. Neural Networks" class="md-nav__link">
    Probabilistic Graphical Models Vs. Neural Networks
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-unreasonable-easiness-of-deep-learning" title="The Unreasonable Easiness of Deep Learning" class="md-nav__link">
    The Unreasonable Easiness of Deep Learning
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#differentiable-models" title="Differentiable models" class="md-nav__link">
    Differentiable models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modeling-idea" title="Modeling Idea" class="md-nav__link">
    Modeling Idea
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-outcomes" title="Learning Outcomes" class="md-nav__link">
    Learning Outcomes
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tentative-list-of-topics" title="Tentative List of Topics" class="md-nav__link">
    Tentative List of Topics
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-centric-history-of-probabilistic-models" title="Machine-learning-centric History of Probabilistic Models" class="md-nav__link">
    Machine-learning-centric History of Probabilistic Models
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#appendix" title="Appendix" class="md-nav__link">
    Appendix
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#useful-resources" title="Useful Resources" class="md-nav__link">
    Useful Resources
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#glossary-of-terms" title="Glossary of Terms" class="md-nav__link">
    Glossary of Terms
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_2/" title="Week 2" class="md-nav__link">
      Week 2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_3/" title="Week 3" class="md-nav__link">
      Week 3
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_4/" title="Week 4" class="md-nav__link">
      Week 4
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_5/" title="Week 5" class="md-nav__link">
      Week 5
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_6/" title="Week 6" class="md-nav__link">
      Week 6
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_8/" title="Week 8" class="md-nav__link">
      Week 8
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_9/" title="Week 9" class="md-nav__link">
      Week 9
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_10/" title="Week 10" class="md-nav__link">
      Week 10
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Tutorials
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Tutorials
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/week_1/" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../sample_midterm_answers/" title="Sample Midterm (Answers)" class="md-nav__link">
      Sample Midterm (Answers)
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#assigned-reading" title="Assigned Reading" class="md-nav__link">
    Assigned Reading
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overview" title="Overview" class="md-nav__link">
    Overview
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#textbook-and-resources" title="Textbook and Resources" class="md-nav__link">
    Textbook and Resources
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#assignments" title="Assignments" class="md-nav__link">
    Assignments
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#related-courses" title="Related Courses" class="md-nav__link">
    Related Courses
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stats-vs-machine-learning" title="Stats vs Machine Learning" class="md-nav__link">
    Stats vs Machine Learning
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#types-of-learning" title="Types of Learning" class="md-nav__link">
    Types of Learning
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#finding-structure-in-data" title="Finding Structure in Data" class="md-nav__link">
    Finding Structure in Data
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#matrix-factorization" title="Matrix Factorization" class="md-nav__link">
    Matrix Factorization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multiple-kinds-of-data-in-one-model" title="Multiple Kinds of Data in One Model" class="md-nav__link">
    Multiple Kinds of Data in One Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#latent-representations" title="Latent Representations" class="md-nav__link">
    Latent Representations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#latent-representations-of-structured-data" title="Latent Representations of Structured Data" class="md-nav__link">
    Latent Representations of Structured Data
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#course-themes" title="Course Themes" class="md-nav__link">
    Course Themes
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#computation" title="Computation" class="md-nav__link">
    Computation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ml-as-a-bag-of-tricks" title="ML as a Bag of Tricks" class="md-nav__link">
    ML as a Bag of Tricks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regularization-as-a-bag-of-tricks" title="Regularization as a Bag of Tricks" class="md-nav__link">
    Regularization as a Bag of Tricks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-language-of-models" title="A Language of Models" class="md-nav__link">
    A Language of Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#advantages-of-probabilistic-latent-variable-models" title="Advantages of probabilistic latent-variable models" class="md-nav__link">
    Advantages of probabilistic latent-variable models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#disadvantages-of-probabilistic-latent-variable-models" title="Disadvantages of Probabilistic Latent-variable Models" class="md-nav__link">
    Disadvantages of Probabilistic Latent-variable Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probabilistic-graphical-models-vs-neural-networks" title="Probabilistic Graphical Models Vs. Neural Networks" class="md-nav__link">
    Probabilistic Graphical Models Vs. Neural Networks
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-unreasonable-easiness-of-deep-learning" title="The Unreasonable Easiness of Deep Learning" class="md-nav__link">
    The Unreasonable Easiness of Deep Learning
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#differentiable-models" title="Differentiable models" class="md-nav__link">
    Differentiable models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modeling-idea" title="Modeling Idea" class="md-nav__link">
    Modeling Idea
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-outcomes" title="Learning Outcomes" class="md-nav__link">
    Learning Outcomes
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tentative-list-of-topics" title="Tentative List of Topics" class="md-nav__link">
    Tentative List of Topics
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-centric-history-of-probabilistic-models" title="Machine-learning-centric History of Probabilistic Models" class="md-nav__link">
    Machine-learning-centric History of Probabilistic Models
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#appendix" title="Appendix" class="md-nav__link">
    Appendix
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#useful-resources" title="Useful Resources" class="md-nav__link">
    Useful Resources
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#glossary-of-terms" title="Glossary of Terms" class="md-nav__link">
    Glossary of Terms
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/edit/master/docs/lectures/week_1.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="week-1-introduction">Week 1: Introduction<a class="headerlink" href="#week-1-introduction" title="Permanent link">&para;</a></h1>
<h3 id="assigned-reading">Assigned Reading<a class="headerlink" href="#assigned-reading" title="Permanent link">&para;</a></h3>
<ul>
<li>Murphy: Chapters 1 and 2</li>
<li><a href="http://www.inference.org.uk/mackay/itprnn/ps/22.40.pdf">Chapter 2 of David Mackay's textbook</a></li>
</ul>
<h3 id="overview">Overview<a class="headerlink" href="#overview" title="Permanent link">&para;</a></h3>
<ul>
<li>Course information</li>
<li>Overview of ML with examples</li>
<li>Ungraded, anonymous background quiz</li>
</ul>
<h3 id="textbook-and-resources">Textbook and Resources<a class="headerlink" href="#textbook-and-resources" title="Permanent link">&para;</a></h3>
<p>There is no required textbook, but optional reading will be assigned each week</p>
<ul>
<li>Kevin Murphy (2012), <em>Machine Learning: A Probabilistic Perspective</em>.</li>
<li>David MacKay (2003) <em>Information Theory, Inference, and Learning Algorithms</em>.</li>
</ul>
<p>The David MacKay textbook is recommended, although 100% of tested material come from class.</p>
<p>In this course, lecture slides are more a supplement then main content. The most important stuff will be done on the blackboard, so it is important to come to class with pen and paper. Tutorial slides are relevant, but probably won't be tested.</p>
<h3 id="assignments">Assignments<a class="headerlink" href="#assignments" title="Permanent link">&para;</a></h3>
<p>Assignments must be your own individual work. You can collaborate with up to 2 other students. You should name these people in your submission. <em>Code should be readable</em>. Make sure to put all plots and important results in your PDF submission.</p>
<h3 id="related-courses">Related Courses<a class="headerlink" href="#related-courses" title="Permanent link">&para;</a></h3>
<ul>
<li>CSC411: List of methods, (K-NN, Decision trees), more focus on computation</li>
<li>STA302: Linear regression and classical stats</li>
<li>ECE521: Similar material, more focus on computation</li>
<li>STA414: Mostly same material, slightly more emphasis on theory than coding</li>
<li>CSC321: Neural networks - about 30% overlap</li>
</ul>
<h2 id="stats-vs-machine-learning">Stats vs Machine Learning<a class="headerlink" href="#stats-vs-machine-learning" title="Permanent link">&para;</a></h2>
<p><strong>Statisticians</strong> look at the data, consider the problem, and design a model we can understand. They</p>
<ul>
<li>Analyze methods to give guarantees</li>
<li>Want to make few assumptions</li>
</ul>
<p>In <strong>machine learning</strong>, We only care about making good predictions! The basic idea is to <em>learn</em> a general procedure that works for lots of datasets.</p>
<ul>
<li>Often, there is no way around making assumptions, so we make our model large enough to <em>hopefully</em> learn something close to the truth.</li>
<li>We can't use bounds in practice, so we evaluate and empirically choose model details.</li>
<li>Sometimes, we end up with interpretable models anyways!</li>
</ul>
<p>In short, statistics starts with a model <em>based on the data</em>, machine learning aims to learn a model <em>from</em> the data.</p>
<h2 id="types-of-learning">Types of Learning<a class="headerlink" href="#types-of-learning" title="Permanent link">&para;</a></h2>
<ul>
<li>Unsupervised Learning: Given unlabeled data instances <span><span class="MathJax_Preview">x_1</span><script type="math/tex">x_1</script></span>, <span><span class="MathJax_Preview">x_2</span><script type="math/tex">x_2</script></span>, <span><span class="MathJax_Preview">x_3</span><script type="math/tex">x_3</script></span>... build a statistical model of <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>, which can be used for making predictions, decisions.</li>
<li>Supervised Learning: Given input-output pairs <span><span class="MathJax_Preview">(x,y)</span><script type="math/tex">(x,y)</script></span> the goal is to predict correct output given a new input.</li>
<li>Semi-supervised Learning: We are given only a limited amount of <span><span class="MathJax_Preview">(x, y)</span><script type="math/tex">(x, y)</script></span> pairs, but lots of unlabeled <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>'s</li>
<li>Active learning and RL: Also get to choose actions that influence <span><span class="MathJax_Preview">(x, y)</span><script type="math/tex">(x, y)</script></span> pairs, but lots of unlabeled <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>’s. future information + reward. Can just use basic decision theory.</li>
</ul>
<p>Note that these are all just special cases of estimating distributions from data: <span><span class="MathJax_Preview">p(y | x)</span><script type="math/tex">p(y | x)</script></span>, <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span>, <span><span class="MathJax_Preview">p(x, y)</span><script type="math/tex">p(x, y)</script></span>!</p>
<h2 id="finding-structure-in-data">Finding Structure in Data<a class="headerlink" href="#finding-structure-in-data" title="Permanent link">&para;</a></h2>
<p>With a big enough dataset, we can identify structure in the data. Take a large newswire corpus, for example. A simple model based on the word counts of webpages</p>
<p>\[P(x) = \frac{1}{Z} \sum_h \exp [x^TWh]\]</p>
<p>could learn to discretize data into topics. In this case, our topics are our <strong>hidden</strong> (or <a href="https://en.wikipedia.org/wiki/Latent_variable"><strong>latent</strong></a>) variables.</p>
<p><img alt="" src="../../img/lecture_1_1.png" /></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Sometimes latent variables correspond to aspects of physical reality, which could in principle be measured, but may not be for practical reasons. In this situation, the term hidden variables is commonly used (reflecting the fact that the variables are "really there", but hidden).</p>
</div>
<h3 id="matrix-factorization">Matrix Factorization<a class="headerlink" href="#matrix-factorization" title="Permanent link">&para;</a></h3>
<p>Lets take a look at a specific example which uses <a href="https://en.wikipedia.org/wiki/Matrix_decomposition">matrix factorization</a> for <a href="https://en.wikipedia.org/wiki/Collaborative_filtering">collaborative filtering</a>.</p>
<p>Part of the winning solution in the Netflix contest started with a Netflix dataset of 480,189 users, 17,770 movies and over 100 million ratings. The job was essentially to "fill-in" the missing information in a table that looked something like the following (hence the collaborative in collaborative filtering):</p>
<p><img alt="" src="../../img/lecture_1_2.png" /></p>
<p>After the modal was learned, it was clear that the latent representations it learned closley mapped what we might call <em>genre</em>:</p>
<p><img alt="" src="../../img/lecture_1_3.png" /></p>
<h3 id="multiple-kinds-of-data-in-one-model">Multiple Kinds of Data in One Model<a class="headerlink" href="#multiple-kinds-of-data-in-one-model" title="Permanent link">&para;</a></h3>
<p>My modeling the joint distribution of our data <span><span class="MathJax_Preview">p(x, y)</span><script type="math/tex">p(x, y)</script></span>, we can incorporate multiple types of data under one model. In this example, our dataset consists of both images and text.</p>
<p><img alt="" src="../../img/lecture_1_4.png" /></p>
<p>Once the joint distribution is learned, we could provide a word and ask the model to sample from the learned distribution and return a picture (or vice versa!):</p>
<p><img alt="" src="../../img/lecture_1_5.png" /></p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>Nguyen A, Dosovitskiy A, Yosinski J, Brox T, Clune J (2016). Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. Advances in Neural Information Processing Systems 29</p>
</div>
<p>In fact, this is the key idea behind image captioning models:</p>
<p><img alt="" src="../../img/lecture_1_6.png" /></p>
<h3 id="latent-representations">Latent Representations<a class="headerlink" href="#latent-representations" title="Permanent link">&para;</a></h3>
<p>Once learned, latent representations of our data allow us to do some powerful things. For example, neural networks that are able to fill-in occuluded (or missing) portions of digital images:</p>
<p><img alt="" src="../../img/lecture_1_7.png" /></p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>Pixel Recurrent Neural Networks. Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu</p>
</div>
<p>Because our latent space is really a <a href="https://en.wikipedia.org/wiki/Vector_space"><strong>vector space</strong></a>, we have access to all the mathematical operations that are defined on vectors, such as <em>addition</em> and <em>subtraction</em>. Furthermore, out latent representations (which themselves are just vector learned during model training) can be decoded into images (or words, or molecules, etc!).</p>
<p>For example, if we were to learn latent representations of human faces, we could add and subtract these representations to create entirely new representations:</p>
<p><img alt="" src="../../img/lecture_1_8.png" /></p>
<h3 id="latent-representations-of-structured-data">Latent Representations of Structured Data<a class="headerlink" href="#latent-representations-of-structured-data" title="Permanent link">&para;</a></h3>
<p>Some data is structured in a way that is semantically meaningful. Put another way, there is a "grammar" to the data. Take the example of molecules representing pharmaceutical drugs.</p>
<p><img alt="" src="../../img/lecture_1_9.png" /></p>
<p>In this case, it is much more difficult to interpolate between two valid structures than it would be to interpolate between images of human faces, for example, because of the grammar of organic chemistry. Simplifying this point, two molecules that look extremely similar could in fact have wildly different behavior in the human body.</p>
<p>The take-home point is that we need different methods for learning latent representations of <em>structured</em> data than <em>unstructured</em> data</p>
<h2 id="course-themes">Course Themes<a class="headerlink" href="#course-themes" title="Permanent link">&para;</a></h2>
<p>This course will follow the broad theme of starting with a simple model and then adding to it.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Linear regression and principal component analysis (PCA) are special cases of almost everything.</p>
</div>
<p>A few "lego bricks" are enough to build most models (e.g. gaussians, categorical variables, linear transforms and neural networks). While the exact for of each distribution / function shouldn't mater much, your model should have a million parameters in it somewhere (the real world is messy!)</p>
<p>Model checking is hard, but important. Learning algorithms are especially hard to debug.</p>
<h3 id="computation">Computation<a class="headerlink" href="#computation" title="Permanent link">&para;</a></h3>
<p>Later assignments will involve a bit of programming. You can use whatever language you want, but <code>python</code> and <code>numpy</code> are recommended.</p>
<p>For fitting and inference in high-dimensional models, gradient-based methods are basically the only game in town</p>
<p>Lots of methods conflate model and fitting algorithm, we will try to separate these.</p>
<h3 id="ml-as-a-bag-of-tricks">ML as a Bag of Tricks<a class="headerlink" href="#ml-as-a-bag-of-tricks" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Fast special cases</th>
<th>Extensible family</th>
</tr>
</thead>
<tbody>
<tr>
<td>K-means</td>
<td>Mixture of Gaussians</td>
</tr>
<tr>
<td>Kernel Density Estimation</td>
<td>Latent variable models</td>
</tr>
<tr>
<td>SVMs</td>
<td>Gaussian processes</td>
</tr>
<tr>
<td>Boosting</td>
<td>Deep neural nets</td>
</tr>
<tr>
<td>Random Forests</td>
<td>Bayesian neural nets</td>
</tr>
<tr>
<td>K-Nearest Neighbours</td>
<td>??</td>
</tr>
</tbody>
</table>
<h3 id="regularization-as-a-bag-of-tricks">Regularization as a Bag of Tricks<a class="headerlink" href="#regularization-as-a-bag-of-tricks" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Fast special cases</th>
<th>Extensible family</th>
</tr>
</thead>
<tbody>
<tr>
<td>Early stopping</td>
<td></td>
</tr>
<tr>
<td>Ensembling</td>
<td></td>
</tr>
<tr>
<td>L2 Regularization</td>
<td>Stochastic variational inference</td>
</tr>
<tr>
<td>Gradient noise</td>
<td></td>
</tr>
<tr>
<td>Dropout</td>
<td></td>
</tr>
<tr>
<td>Expectation-Maximization</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="a-language-of-models">A Language of Models<a class="headerlink" href="#a-language-of-models" title="Permanent link">&para;</a></h3>
<p>Our goal will be to develop a language of models, a <em>toolbox</em>. For example, hidden Markov models, mixture of Gaussians, and logistic regression are all examples from a language of models. We will try to show a larger family, and point out common special cases.</p>
<p>Using this language, you will be able to build you own custom models.</p>
<p>In fact, we can talk about this family of models using very few ideas. Really, all we need are <em>deep probabilistic latent-variable models</em> and some <em>decision theory</em>.</p>
<table>
<thead>
<tr>
<th>Russel and Norvig’s parts of AI</th>
<th>Extensible family</th>
</tr>
</thead>
<tbody>
<tr>
<td>Machine learning</td>
<td></td>
</tr>
<tr>
<td>Natural language processing</td>
<td></td>
</tr>
<tr>
<td>Knowledge representation</td>
<td>Deep probabilistic latent-variable models + decision theory</td>
</tr>
<tr>
<td>Automated reasoning</td>
<td></td>
</tr>
<tr>
<td>Computer vision</td>
<td></td>
</tr>
<tr>
<td>Robotics</td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="advantages-of-probabilistic-latent-variable-models">Advantages of probabilistic latent-variable models<a class="headerlink" href="#advantages-of-probabilistic-latent-variable-models" title="Permanent link">&para;</a></h4>
<p><strong>Data-efficient Learning</strong></p>
<ul>
<li>Automatic regularization, can take advantage of more information.</li>
<li>Model is aware what it doesn’t know.</li>
<li>E.g. after many cat images model updates get small. Showing a dog image for the first time would lead to large updates. </li>
</ul>
<p><strong>Compos-able models</strong></p>
<ul>
<li>Models are built like lego blocks.</li>
<li>E.g. you could incorporate a data corruption model.</li>
</ul>
<p><strong>Handle Missing &amp; Corrupted Data</strong></p>
<ul>
<li>Latent-variable models can easily handle missing and corrupted data, without the standard hacks of guessing missing values using averages.</li>
</ul>
<p><strong>Predictive Uncertainty</strong></p>
<ul>
<li>Necessary for decision-making.</li>
<li>Bad models can confidently give us bad predictions. So can good modals! The key is to be able to express uncertainty.</li>
</ul>
<p><strong>Conditional Predictions</strong></p>
<ul>
<li>Be able to condition predictions can be powerful.</li>
<li>E.g. if brexit happens, the value of the pound will fall.</li>
</ul>
<p><strong>Active Learning</strong></p>
<ul>
<li>What data would be expected to increase our confidence about a prediction.</li>
</ul>
<h4 id="disadvantages-of-probabilistic-latent-variable-models">Disadvantages of Probabilistic Latent-variable Models<a class="headerlink" href="#disadvantages-of-probabilistic-latent-variable-models" title="Permanent link">&para;</a></h4>
<p>Intractable integral over latent variables. Integrating over many dimensions is difficult and sometimes intractable.</p>
<h4 id="probabilistic-graphical-models-vs-neural-networks">Probabilistic Graphical Models Vs. Neural Networks<a class="headerlink" href="#probabilistic-graphical-models-vs-neural-networks" title="Permanent link">&para;</a></h4>
<p>Imagine we had the following data</p>
<p><img alt="" src="../../img/lecture_1_10.png" /></p>
<p>we may try to model this data by fitting a mixture of Gaussians, as so</p>
<p><img alt="" src="../../img/lecture_1_11.png" /></p>
<p>which seems perfectly reasonable in this case. However, imagine instead we had the following data</p>
<p><img alt="" src="../../img/lecture_1_12.png" /></p>
<p>the use of a mixture model may not be appropriate in this case, as it fits the data poorly and reports too many clusters</p>
<p><img alt="" src="../../img/lecture_1_13.png" /></p>
<p>but a neural network who's job is to come up with a convincing distribution over where you can expect to see data does much better</p>
<p><img alt="" src="../../img/lecture_1_14.png" /></p>
<p>this brings us to a comparison of probabilistic graphical models and deep learning</p>
<table>
<thead>
<tr>
<th>Probabilistic graphical models</th>
<th>Deep learning</th>
</tr>
</thead>
<tbody>
<tr>
<td>➕ structured representations</td>
<td>➖ neural net "goo"</td>
</tr>
<tr>
<td>➕ priors and uncertainty</td>
<td>➖ difficult parameterization</td>
</tr>
<tr>
<td>➕ data and computational efficiency</td>
<td>➖ can require lots of data</td>
</tr>
<tr>
<td>➖ rigid assumptions may not fit</td>
<td>➕ flexible</td>
</tr>
<tr>
<td>➖ feature engineering</td>
<td>➕ feature learning</td>
</tr>
<tr>
<td>➖top-down inference</td>
<td>➕ recognition networks</td>
</tr>
</tbody>
</table>
<p>❗ Left off on slide 35</p>
<h2 id="the-unreasonable-easiness-of-deep-learning">The Unreasonable Easiness of Deep Learning<a class="headerlink" href="#the-unreasonable-easiness-of-deep-learning" title="Permanent link">&para;</a></h2>
<p>The deep learning recipe involves defining an <strong>objective function</strong> (i.e. a probability of data given parameters) and optimizing the parameters to maximize the object. Gradients are computed automatically, you just need to define a model by some computation.</p>
<p><img alt="" src="../../img/lecture_1_15.png" /></p>
<h3 id="differentiable-models">Differentiable models<a class="headerlink" href="#differentiable-models" title="Permanent link">&para;</a></h3>
<p>Differentiable models, in general, follow these general principals</p>
<p><strong>1. Model Distributions implicitly by a variable pushed through a deep net</strong></p>
<p>\[y = f_{\theta}(x) ; x \sim \mathcal N(0, I) \]</p>
<p><strong>2. Approximate intractable distribution by a tractable distribution parameterized by a deep net</strong></p>
<p>\[p(y | x) = \mathcal N(y | \mu = f_{\theta}(x), \Sigma = g_{\theta}(x))]  ; x \sim \mathcal N(0, I) \]</p>
<p><strong>3. Optimize all parameters using stochastic gradient descent</strong></p>
<h3 id="modeling-idea">Modeling Idea<a class="headerlink" href="#modeling-idea" title="Permanent link">&para;</a></h3>
<p>Graphical models and neural networks have complimentary strengths, and can be combined. One such way to combine these models is by using <em>structured prior distributions formulated as graphical models</em> with <em>highly nonlinear observation models implemented using neural networks</em>.</p>
<p><img alt="" src="../../img/lecture_1_16.png" /></p>
<p>By pushing these structured prior distributions through a neural network</p>
<p><img alt="" src="../../img/lecture_1_17.gif" /></p>
<p>we can get a model which takes exploits the best of both worlds</p>
<p><img alt="" src="../../img/lecture_1_18.png" /></p>
<p>This idea can be extended to supervised or unsupervised learning</p>
<p><img alt="" src="../../img/lecture_1_19.png" /></p>
<h2 id="learning-outcomes">Learning Outcomes<a class="headerlink" href="#learning-outcomes" title="Permanent link">&para;</a></h2>
<ul>
<li>Know standard algorithms (bag of tricks), when to use them, and their limitations. For basic applications and baselines.</li>
<li>Know main elements of language of deep probabilistic models (bag of bricks: distributions, expectations, latent variables, neural networks) and how to combine them. For custom applications + research.</li>
<li>Know standard computational tools (Monte Carlo, Stochastic optimization, regularization, automatic differentiation). For fitting models.</li>
</ul>
<h3 id="tentative-list-of-topics">Tentative List of Topics<a class="headerlink" href="#tentative-list-of-topics" title="Permanent link">&para;</a></h3>
<ul>
<li>Linear methods for regression + classification</li>
<li>Bayesian linear regression</li>
<li>Probabilistic Generative and Discriminative models - Regularization methods</li>
<li>Stochastic Optimization and Neural Networks</li>
<li>Graphical model notation and exact inference</li>
<li>Mixture Models, Bayesian Networks</li>
<li>Model Comparison and marginal likelihood</li>
<li>Stochastic Variational Inference</li>
<li>Time series and recurrent models</li>
<li>Gaussian processes</li>
<li>Variational Autoencoders</li>
</ul>
<h3 id="machine-learning-centric-history-of-probabilistic-models">Machine-learning-centric History of Probabilistic Models<a class="headerlink" href="#machine-learning-centric-history-of-probabilistic-models" title="Permanent link">&para;</a></h3>
<ul>
<li>1940s - 1960s Motivating probability and Bayesian inference</li>
<li>1980s - 2000s Bayesian machine learning with MCMC</li>
<li>1990s - 2000s Graphical models with exact inference</li>
<li>1990s - present Bayesian Nonparametrics with MCMC (Indian Buffet</li>
<li>process, Chinese restaurant process)</li>
<li>1990s - 2000s Bayesian ML with mean-field variational inference 2000s - present Probabilistic Programming</li>
<li>2000s - 2013 Deep undirected graphical models (RBMs, pretraining) 2010s - present Stan - Bayesian Data Analysis with HMC</li>
<li>2000s - 2013 Autoencoders, denoising autoencoders</li>
<li>2000s - present Invertible density estimation</li>
<li>2013 - present Stochastic variational inference, variational autoencoders</li>
<li>2014 - present Generative adversarial nets, Real NVP, Pixelnet</li>
<li>2016 - present Lego-style deep generative models (attend, infer, repeat)</li>
</ul>
<h2 id="appendix">Appendix<a class="headerlink" href="#appendix" title="Permanent link">&para;</a></h2>
<h3 id="useful-resources">Useful Resources<a class="headerlink" href="#useful-resources" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/">Blog + tutorial</a> on matrix factorization for movie recommendation.</li>
<li><a href="https://blog.openai.com/glow/">Glow</a> an interactive OpenAI blog on Generative Models.</li>
<li>It appears that a few of these slides were taken straight from <a href="https://www.youtube.com/watch?v=btr1poCYIzw">this</a> video.</li>
<li><a href="https://en.wikipedia.org/wiki/Notation_in_probability_and_statistics">Summary of notation in probability and statistics</a>.</li>
</ul>
<h3 id="glossary-of-terms">Glossary of Terms<a class="headerlink" href="#glossary-of-terms" title="Permanent link">&para;</a></h3>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../.." title="About" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                About
              </span>
            </div>
          </a>
        
        
          <a href="../week_2/" title="Week 2" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Week 2
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
  <div class="md-footer-social">
    <link rel="stylesheet" href="../../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/JohnGiorgi" class="md-footer-social__link fa fa-github"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/application.b41f3d20.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"../.."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
    
      
    
  </body>
</html>