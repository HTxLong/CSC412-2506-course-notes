



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="https://johngiorgi.github.io/CSC412-2506-course-notes/. /lectures/week_8/">
      
      
        <meta name="author" content="John Giorgi">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.1.0">
    
    
      
        <title>Week 8 - CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/application.982221ab.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../../assets/javascripts/modernizr.01ccdecf.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="../../styles/extra.css">
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#week-8-sampling-and-monte-carlo-methods" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://johngiorgi.github.io/CSC412-2506-course-notes/. " title="CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning
            </span>
            <span class="md-header-nav__topic">
              Week 8
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    JohnGiorgi/CSC412-2506-course-notes
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://johngiorgi.github.io/CSC412-2506-course-notes/. " title="CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    JohnGiorgi/CSC412-2506-course-notes
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../.." title="About" class="md-nav__link">
      About
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      Lectures
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Lectures
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../week_1/" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_2/" title="Week 2" class="md-nav__link">
      Week 2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_3/" title="Week 3" class="md-nav__link">
      Week 3
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_4/" title="Week 4" class="md-nav__link">
      Week 4
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_5/" title="Week 5" class="md-nav__link">
      Week 5
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_6/" title="Week 6" class="md-nav__link">
      Week 6
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Week 8
      </label>
    
    <a href="./" title="Week 8" class="md-nav__link md-nav__link--active">
      Week 8
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#assigned-reading" title="Assigned Reading" class="md-nav__link">
    Assigned Reading
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overview" title="Overview" class="md-nav__link">
    Overview
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sampling" title="Sampling" class="md-nav__link">
    Sampling
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-problems-to-be-solved" title="The problems to be solved" class="md-nav__link">
    The problems to be solved
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simple-example" title="Simple example" class="md-nav__link">
    Simple example
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simple-monte-carlo" title="Simple Monte Carlo" class="md-nav__link">
    Simple Monte Carlo
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#properties-of-mc" title="Properties of MC" class="md-nav__link">
    Properties of MC
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sampling-px-is-hard" title="Sampling p(x) is hard" class="md-nav__link">
    Sampling p(x) is hard
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bad-idea-lattice-discretization" title="Bad Idea: Lattice Discretization" class="md-nav__link">
    Bad Idea: Lattice Discretization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-useful-analogy" title="A useful analogy" class="md-nav__link">
    A useful analogy
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#monte-carlo-methods" title="Monte Carlo Methods" class="md-nav__link">
    Monte Carlo Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#importance-sampling" title="Importance Sampling" class="md-nav__link">
    Importance Sampling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rejection-sampling" title="Rejection Sampling" class="md-nav__link">
    Rejection Sampling
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rejection-sampling-in-many-dimensions" title="Rejection sampling in many dimensions" class="md-nav__link">
    Rejection sampling in many dimensions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#metropolis-hastings-method" title="Metropolis-Hastings method" class="md-nav__link">
    Metropolis-Hastings method
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_9/" title="Week 9" class="md-nav__link">
      Week 9
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_10/" title="Week 10" class="md-nav__link">
      Week 10
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_11/" title="Week 11" class="md-nav__link">
      Week 11
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_12.md" title="Week 12" class="md-nav__link">
      Week 12
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_13.md" title="Week 13" class="md-nav__link">
      Week 13
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Tutorials
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Tutorials
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/week_1/" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../sample_midterm_answers/" title="Sample Midterm (Answers)" class="md-nav__link">
      Sample Midterm (Answers)
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#assigned-reading" title="Assigned Reading" class="md-nav__link">
    Assigned Reading
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overview" title="Overview" class="md-nav__link">
    Overview
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sampling" title="Sampling" class="md-nav__link">
    Sampling
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-problems-to-be-solved" title="The problems to be solved" class="md-nav__link">
    The problems to be solved
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simple-example" title="Simple example" class="md-nav__link">
    Simple example
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simple-monte-carlo" title="Simple Monte Carlo" class="md-nav__link">
    Simple Monte Carlo
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#properties-of-mc" title="Properties of MC" class="md-nav__link">
    Properties of MC
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sampling-px-is-hard" title="Sampling p(x) is hard" class="md-nav__link">
    Sampling p(x) is hard
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bad-idea-lattice-discretization" title="Bad Idea: Lattice Discretization" class="md-nav__link">
    Bad Idea: Lattice Discretization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-useful-analogy" title="A useful analogy" class="md-nav__link">
    A useful analogy
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#monte-carlo-methods" title="Monte Carlo Methods" class="md-nav__link">
    Monte Carlo Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#importance-sampling" title="Importance Sampling" class="md-nav__link">
    Importance Sampling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rejection-sampling" title="Rejection Sampling" class="md-nav__link">
    Rejection Sampling
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rejection-sampling-in-many-dimensions" title="Rejection sampling in many dimensions" class="md-nav__link">
    Rejection sampling in many dimensions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#metropolis-hastings-method" title="Metropolis-Hastings method" class="md-nav__link">
    Metropolis-Hastings method
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/edit/master/docs/lectures/week_8.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="week-8-sampling-and-monte-carlo-methods">Week 8:  Sampling and Monte Carlo Methods<a class="headerlink" href="#week-8-sampling-and-monte-carlo-methods" title="Permanent link">&para;</a></h1>
<h3 id="assigned-reading">Assigned Reading<a class="headerlink" href="#assigned-reading" title="Permanent link">&para;</a></h3>
<ul>
<li>MacKay: Chapter 29</li>
</ul>
<h3 id="overview">Overview<a class="headerlink" href="#overview" title="Permanent link">&para;</a></h3>
<ul>
<li>Simple Monte Carlo</li>
<li>Importance Sampling</li>
<li>Rejection Sampling</li>
<li>Metropolis Hastings</li>
<li>Gibbs</li>
<li>Properties of Markov Chains</li>
</ul>
<p>We are going to put a pause on variational methods and return to them in a few weeks. Today we will talk about <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)"><strong>sampling</strong></a>: ways to compute the joint probability in a tractable way.</p>
<h2 id="sampling">Sampling<a class="headerlink" href="#sampling" title="Permanent link">&para;</a></h2>
<p>We will use the word "<em>sample</em>" in the following sense: a sample from a distribution <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> is a single realization <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> whose probability distribution is <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span>. This contrasts with the alternative usage in statistics, where <em>sample</em> refers to a collection of realizations <span><span class="MathJax_Preview">{x}</span><script type="math/tex">{x}</script></span>.</p>
<p>Recall from last week that we assume the density from which we wish to draw samples, <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span>, can be evaluated to within a multiplicative constant. That is, we can evaluate a function <span><span class="MathJax_Preview">\tilde p(x)</span><script type="math/tex">\tilde p(x)</script></span> such that</p>
<div>
<div class="MathJax_Preview">
p(x) = \frac{\tilde p(x)}{Z}
</div>
<script type="math/tex; mode=display">
p(x) = \frac{\tilde p(x)}{Z}
</script>
</div>
<h2 id="the-problems-to-be-solved">The problems to be solved<a class="headerlink" href="#the-problems-to-be-solved" title="Permanent link">&para;</a></h2>
<p><a href="https://en.wikipedia.org/wiki/Monte_Carlo_method"><strong>Monte Carlo methods</strong></a> are computational techniques that make use of random numbers. The aims of Monte Carlo methods are to solve one or both of the following problems.</p>
<p><strong>Problem 1</strong>: To generate samples <span><span class="MathJax_Preview">\{x^{(r)}\}^R_{r=1}</span><script type="math/tex">\{x^{(r)}\}^R_{r=1}</script></span> from a given probability distribution <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span>.</p>
<p><strong>Problem 2</strong>: To estimate expectations of functions, <span><span class="MathJax_Preview">\phi(x)</span><script type="math/tex">\phi(x)</script></span>, under this distribution, <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span></p>
<div>
<div class="MathJax_Preview">
\Phi = \mathbb E_{x \sim p(x)}[\phi(x)] = \int \phi(x)p(x)dx
</div>
<script type="math/tex; mode=display">
\Phi = \mathbb E_{x \sim p(x)}[\phi(x)] = \int \phi(x)p(x)dx
</script>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><span><span class="MathJax_Preview">\phi</span><script type="math/tex">\phi</script></span> is unrelated to the factors we talked about in week 6; here it is just some function we want to compute the expectation of.</p>
</div>
<h3 id="simple-example">Simple example<a class="headerlink" href="#simple-example" title="Permanent link">&para;</a></h3>
<p>Simple examples of functions <span><span class="MathJax_Preview">\phi(x)</span><script type="math/tex">\phi(x)</script></span> whose expectations we might be interested in include the first and second moments of quantities that we wish to predict, from which we can compute means and variances; for example if some quantity <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> depends on <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>, we can find the mean and variance of <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> under <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> by finding the expectations of the functions <span><span class="MathJax_Preview">\phi_1(x) = t(x)</span><script type="math/tex">\phi_1(x) = t(x)</script></span> and <span><span class="MathJax_Preview">\phi_2(x) = (t(x))^2</span><script type="math/tex">\phi_2(x) = (t(x))^2</script></span></p>
<div>
<div class="MathJax_Preview">
\phi_1(x) = t(x) \Rightarrow \Phi_1 = \mathbb E_{x \sim p(x)}[\phi_1(x)] \Rightarrow \text{mean}(t) = \Phi_1 \\
\phi_2(x) = (t(x))^2 \Rightarrow \Phi_2 = \mathbb E_{x \sim p(x)}[\phi_2(x)] \Rightarrow \text{var}(t) = \Phi_2 - (\Phi_2)^2 \\
</div>
<script type="math/tex; mode=display">
\phi_1(x) = t(x) \Rightarrow \Phi_1 = \mathbb E_{x \sim p(x)}[\phi_1(x)] \Rightarrow \text{mean}(t) = \Phi_1 \\
\phi_2(x) = (t(x))^2 \Rightarrow \Phi_2 = \mathbb E_{x \sim p(x)}[\phi_2(x)] \Rightarrow \text{var}(t) = \Phi_2 - (\Phi_2)^2 \\
</script>
</div>
<h3 id="simple-monte-carlo">Simple Monte Carlo<a class="headerlink" href="#simple-monte-carlo" title="Permanent link">&para;</a></h3>
<p>We will concentrate on the first problem (sampling), because if we have solved it, then we can solve the second problem by using the random samples <span><span class="MathJax_Preview">\{x^{(r)}\}^R_{r=1}</span><script type="math/tex">\{x^{(r)}\}^R_{r=1}</script></span> to give an estimator.</p>
<p><em>def</em>. <strong>Simple Monte Carlo</strong>: Given <span><span class="MathJax_Preview">\{x^{(r)}\}^R_{r=1} \sim p(x)</span><script type="math/tex">\{x^{(r)}\}^R_{r=1} \sim p(x)</script></span> we estimate the expectation <span><span class="MathJax_Preview">\mathbb E_{x \sim p(x)}[\phi(x)]</span><script type="math/tex">\mathbb E_{x \sim p(x)}[\phi(x)]</script></span> to be the estimator <span><span class="MathJax_Preview">\hat \Phi</span><script type="math/tex">\hat \Phi</script></span></p>
<div>
<div class="MathJax_Preview">
\Phi = \mathbb E_{x \sim p(x)}[\phi(x)] \approx \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) = \hat \Phi
</div>
<script type="math/tex; mode=display">
\Phi = \mathbb E_{x \sim p(x)}[\phi(x)] \approx \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) = \hat \Phi
</script>
</div>
<h4 id="properties-of-mc">Properties of MC<a class="headerlink" href="#properties-of-mc" title="Permanent link">&para;</a></h4>
<p>If the vectors <span><span class="MathJax_Preview">\{x^{(r)}\}^R_{r=1}</span><script type="math/tex">\{x^{(r)}\}^R_{r=1}</script></span> are generated from <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> then the expectation of <span><span class="MathJax_Preview">\hat \Phi</span><script type="math/tex">\hat \Phi</script></span> is <span><span class="MathJax_Preview">\Phi</span><script type="math/tex">\Phi</script></span>. E.g. <span><span class="MathJax_Preview">\hat \Phi</span><script type="math/tex">\hat \Phi</script></span> is an <a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">unbiased estimator</a> of <span><span class="MathJax_Preview">\Phi</span><script type="math/tex">\Phi</script></span>.</p>
<p><em>Proof</em></p>
<div>
<div class="MathJax_Preview">
\mathbb E [\hat \Phi]_{x \sim p(\{x^{(r)}\}^R_{r=1})} = \mathbb E \bigg [ \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \bigg ] \\
= \frac{1}{R} \sum_{r=1}^R \mathbb E \big [ \phi(x^{(r)}) \big ]  \\
= \frac{1}{R} \sum_{r=1}^R \mathbb E_{x \sim p(x)} \big [ \phi(x) \big ] \\
= \frac{R}{R} \mathbb E_{x \sim p(x)} \big [ \phi(x) \big ] \\
= \Phi \quad \square
</div>
<script type="math/tex; mode=display">
\mathbb E [\hat \Phi]_{x \sim p(\{x^{(r)}\}^R_{r=1})} = \mathbb E \bigg [ \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \bigg ] \\
= \frac{1}{R} \sum_{r=1}^R \mathbb E \big [ \phi(x^{(r)}) \big ]  \\
= \frac{1}{R} \sum_{r=1}^R \mathbb E_{x \sim p(x)} \big [ \phi(x) \big ] \\
= \frac{R}{R} \mathbb E_{x \sim p(x)} \big [ \phi(x) \big ] \\
= \Phi \quad \square
</script>
</div>
<p>As the number of samples of <span><span class="MathJax_Preview">R</span><script type="math/tex">R</script></span> increases, the variance of <span><span class="MathJax_Preview">\hat \Phi</span><script type="math/tex">\hat \Phi</script></span> will decrease proportional to <span><span class="MathJax_Preview">\frac{1}{R}</span><script type="math/tex">\frac{1}{R}</script></span></p>
<p><em>Proof</em></p>
<div>
<div class="MathJax_Preview">
\text{var}[\hat \Phi] = \text{var} \bigg [ \frac{1}{R}\sum^R_{r=1}\phi(x^{(r)}) \bigg ] \\
= \frac{1}{R^2} \text{var} \bigg [\sum^R_{r=1}\phi(x^{(r)}) \bigg ]\\
= \frac{1}{R^2} \sum^R_{r=1} \text{var} \bigg [\phi(x^{(r)}) \bigg ] \\
= \frac{R}{R^2} \text{var} [\phi(x) ] \\
= \frac{1}{R} \text{var} [\phi(x) ] \quad \square
</div>
<script type="math/tex; mode=display">
\text{var}[\hat \Phi] = \text{var} \bigg [ \frac{1}{R}\sum^R_{r=1}\phi(x^{(r)}) \bigg ] \\
= \frac{1}{R^2} \text{var} \bigg [\sum^R_{r=1}\phi(x^{(r)}) \bigg ]\\
= \frac{1}{R^2} \sum^R_{r=1} \text{var} \bigg [\phi(x^{(r)}) \bigg ] \\
= \frac{R}{R^2} \text{var} [\phi(x) ] \\
= \frac{1}{R} \text{var} [\phi(x) ] \quad \square
</script>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The accuracy of the Monte Carlo estimate depends only on the variance of <span><span class="MathJax_Preview">\phi</span><script type="math/tex">\phi</script></span>, not on the dimensionality of the <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>. So regardless of the dimensionality of <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>, it may be that as few as a dozen independent samples <span><span class="MathJax_Preview">\{x^{(r)}\}</span><script type="math/tex">\{x^{(r)}\}</script></span> suffice to estimate <span><span class="MathJax_Preview">\phi</span><script type="math/tex">\phi</script></span> satisfactorily.</p>
</div>
<h4 id="sampling-px-is-hard">Sampling <em>p(x)</em> is hard<a class="headerlink" href="#sampling-px-is-hard" title="Permanent link">&para;</a></h4>
<p>Earlier we said that we will assume we can sample from the density <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> to within a multiplicative constant</p>
<div>
<div class="MathJax_Preview">
p(x) = \frac{\tilde p(x)}{Z}
</div>
<script type="math/tex; mode=display">
p(x) = \frac{\tilde p(x)}{Z}
</script>
</div>
<p>If we can evaluate <span><span class="MathJax_Preview">\tilde p(x)</span><script type="math/tex">\tilde p(x)</script></span>, then why can't we solve problem 1? There are two difficulties</p>
<ol>
<li>We do not typically know the normalizing constant, <span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span></li>
<li>Even if we did know <span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span>, the problem of drawing samples from <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> is still a challenging one, especially in high-dimensional spaces, because there is no obvious way to sample from <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> without enumerating most or all of the possible states.</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Correct samples from <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> will by definition tend to come from places in <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>-space where <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> is big; how can we identify those places where <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> is big, without evaluating <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> everywhere? There are only a few high-dimensional densities from which it is easy to draw samples, for example the Gaussian distribution.</p>
</div>
<p>First, we will build our intuition for why sampling from <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> is difficult. In the next section, will discuss clever Monte Carlo methods can solve them.</p>
<h5 id="bad-idea-lattice-discretization">Bad Idea: Lattice Discretization<a class="headerlink" href="#bad-idea-lattice-discretization" title="Permanent link">&para;</a></h5>
<p>Imagine that we wish to draw samples from the density</p>
<div>
<div class="MathJax_Preview">
p(x) = \frac{\tilde p(x)}{Z}
</div>
<script type="math/tex; mode=display">
p(x) = \frac{\tilde p(x)}{Z}
</script>
</div>
<p>given in figure (a).</p>
<p><img alt="" src="../../img/lecture_7_1.png" /></p>
<p>Just because we can plot this distribution, that does not mean we can draw samples from it. To start, we don't know the normalizing constant, <span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Note the some of these images come from MacKay, who uses a different notation. We use lowercase <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> and <span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span> in place of his upper case <span><span class="MathJax_Preview">P</span><script type="math/tex">P</script></span> and <span><span class="MathJax_Preview">Q</span><script type="math/tex">Q</script></span>, and we use <span><span class="MathJax_Preview">\tilde p</span><script type="math/tex">\tilde p</script></span> / <span><span class="MathJax_Preview">\tilde q</span><script type="math/tex">\tilde q</script></span> in place of his <span><span class="MathJax_Preview">p^*</span><script type="math/tex">p^*</script></span> / <span><span class="MathJax_Preview">q^*</span><script type="math/tex">q^*</script></span>.</p>
</div>
<p>To simplify the problem, we could discretize the variable <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and sample from the discrete probability distribution over a finite set of uniformly spaced points <span><span class="MathJax_Preview">\{x_i\}</span><script type="math/tex">\{x_i\}</script></span> (figure (d)). If we evaluate <span><span class="MathJax_Preview">\tilde p_i = \tilde p(x_i)</span><script type="math/tex">\tilde p_i = \tilde p(x_i)</script></span> at each point <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span>, we could compute</p>
<div>
<div class="MathJax_Preview">
Z = \sum_i \tilde p_i
</div>
<script type="math/tex; mode=display">
Z = \sum_i \tilde p_i
</script>
</div>
<p>and</p>
<div>
<div class="MathJax_Preview">
\tilde p_i = \frac{\tilde p_i}{Z}
</div>
<script type="math/tex; mode=display">
\tilde p_i = \frac{\tilde p_i}{Z}
</script>
</div>
<p>and could sample from the probability distribution <span><span class="MathJax_Preview">\{p_i\}_{i=1}^R</span><script type="math/tex">\{p_i\}_{i=1}^R</script></span> using various methods based on a source of random bits. Unfortunately, the cost of this procedure is intractable.</p>
<p>To evaluate <span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span>, we must visit every point in the space. In figure (b) there are <span><span class="MathJax_Preview">50</span><script type="math/tex">50</script></span> uniformly spaced points in one dimension. If our system had, <span><span class="MathJax_Preview">N=1000</span><script type="math/tex">N=1000</script></span> dimensions say, then the corresponding number of points would be <span><span class="MathJax_Preview">50^{D} = 50^{1000}</span><script type="math/tex">50^{D} = 50^{1000}</script></span>. Even if each component <span><span class="MathJax_Preview">x_n</span><script type="math/tex">x_n</script></span> took only two discrete values, the number of evaluations of <span><span class="MathJax_Preview">\tilde p</span><script type="math/tex">\tilde p</script></span> needed to evaluate <span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> would take many times longer than the age of the universe.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><strong>TL;DR</strong> The cost of this lattice discretization method of sampling from <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> is exponential in the dimension of our data (e.g. <span><span class="MathJax_Preview">D^N</span><script type="math/tex">D^N</script></span> where <span><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span> is the number of data points and <span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> their dimension).</p>
</div>
<h5 id="a-useful-analogy">A useful analogy<a class="headerlink" href="#a-useful-analogy" title="Permanent link">&para;</a></h5>
<p>Imagine the tasks of drawing random water samples from a lake and finding the average plankton concentration (figure 29.2). Let</p>
<ul>
<li><span><span class="MathJax_Preview">\tilde p({\bf x})</span><script type="math/tex">\tilde p({\bf x})</script></span> = the depth of the lake at <span><span class="MathJax_Preview">{\bf x} = (x, y)</span><script type="math/tex">{\bf x} = (x, y)</script></span></li>
<li><span><span class="MathJax_Preview">\phi({\bf x})</span><script type="math/tex">\phi({\bf x})</script></span> = the plankton concentration as a function of <span><span class="MathJax_Preview">{\bf x}</span><script type="math/tex">{\bf x}</script></span></li>
<li><span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> = the volume of the lake = <span><span class="MathJax_Preview">\int \tilde p({\bf x }) d{\bf x }</span><script type="math/tex">\int \tilde p({\bf x }) d{\bf x }</script></span></li>
</ul>
<p><img alt="" src="../../img/lecture_7_2.png" /></p>
<p>The average concentration of plankton is therefore</p>
<div>
<div class="MathJax_Preview">
\Phi = \frac{1}{Z} \int \phi({\bf x }) \tilde p({\bf x }) d{\bf x }
</div>
<script type="math/tex; mode=display">
\Phi = \frac{1}{Z} \int \phi({\bf x }) \tilde p({\bf x }) d{\bf x }
</script>
</div>
<p>Say you can take the boat to any desired location <span><span class="MathJax_Preview">{\bf x }</span><script type="math/tex">{\bf x }</script></span> on the lake, and can measure the depth, <span><span class="MathJax_Preview">\tilde p({\bf x })</span><script type="math/tex">\tilde p({\bf x })</script></span>, and plankton concentration, <span><span class="MathJax_Preview">\phi({\bf x})</span><script type="math/tex">\phi({\bf x})</script></span>, at that point. Therefore,</p>
<ul>
<li><strong>Problem 1</strong> is to draw water samples at random from the lake, in such a way that each sample is equally likely to come from any point within the lake.</li>
<li><strong>Problem 2</strong> is to find the average plankton concentration.</li>
</ul>
<p><img alt="" src="../../img/lecture_7_3.png" /></p>
<p>These are difficult problems to solve because at the outset we know nothing about the depth <span><span class="MathJax_Preview">\tilde p({\bf x })</span><script type="math/tex">\tilde p({\bf x })</script></span>. Perhaps much of the volume of the lake is contained in narrow, deep underwater canyons (figure 29.3), in which case, to correctly sample from the lake and correctly estimate <span><span class="MathJax_Preview">\Phi</span><script type="math/tex">\Phi</script></span> our method must implicitly discover the canyons and find their volume relative to the rest of the lake.</p>
<h2 id="monte-carlo-methods">Monte Carlo Methods<a class="headerlink" href="#monte-carlo-methods" title="Permanent link">&para;</a></h2>
<h3 id="importance-sampling">Importance Sampling<a class="headerlink" href="#importance-sampling" title="Permanent link">&para;</a></h3>
<p><a href="https://en.wikipedia.org/wiki/Importance_sampling"><strong>Importance sampling</strong></a> is not a method for generating samples from <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> (<strong>Problem 1</strong>); it is just a method for estimating the expectation of a function <span><span class="MathJax_Preview">\phi(x)</span><script type="math/tex">\phi(x)</script></span> (<strong>Problem 2</strong>).</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Importance sampling can be viewed as a generalization of the <strong>uniform sampling</strong> method, something we did not discuss in class but is discussed in MacKay: Chapter 29.</p>
</div>
<p>We begin with the same assumption we have made earlier; the density from which we wish to draw samples, <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span>, can be evaluated to within a multiplicative constant. That is, we can evaluate a function <span><span class="MathJax_Preview">\tilde p(x)</span><script type="math/tex">\tilde p(x)</script></span> such that</p>
<div>
<div class="MathJax_Preview">
p(x) = \frac{\tilde p(x)}{Z}
</div>
<script type="math/tex; mode=display">
p(x) = \frac{\tilde p(x)}{Z}
</script>
</div>
<p>We further assume we have a simpler density, <span><span class="MathJax_Preview">q(x)</span><script type="math/tex">q(x)</script></span> from which it is easy to <em>sample</em> from (i.e. <span><span class="MathJax_Preview">x \sim q(x)</span><script type="math/tex">x \sim q(x)</script></span>) and easy to <em>evaluate</em> (i.e. <span><span class="MathJax_Preview">\tilde q(x)</span><script type="math/tex">\tilde q(x)</script></span>)</p>
<div>
<div class="MathJax_Preview">
q(x) = \frac{\tilde q(x)}{Z_Q}
</div>
<script type="math/tex; mode=display">
q(x) = \frac{\tilde q(x)}{Z_Q}
</script>
</div>
<p>we call such a density <span><span class="MathJax_Preview">q(x)</span><script type="math/tex">q(x)</script></span> the <strong>sampler density</strong>. An example of the functions <span><span class="MathJax_Preview">\tilde p, \tilde q</span><script type="math/tex">\tilde p, \tilde q</script></span> and <span><span class="MathJax_Preview">\phi</span><script type="math/tex">\phi</script></span> is given in in figure 29.5.</p>
<p><img alt="" src="../../img/lecture_7_4.png" /></p>
<p>In importance sampling, we generate <span><span class="MathJax_Preview">R</span><script type="math/tex">R</script></span> samples from <span><span class="MathJax_Preview">q(x)</span><script type="math/tex">q(x)</script></span></p>
<div>
<div class="MathJax_Preview">
\{x^{(r)}\}^R_{r=1} \sim q(x)
</div>
<script type="math/tex; mode=display">
\{x^{(r)}\}^R_{r=1} \sim q(x)
</script>
</div>
<p>If these points were samples from <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> then we could estimate <span><span class="MathJax_Preview">\Phi</span><script type="math/tex">\Phi</script></span> by</p>
<div>
<div class="MathJax_Preview">
\Phi = \mathbb E_{x \sim p(x)}[\phi(x)] \approx \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) = \hat \Phi
</div>
<script type="math/tex; mode=display">
\Phi = \mathbb E_{x \sim p(x)}[\phi(x)] \approx \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) = \hat \Phi
</script>
</div>
<p>But when we generate samples from <span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span>, values of <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> where <span><span class="MathJax_Preview">q(x)</span><script type="math/tex">q(x)</script></span> is greater than <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> will be <em>over-represented</em> in this estimator, and points where <span><span class="MathJax_Preview">q(x)</span><script type="math/tex">q(x)</script></span> is less than <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> will be <em>under-represented</em>. To take into account the fact that we have sampled from the wrong distribution, we introduce <em>weights</em>.</p>
<div>
<div class="MathJax_Preview">
\tilde w_r = \frac{\tilde p(x^{(r)})}{\tilde q(x^{(r)})}
</div>
<script type="math/tex; mode=display">
\tilde w_r = \frac{\tilde p(x^{(r)})}{\tilde q(x^{(r)})}
</script>
</div>
<p>Finally, we rewrite our estimator under <span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span></p>
<div>
<div class="MathJax_Preview">
\Phi = \int \phi(x)p(x)dx\\
= \int \phi(x) \cdot \frac{p(x)}{q(x)} \cdot q(x)dx \\
\approx \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)})\frac{p(x^{(r)})}{q(x^{(r)})}
</div>
<script type="math/tex; mode=display">
\Phi = \int \phi(x)p(x)dx\\
= \int \phi(x) \cdot \frac{p(x)}{q(x)} \cdot q(x)dx \\
\approx \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)})\frac{p(x^{(r)})}{q(x^{(r)})}
</script>
</div>
<p>however, the estimator as written still relies on <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span>, we want an estimator that relies on <span><span class="MathJax_Preview">\tilde p(x)</span><script type="math/tex">\tilde p(x)</script></span></p>
<div>
<div class="MathJax_Preview">
= \frac{Z_q}{Z_p}\int \phi(x) \cdot \frac{\tilde p(x)}{\tilde q(x)} \cdot q(x) dx \\
\approx \frac{Z_q}{Z_p} \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot \tilde w_r \\
= \frac{\frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot  \tilde w_r}{\frac{1}{R}\sum_{r=1}^R \tilde w_r} \\
= \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot  w_r \\
= \hat \Phi_{iw}
</div>
<script type="math/tex; mode=display">
= \frac{Z_q}{Z_p}\int \phi(x) \cdot \frac{\tilde p(x)}{\tilde q(x)} \cdot q(x) dx \\
\approx \frac{Z_q}{Z_p} \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot \tilde w_r \\
= \frac{\frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot  \tilde w_r}{\frac{1}{R}\sum_{r=1}^R \tilde w_r} \\
= \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot  w_r \\
= \hat \Phi_{iw}
</script>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>I don't know where the <span><span class="MathJax_Preview">q(x)</span><script type="math/tex">q(x)</script></span> went from line 1 to 2?</p>
</div>
<p>where <span><span class="MathJax_Preview">\frac{Z_p}{Z_q} = \frac{1}{R}\sum_{r=1}^R \tilde w_r</span><script type="math/tex">\frac{Z_p}{Z_q} = \frac{1}{R}\sum_{r=1}^R \tilde w_r</script></span>, <span><span class="MathJax_Preview">w_r = \frac{\tilde w_r}{\sum_{r=1}^R \tilde w_r}</span><script type="math/tex">w_r = \frac{\tilde w_r}{\sum_{r=1}^R \tilde w_r}</script></span> and <span><span class="MathJax_Preview">\hat \Phi_{iw}</span><script type="math/tex">\hat \Phi_{iw}</script></span> is our importance weighted estimator.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span><span class="MathJax_Preview">\hat \Phi_{iw}</span><script type="math/tex">\hat \Phi_{iw}</script></span> is biased, by consistent.</p>
</div>
<h3 id="rejection-sampling">Rejection Sampling<a class="headerlink" href="#rejection-sampling" title="Permanent link">&para;</a></h3>
<p>In <a href="https://en.wikipedia.org/wiki/Rejection_sampling"><strong>rejection sampling</strong></a> we assume again a one-dimensional density <span><span class="MathJax_Preview">p(x) = \tilde p(x)/Z</span><script type="math/tex">p(x) = \tilde p(x)/Z</script></span> that is too complicated a function for us to be able to sample from it directly. We assume that we have a simpler <em>proposal density</em> <span><span class="MathJax_Preview">q(x)</span><script type="math/tex">q(x)</script></span> which we can evaluate (within a multiplicative factor <span><span class="MathJax_Preview">Z_Q</span><script type="math/tex">Z_Q</script></span>, as before), and from which we can generate samples. We further assume that we know the value of a constant <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> such that</p>
<div>
<div class="MathJax_Preview">
c \tilde q(x) &gt; \tilde p(x) \quad \forall x
</div>
<script type="math/tex; mode=display">
c \tilde q(x) > \tilde p(x) \quad \forall x
</script>
</div>
<p>A schematic picture of two such functions is shown below</p>
<p><img alt="" src="../../img/lecture_7_5.png" /></p>
<p>The procedure is as follows:</p>
<ol>
<li>Generate two random numbers.<ol>
<li>The first, <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>, is generated from the proposal density <span><span class="MathJax_Preview">q(x)</span><script type="math/tex">q(x)</script></span>.</li>
<li>The second, <span><span class="MathJax_Preview">u</span><script type="math/tex">u</script></span> is generated uniformly from the interval <span><span class="MathJax_Preview">[0, c \tilde q(x)]</span><script type="math/tex">[0, c \tilde q(x)]</script></span> (see figure (b) above).</li>
</ol>
</li>
<li>Evaluate <span><span class="MathJax_Preview">\tilde p(x)</span><script type="math/tex">\tilde p(x)</script></span> and accept or reject the sample <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> by comparing the value of <span><span class="MathJax_Preview">u</span><script type="math/tex">u</script></span> with the value of <span><span class="MathJax_Preview">\tilde p(x)</span><script type="math/tex">\tilde p(x)</script></span><ol>
<li>If <span><span class="MathJax_Preview">u &gt; \tilde p(x)</span><script type="math/tex">u > \tilde p(x)</script></span>, then <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is rejected</li>
<li>Otherwise <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is accepted; <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is added to our set of samples <span><span class="MathJax_Preview">\{x^{(r)}\}</span><script type="math/tex">\{x^{(r)}\}</script></span> and the value of <span><span class="MathJax_Preview">u</span><script type="math/tex">u</script></span> discarded.</li>
</ol>
</li>
</ol>
<p>Why does this procedure generate samples from <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span>? The proposed point <span><span class="MathJax_Preview">(x, u)</span><script type="math/tex">(x, u)</script></span> comes with uniform probability from the lightly shaded area underneath the curve <span><span class="MathJax_Preview">c \tilde q(x)</span><script type="math/tex">c \tilde q(x)</script></span> as shown in figure (b) above. The rejection rule rejects all the points that lie above the curve <span><span class="MathJax_Preview">\tilde p(x)</span><script type="math/tex">\tilde p(x)</script></span>. So the points <span><span class="MathJax_Preview">(x,u)</span><script type="math/tex">(x,u)</script></span> that are accepted are uniformly distributed in the heavily shaded area under <span><span class="MathJax_Preview">\tilde p(x)</span><script type="math/tex">\tilde p(x)</script></span>. This implies that the probability density of the x-coordinates of the accepted points must be proportional to <span><span class="MathJax_Preview">\tilde p(x)</span><script type="math/tex">\tilde p(x)</script></span>, so the samples must be independent samples from <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Rejection sampling will work best if <span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span> is a good approximation to <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span>. If <span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span> is very different from <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> then, for <span><span class="MathJax_Preview">cq</span><script type="math/tex">cq</script></span> to exceed <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> everywhere, <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> will necessarily have to be large and the frequency of rejection will be large.</p>
</div>
<h4 id="rejection-sampling-in-many-dimensions">Rejection sampling in many dimensions<a class="headerlink" href="#rejection-sampling-in-many-dimensions" title="Permanent link">&para;</a></h4>
<p>In a high-dimensional problem it is very likely that the requirement that <span><span class="MathJax_Preview">c \tilde q</span><script type="math/tex">c \tilde q</script></span> be an upper bound for <span><span class="MathJax_Preview">\tilde p</span><script type="math/tex">\tilde p</script></span> will force <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> to be so huge that acceptances will be very rare indeed. Finding such a value of <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> may be difficult too, since in many problems we know neither where the modes of <span><span class="MathJax_Preview">\tilde p</span><script type="math/tex">\tilde p</script></span> are located nor how high they are.</p>
<p>In general <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> grows exponentially with the dimensionality <span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>, so the acceptance rate is expected to be exponentially small in <span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span></p>
<div>
<div class="MathJax_Preview">
\text{acceptance rate} = \frac{\text{area under } \tilde p}{\text{area under } \tilde q} = \frac{1}{Z}
</div>
<script type="math/tex; mode=display">
\text{acceptance rate} = \frac{\text{area under } \tilde p}{\text{area under } \tilde q} = \frac{1}{Z}
</script>
</div>
<h3 id="metropolis-hastings-method">Metropolis-Hastings method<a class="headerlink" href="#metropolis-hastings-method" title="Permanent link">&para;</a></h3>
<p>Importance sampling and rejection sampling work well only if the proposal density <span><span class="MathJax_Preview">q(x)</span><script type="math/tex">q(x)</script></span> is similar to <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span>. In high dimensions, it is hard to find one such <span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span>.</p>
<p>The <strong>Metropolis–Hastings</strong> algorithm instead makes use of a <em>proposal density</em> <span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span> which depends on the current state <span><span class="MathJax_Preview">x^{(t)}</span><script type="math/tex">x^{(t)}</script></span>. The density <span><span class="MathJax_Preview">q(x' | x^{(t)})</span><script type="math/tex">q(x' | x^{(t)})</script></span> might be a simple distribution such as a Gaussian centered on the current <span><span class="MathJax_Preview">x^{(t)}</span><script type="math/tex">x^{(t)}</script></span>, but in general can be <em>any</em> fixed density from which we can draw samples.</p>
<p>In contrast to importance sampling and rejection sampling, it is not necessary <span><span class="MathJax_Preview">q(x' | x^{(t)})</span><script type="math/tex">q(x' | x^{(t)})</script></span> look at all similar to <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> in order for the algorithm to be practically useful. An example of a proposal density with two different states (<span><span class="MathJax_Preview">x^{(1)}, x^{(2)}</span><script type="math/tex">x^{(1)}, x^{(2)}</script></span>) is shown in figure 29.10.</p>
<p><img alt="" src="../../img/lecture_7_6.png" /></p>
<p>As before, we assume we can evaluate <span><span class="MathJax_Preview">\tilde p(x)</span><script type="math/tex">\tilde p(x)</script></span> for any <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>. The procedure is as follows:</p>
<p>A tentative new state <span><span class="MathJax_Preview">x'</span><script type="math/tex">x'</script></span> is generated from the proposal density <span><span class="MathJax_Preview">q(x' | x^{(t)})</span><script type="math/tex">q(x' | x^{(t)})</script></span>. To decide whether to accept the new state, we compute</p>
<div>
<div class="MathJax_Preview">
a = \frac{\tilde p(x')q(x^{(t)} | x')}{\tilde p(x^{(t)}) q(x' | x^{(t)})}
</div>
<script type="math/tex; mode=display">
a = \frac{\tilde p(x')q(x^{(t)} | x')}{\tilde p(x^{(t)}) q(x' | x^{(t)})}
</script>
</div>
<ul>
<li>If <span><span class="MathJax_Preview">a \ge 1</span><script type="math/tex">a \ge 1</script></span> then the new state is accepted. Set <span><span class="MathJax_Preview">x^{(t + 1)} = x'</span><script type="math/tex">x^{(t + 1)} = x'</script></span>.</li>
<li>Otherwise, the new state is accepted with probability <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span>. Set <span><span class="MathJax_Preview">x^{(t + 1)} = x^{(t)}</span><script type="math/tex">x^{(t + 1)} = x^{(t)}</script></span>.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note the difference from rejection sampling: in rejection sampling, rejected points are discarded and have no influence on the list of samples <span><span class="MathJax_Preview">\{x^{(r)}\}</span><script type="math/tex">\{x^{(r)}\}</script></span> that we collected. Here, a rejection causes the current state to be written again onto the list.</p>
</div>
<p>Metropolis–Hastings converges to <span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> for any <span><span class="MathJax_Preview">q(x' | x^{(t)}) \ge 0 \quad \forall x', x^{(t)}</span><script type="math/tex">q(x' | x^{(t)}) \ge 0 \quad \forall x', x^{(t)}</script></span> as <span><span class="MathJax_Preview">t \rightarrow \infty</span><script type="math/tex">t \rightarrow \infty</script></span>. That is, our list of samples converges towards the true distribution <span><span class="MathJax_Preview">\{x^{(r)}\}_{r=1}^R \rightarrow p(x) </span><script type="math/tex">\{x^{(r)}\}_{r=1}^R \rightarrow p(x) </script></span>.</p>
<p>There are however, no guarantees on convergence. The Metropolis method is an example of a Markov chain Monte Carlo method (abbreviated MCMC). In contrast to rejection sampling, where the accepted points <span><span class="MathJax_Preview">\{x^{(t)}\}</span><script type="math/tex">\{x^{(t)}\}</script></span> are independent samples from the desired distribution, Markov chain Monte Carlo methods involve a Markov process in which a sequence of states <span><span class="MathJax_Preview">\{x^{(t)}\}</span><script type="math/tex">\{x^{(t)}\}</script></span> is generated, each sample <span><span class="MathJax_Preview">x^{(t)}</span><script type="math/tex">x^{(t)}</script></span> having a probability distribution that depends on the previous value, <span><span class="MathJax_Preview">x^{(t-1)}</span><script type="math/tex">x^{(t-1)}</script></span>. Since successive samples are dependent, the Markov chain may have to be run for a considerable time in order to generate samples that are effectively independent samples from <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span>.</p>
<p>Just as it was difficult to estimate the variance of an importance sampling estimator, so it is difficult to assess whether a Markov chain Monte Carlo method has ‘converged’, and to quantify how long one has to wait to obtain samples that are effectively independent samples from <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span>.</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../week_6/" title="Week 6" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Week 6
              </span>
            </div>
          </a>
        
        
          <a href="../week_9/" title="Week 9" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Week 9
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
  <div class="md-footer-social">
    <link rel="stylesheet" href="../../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/JohnGiorgi" class="md-footer-social__link fa fa-github"></a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/application.2ba8dec4.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"../.."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>