



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="https://johngiorgi.github.io/CSC412-2506-course-notes/. /lectures/week_3/">
      
      
        <meta name="author" content="John Giorgi">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.2.0">
    
    
      
        <title>Week 3 - CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/application.572ca0f0.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../../assets/javascripts/modernizr.8c900955.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="../../styles/extra.css">
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#week-3-directed-graphical-models" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://johngiorgi.github.io/CSC412-2506-course-notes/. " title="CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning
              </span>
              <span class="md-header-nav__topic">
                Week 3
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      JohnGiorgi/CSC412-2506-course-notes
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://johngiorgi.github.io/CSC412-2506-course-notes/. " title="CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      JohnGiorgi/CSC412-2506-course-notes
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../.." title="About" class="md-nav__link">
      About
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      Lectures
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Lectures
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../week_1/" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_2/" title="Week 2" class="md-nav__link">
      Week 2
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Week 3
      </label>
    
    <a href="./" title="Week 3" class="md-nav__link md-nav__link--active">
      Week 3
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#assigned-reading" title="Assigned Reading" class="md-nav__link">
    Assigned Reading
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overview" title="Overview" class="md-nav__link">
    Overview
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graphical-model-notation" title="Graphical model notation" class="md-nav__link">
    Graphical model notation
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#conditional-independence" title="Conditional Independence" class="md-nav__link">
    Conditional Independence
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#directed-acyclic-graphical-models-dagm" title="Directed acyclic graphical models (DAGM)" class="md-nav__link">
    Directed acyclic graphical models (DAGM)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#independence-assumptions-on-dagms" title="Independence assumptions on DAGMs" class="md-nav__link">
    Independence assumptions on DAGMs
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#missing-edges" title="Missing Edges" class="md-nav__link">
    Missing Edges
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#d-separation" title="D-Separation" class="md-nav__link">
    D-Separation
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dfs-algorithm-for-checking-independence" title="DFS Algorithm for checking independence" class="md-nav__link">
    DFS Algorithm for checking independence
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bayes-balls-algorithm" title="Bayes-Balls Algorithm" class="md-nav__link">
    Bayes-Balls Algorithm
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#canonical-micrographs" title="Canonical Micrographs" class="md-nav__link">
    Canonical Micrographs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#examples" title="Examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#plates" title="Plates" class="md-nav__link">
    Plates
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#nested-plates" title="Nested Plates" class="md-nav__link">
    Nested Plates
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-of-a-dagm-markov-chain" title="Example of a DAGM: Markov Chain" class="md-nav__link">
    Example of a DAGM: Markov Chain
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#unobserved-variables" title="Unobserved Variables" class="md-nav__link">
    Unobserved Variables
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#partially-unobserved-missing-variables" title="Partially Unobserved (Missing) Variables" class="md-nav__link">
    Partially Unobserved (Missing) Variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#latent-variables" title="Latent variables" class="md-nav__link">
    Latent variables
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#where-do-latent-variables-come-from" title="Where do latent variables come from?" class="md-nav__link">
    Where do latent variables come from?
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixture-models" title="Mixture models" class="md-nav__link">
    Mixture models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mixture-densities" title="Mixture densities" class="md-nav__link">
    Mixture densities
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-gaussian-mixture-models" title="Example: Gaussian Mixture Models" class="md-nav__link">
    Example: Gaussian Mixture Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-mixtures-of-experts" title="Example: Mixtures of Experts" class="md-nav__link">
    Example: Mixtures of Experts
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-mixtures-of-linear-regression-experts" title="Example: Mixtures of Linear Regression Experts" class="md-nav__link">
    Example: Mixtures of Linear Regression Experts
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-learning-with-mixtures" title="Gradient learning with mixtures" class="md-nav__link">
    Gradient learning with mixtures
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hidden-markov-models-hmms" title="Hidden Markov Models (HMMs)" class="md-nav__link">
    Hidden Markov Models (HMMs)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#appendix" title="Appendix" class="md-nav__link">
    Appendix
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#useful-resources" title="Useful Resources" class="md-nav__link">
    Useful Resources
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#glossary-of-terms" title="Glossary of Terms" class="md-nav__link">
    Glossary of Terms
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_4/" title="Week 4" class="md-nav__link">
      Week 4
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_5/" title="Week 5" class="md-nav__link">
      Week 5
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Tutorials
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Tutorials
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/week_1/" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../sample_midterm_answers/" title="Sample Midterm (Answers)" class="md-nav__link">
      Sample Midterm (Answers)
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#assigned-reading" title="Assigned Reading" class="md-nav__link">
    Assigned Reading
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overview" title="Overview" class="md-nav__link">
    Overview
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graphical-model-notation" title="Graphical model notation" class="md-nav__link">
    Graphical model notation
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#conditional-independence" title="Conditional Independence" class="md-nav__link">
    Conditional Independence
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#directed-acyclic-graphical-models-dagm" title="Directed acyclic graphical models (DAGM)" class="md-nav__link">
    Directed acyclic graphical models (DAGM)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#independence-assumptions-on-dagms" title="Independence assumptions on DAGMs" class="md-nav__link">
    Independence assumptions on DAGMs
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#missing-edges" title="Missing Edges" class="md-nav__link">
    Missing Edges
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#d-separation" title="D-Separation" class="md-nav__link">
    D-Separation
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dfs-algorithm-for-checking-independence" title="DFS Algorithm for checking independence" class="md-nav__link">
    DFS Algorithm for checking independence
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bayes-balls-algorithm" title="Bayes-Balls Algorithm" class="md-nav__link">
    Bayes-Balls Algorithm
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#canonical-micrographs" title="Canonical Micrographs" class="md-nav__link">
    Canonical Micrographs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#examples" title="Examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#plates" title="Plates" class="md-nav__link">
    Plates
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#nested-plates" title="Nested Plates" class="md-nav__link">
    Nested Plates
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-of-a-dagm-markov-chain" title="Example of a DAGM: Markov Chain" class="md-nav__link">
    Example of a DAGM: Markov Chain
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#unobserved-variables" title="Unobserved Variables" class="md-nav__link">
    Unobserved Variables
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#partially-unobserved-missing-variables" title="Partially Unobserved (Missing) Variables" class="md-nav__link">
    Partially Unobserved (Missing) Variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#latent-variables" title="Latent variables" class="md-nav__link">
    Latent variables
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#where-do-latent-variables-come-from" title="Where do latent variables come from?" class="md-nav__link">
    Where do latent variables come from?
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixture-models" title="Mixture models" class="md-nav__link">
    Mixture models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mixture-densities" title="Mixture densities" class="md-nav__link">
    Mixture densities
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-gaussian-mixture-models" title="Example: Gaussian Mixture Models" class="md-nav__link">
    Example: Gaussian Mixture Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-mixtures-of-experts" title="Example: Mixtures of Experts" class="md-nav__link">
    Example: Mixtures of Experts
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-mixtures-of-linear-regression-experts" title="Example: Mixtures of Linear Regression Experts" class="md-nav__link">
    Example: Mixtures of Linear Regression Experts
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-learning-with-mixtures" title="Gradient learning with mixtures" class="md-nav__link">
    Gradient learning with mixtures
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hidden-markov-models-hmms" title="Hidden Markov Models (HMMs)" class="md-nav__link">
    Hidden Markov Models (HMMs)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#appendix" title="Appendix" class="md-nav__link">
    Appendix
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#useful-resources" title="Useful Resources" class="md-nav__link">
    Useful Resources
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#glossary-of-terms" title="Glossary of Terms" class="md-nav__link">
    Glossary of Terms
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/edit/master/docs/lectures/week_3.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="week-3-directed-graphical-models">Week 3: Directed Graphical Models<a class="headerlink" href="#week-3-directed-graphical-models" title="Permanent link">&para;</a></h1>
<h3 id="assigned-reading">Assigned Reading<a class="headerlink" href="#assigned-reading" title="Permanent link">&para;</a></h3>
<ul>
<li>Murphy: Chapters 10-12 (excluding * sections)</li>
<li><a href="http://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html">Kevin Murphy's page on graphical models</a></li>
<li><a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec6.pdf">Roger Grosse's slides on backprop</a></li>
</ul>
<h3 id="overview">Overview<a class="headerlink" href="#overview" title="Permanent link">&para;</a></h3>
<ul>
<li>Graphical notations</li>
<li>Conditional independence</li>
<li>Bayes Balls</li>
<li>Latent variables</li>
<li>Common motifs</li>
</ul>
<h2 id="graphical-model-notation">Graphical model notation<a class="headerlink" href="#graphical-model-notation" title="Permanent link">&para;</a></h2>
<p>The joint distribution of <span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> random variables can be computed by the <a href="https://en.wikipedia.org/wiki/Chain_rule_%28probability%29">chain rule</a></p>
<div>
<div class="MathJax_Preview">p(x_{1, ..., N}) = p(x_1)p(x_2|x_1)p(x_3 | x_2, x_1) \ldots </div>
<script type="math/tex; mode=display">p(x_{1, ..., N}) = p(x_1)p(x_2|x_1)p(x_3 | x_2, x_1) \ldots </script>
</div>
<p>this is true for <em>any joint distribution over any random variables</em>.</p>
<p>More formally, in probability the chain rule for two random variables is</p>
<div>
<div class="MathJax_Preview">p(x, y) = p(x | y)p(y)</div>
<script type="math/tex; mode=display">p(x, y) = p(x | y)p(y)</script>
</div>
<p>and for <span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> random variables</p>
<div>
<div class="MathJax_Preview">p(\cap^N_{i=1}) = \prod_{k=1}^N p(x_k | \cap^{k-1}_{j=1} x_j)</div>
<script type="math/tex; mode=display">p(\cap^N_{i=1}) = \prod_{k=1}^N p(x_k | \cap^{k-1}_{j=1} x_j)</script>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that this is a bit of an abuse of notation, but <span><span class="MathJax_Preview">p(x_k | \cap^{k-1}_{j=1} x_j)</span><script type="math/tex">p(x_k | \cap^{k-1}_{j=1} x_j)</script></span> will collpase to <span><span class="MathJax_Preview">p(x_1)</span><script type="math/tex">p(x_1)</script></span> when <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> = 1.</p>
</div>
<p>Graphical, we might represent a model</p>
<div>
<div class="MathJax_Preview">p(x_i, x_{\pi_i}) = p(x_{\pi_i})p(x_i | x_{\pi_i})</div>
<script type="math/tex; mode=display">p(x_i, x_{\pi_i}) = p(x_{\pi_i})p(x_i | x_{\pi_i})</script>
</div>
<p>as</p>
<p><img alt="" src="../../img/lecture_3_1.png" /></p>
<p>where</p>
<ul>
<li>nodes represent random variables</li>
<li>arrows mean "conditioned on", e.g. "<span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> is conditioned on <span><span class="MathJax_Preview">x_{\pi_1}</span><script type="math/tex">x_{\pi_1}</script></span>".</li>
</ul>
<p>For example, the graphical model <span><span class="MathJax_Preview">p(x_{1, ..., 6})</span><script type="math/tex">p(x_{1, ..., 6})</script></span> is represented as</p>
<p><img alt="" src="../../img/lecture_3_2.png" /></p>
<p>This is what the model looks like with <em>no assumptions</em> on the conditional dependence between variables (said otherwise, we assume full conditional dependence of the joint distribution as per the chain rule). This model will <em>scale poorly</em> (exponential with the number of parameters, or <span><span class="MathJax_Preview">k^n</span><script type="math/tex">k^n</script></span> where <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> are states and <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> are random variables, or nodes.).</p>
<p>We can simplify the model by building in our assumptions about the conditional probabilities. More explicitly, a <a href="#directed-acyclic-graphical-models-dagm">directed graphical model</a> implies a restricted factorization of the joint distribution.</p>
<h3 id="conditional-independence">Conditional Independence<a class="headerlink" href="#conditional-independence" title="Permanent link">&para;</a></h3>
<p>Let <span><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> be the set of nodes in our graph (the random variables of our model), then two (sets of) variables <span><span class="MathJax_Preview">X_A</span><script type="math/tex">X_A</script></span>, <span><span class="MathJax_Preview">X_B</span><script type="math/tex">X_B</script></span> are conditionally independent given a third variable <span><span class="MathJax_Preview">X_C</span><script type="math/tex">X_C</script></span></p>
<p>\[(X_A \perp X_B | X_C)\]</p>
<p>if</p>
<div>
<div class="MathJax_Preview">
\Leftrightarrow p(X_A, X_B | X_C) = p(X_A | X_C)p(X_B | X_C) \; (\star)
</div>
<script type="math/tex; mode=display">
\Leftrightarrow p(X_A, X_B | X_C) = p(X_A | X_C)p(X_B | X_C) \; (\star)
</script>
</div>
<div>
<div class="MathJax_Preview">
\Leftrightarrow p(X_A | X_B, X_C) = p(X_A | X_C) \; (\star\star)
</div>
<script type="math/tex; mode=display">
\Leftrightarrow p(X_A | X_B, X_C) = p(X_A | X_C) \; (\star\star)
</script>
</div>
<p>for all <span><span class="MathJax_Preview">X_c</span><script type="math/tex">X_c</script></span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span><span class="MathJax_Preview">\star\star</span><script type="math/tex">\star\star</script></span> is especially important, and we use this several times throughout the lecture.</p>
</div>
<p>Only a subset of all distributions respect any given (nontrivial) conditional independence statement. The subset of distributions that respect all the CI assumptions we make is the family of distributions consistent with our assumptions. Probabilistic graphical models are a powerful, elegant and simple way to specify such a family.</p>
<h2 id="directed-acyclic-graphical-models-dagm">Directed acyclic graphical models (DAGM)<a class="headerlink" href="#directed-acyclic-graphical-models-dagm" title="Permanent link">&para;</a></h2>
<p>A <a href="https://en.wikipedia.org/wiki/Graphical_model#Bayesian_network">directed acyclic graphical model</a> over <span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> random variables looks like</p>
<div>
<div class="MathJax_Preview">p(x_{1, ..., N}) = \prod_i^Np(x_i | x_{\pi_i})</div>
<script type="math/tex; mode=display">p(x_{1, ..., N}) = \prod_i^Np(x_i | x_{\pi_i})</script>
</div>
<p>where <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> is a random variable (node in the graphical model) and <span><span class="MathJax_Preview">x_{\pi_i}</span><script type="math/tex">x_{\pi_i}</script></span> are the parents of this node. In other words, the joint distribution of a DAGM factors into a product of <em>local conditional distributions</em>, where each random variable (or node) is conditionally dependent on its parent node(s), which could be empty.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The Wikipedia entry on <a href="https://en.wikipedia.org/wiki/Graphical_model">Graphical models</a> is helpful, particularly the section on <a href="https://en.wikipedia.org/wiki/Graphical_model#Bayesian_network">Bayesian networks</a>.</p>
</div>
<p>Notice the difference between a DAGM and the chain rule for probability we introduced early: we are conditioning on <em>parent nodes</em> as opposed to <em>every node</em>. Therefore, the model that represents this distribution is exponential in the <a href="https://en.wikipedia.org/wiki/Fan-in">fan-in</a> of each node (the number of nodes in the parent set), instead of in <span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>.</p>
<h3 id="independence-assumptions-on-dagms">Independence assumptions on DAGMs<a class="headerlink" href="#independence-assumptions-on-dagms" title="Permanent link">&para;</a></h3>
<p>Lets look again at the graphical model <span><span class="MathJax_Preview">p(x_{1, ..., 6})</span><script type="math/tex">p(x_{1, ..., 6})</script></span> we introduced above.</p>
<p>First, lets sort the DAGM topologically. The conditional independence of our random variables becomes</p>
<div>
<div class="MathJax_Preview">x_i \bot x_{\widetilde{\pi_i}} | x_{\pi_i}</div>
<script type="math/tex; mode=display">x_i \bot x_{\widetilde{\pi_i}} | x_{\pi_i}</script>
</div>
<p>so random variables <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> and <span><span class="MathJax_Preview">x_{\widetilde{\pi_i}}</span><script type="math/tex">x_{\widetilde{\pi_i}}</script></span> are conditionally independent of each other but conditionally dependent on their parent nodes <span><span class="MathJax_Preview">x_{\pi_i}</span><script type="math/tex">x_{\pi_i}</script></span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To <a href="https://en.wikipedia.org/wiki/Topological_sorting">topological sort</a> or order a DAGM means to sort all parents before their children.</p>
</div>
<p>Lastly, lets place some assumptions on the conditional dependence of our random variables. Say our model looks like</p>
<p><img alt="" src="../../img/lecture_3_3.png" /></p>
<p>What have the assumptions done to our joint distribution represented by our model?</p>
<div>
<div class="MathJax_Preview">p(x_{1, ..., 6}) = p(x_1)p(x_2 | x_1)p(x_3 | x_1)p(x_4 | x_2)p(x_5 | x_3)p(x_6 | x_2, x_5)</div>
<script type="math/tex; mode=display">p(x_{1, ..., 6}) = p(x_1)p(x_2 | x_1)p(x_3 | x_1)p(x_4 | x_2)p(x_5 | x_3)p(x_6 | x_2, x_5)</script>
</div>
<p>Cleary our assumptions on conditional independence have vastly simplified the model.</p>
<p>Now Suppose each is <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> is a binary random variable. Our assumptions on conditional independence also reduce the dimensionality of our model</p>
<p><img alt="" src="../../img/lecture_3_4.png" /></p>
<h4 id="missing-edges">Missing Edges<a class="headerlink" href="#missing-edges" title="Permanent link">&para;</a></h4>
<p>Missing edges <em>imply conditional independence</em>. Recall that from the chain rule, we can get (for any joint distribution)</p>
<div>
<div class="MathJax_Preview">p(\cap^N_{i=1}) = \prod_{k=1}^N p(x_k | \cap^{k-1}_{j=1} x_j)</div>
<script type="math/tex; mode=display">p(\cap^N_{i=1}) = \prod_{k=1}^N p(x_k | \cap^{k-1}_{j=1} x_j)</script>
</div>
<p>If our joint distribution is represented by a DAGM, however, then some of the conditioned variables can be dropped. This is equivalent to enforcing conditional independence.</p>
<h3 id="d-separation">D-Separation<a class="headerlink" href="#d-separation" title="Permanent link">&para;</a></h3>
<p><strong>D-separation</strong>, or <strong>directed-separation</strong> is a notion of connectedness in DAGMs in which two (sets of) variables <em>may or may not be connected</em> conditioned on a third (set of) variable(s); where D-connection implies conditional <em>dependence</em> and d-separation implies conditional <em>independence</em>.</p>
<p>In particular, we say that</p>
<div>
<div class="MathJax_Preview">x_A \bot x_B | x_C</div>
<script type="math/tex; mode=display">x_A \bot x_B | x_C</script>
</div>
<p>if every variable in <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> is d-separated from every variable in <span><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span> conditioned on all the variables in <span><span class="MathJax_Preview">C</span><script type="math/tex">C</script></span>. We will look at two methods for checking if an independence is true: A depth-first search algorithm and <a href="https://metacademy.org/graphs/concepts/bayes_ball#focus=bayes_ball&amp;mode=learn">Bayes Balls</a>.</p>
<h4 id="dfs-algorithm-for-checking-independence">DFS Algorithm for checking independence<a class="headerlink" href="#dfs-algorithm-for-checking-independence" title="Permanent link">&para;</a></h4>
<p>To check if an independence is true, we can cycle through each node in <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>, do a depth-first search to reach every node in <span><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span>, and examine the path between them. If all of the paths are d-separated, then we can assert</p>
<div>
<div class="MathJax_Preview">x_A \bot x_B | x_C</div>
<script type="math/tex; mode=display">x_A \bot x_B | x_C</script>
</div>
<p>Thus, it will be sufficient to consider triples of nodes.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is not totally clear to me <em>why</em> it is sufficient to consider triples of nodes. This is simply stated "as is" on the lecture slides.</p>
</div>
<p>Lets go through some of the most common triples.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>It was suggested in class that these types of examples make for really good midterm questions!</p>
</div>
<p><strong>1. Chain</strong></p>
<p><img alt="" src="../../img/lecture_3_5.png" /></p>
<p><em>Question</em>: When we condition on <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>, are <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> independent?</p>
<p><em>Answer</em>:</p>
<p>From the graph, we get</p>
<div>
<div class="MathJax_Preview">P(x, y, z) = P(x)P(y|x)P(z|y)</div>
<script type="math/tex; mode=display">P(x, y, z) = P(x)P(y|x)P(z|y)</script>
</div>
<p>which implies</p>
<div>
<div class="MathJax_Preview">\begin{align}
P(z | x, y) &amp;= \frac{P(x, y, z)}{P(x, y)} \\
&amp;= \frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\
&amp;= P(z | y) \\
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
P(z | x, y) &= \frac{P(x, y, z)}{P(x, y)} \\
&= \frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\
&= P(z | y) \\
\end{align}</script>
</div>
<p><span><span class="MathJax_Preview">\therefore</span><script type="math/tex">\therefore</script></span> <span><span class="MathJax_Preview">P(z | x, y) = P(z | y)</span><script type="math/tex">P(z | x, y) = P(z | y)</script></span> and so by <span><span class="MathJax_Preview">\star\star</span><script type="math/tex">\star\star</script></span>, <span><span class="MathJax_Preview">x \bot z | y</span><script type="math/tex">x \bot z | y</script></span>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>It is helpful to think about <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> as the past, <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> as the present and <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> as the future when working with chains such as this one.</p>
</div>
<p><strong>2. Common Cause</strong></p>
<p><img alt="" src="../../img/lecture_3_6.png" /></p>
<p>Where we think of <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> as the "common cause" of the two independent effects <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span>.</p>
<p><em>Question</em>: When we condition on <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>, are <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> independent?</p>
<p><em>Answer</em>:</p>
<p>From the graph, we get</p>
<div>
<div class="MathJax_Preview">P(x, y, z) = P(y)P(x|y)P(z|y)</div>
<script type="math/tex; mode=display">P(x, y, z) = P(y)P(x|y)P(z|y)</script>
</div>
<p>which implies</p>
<div>
<div class="MathJax_Preview">\begin{align}
P(x, z | y) &amp;= \frac{P(x, y, z)}{P(y)} \\
&amp;= \frac{P(y)P(x|y)P(z|y)}{P(y)} \\
&amp;= P(x|y)P(z|y) \\
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
P(x, z | y) &= \frac{P(x, y, z)}{P(y)} \\
&= \frac{P(y)P(x|y)P(z|y)}{P(y)} \\
&= P(x|y)P(z|y) \\
\end{align}</script>
</div>
<p><span><span class="MathJax_Preview">\therefore</span><script type="math/tex">\therefore</script></span> <span><span class="MathJax_Preview">P(x, z| y) = P(x|y)P(z|y)</span><script type="math/tex">P(x, z| y) = P(x|y)P(z|y)</script></span> and so by <span><span class="MathJax_Preview">\star</span><script type="math/tex">\star</script></span>, <span><span class="MathJax_Preview">x \bot z | y</span><script type="math/tex">x \bot z | y</script></span>.</p>
<p><strong>3. Explaining Away</strong></p>
<p><img alt="" src="../../img/lecture_3_7.png" /></p>
<p><em>Question</em>: When we condition on <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>, are <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> independent?</p>
<p><em>Answer</em>:</p>
<p>From the graph, we get</p>
<div>
<div class="MathJax_Preview">P(x, y, z) = P(x)P(z)P(y|x, z)</div>
<script type="math/tex; mode=display">P(x, y, z) = P(x)P(z)P(y|x, z)</script>
</div>
<p>which implies</p>
<div>
<div class="MathJax_Preview">\begin{align}
P(z | x, y) &amp;= \frac{P(x)P(z)P(y | x,  z)}{P(x)P(y|x)} \\
&amp;= \frac{P(z)P(y | x,  z)}{P(y|x)}  \\
&amp;\not = P(z|y) \\
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
P(z | x, y) &= \frac{P(x)P(z)P(y | x,  z)}{P(x)P(y|x)} \\
&= \frac{P(z)P(y | x,  z)}{P(y|x)}  \\
&\not = P(z|y) \\
\end{align}</script>
</div>
<p><span><span class="MathJax_Preview">\therefore</span><script type="math/tex">\therefore</script></span> <span><span class="MathJax_Preview">P(z | x, y) \not = P(z|y)</span><script type="math/tex">P(z | x, y) \not = P(z|y)</script></span> and so by <span><span class="MathJax_Preview">\star\star</span><script type="math/tex">\star\star</script></span>, <span><span class="MathJax_Preview">x \not \bot z | y</span><script type="math/tex">x \not \bot z | y</script></span>.</p>
<p>In fact, <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> are <em>marginally independent</em>, but given <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> they are <em>conditionally independent</em>. This important effect is called explaining away (<a href="https://en.wikipedia.org/wiki/Berkson%27s_paradox">Berkson’s paradox</a>).</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Imaging flipping two coins independently, represented by events <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span>. Furthermore, let <span><span class="MathJax_Preview">y=1</span><script type="math/tex">y=1</script></span> if the coins come up the same and <span><span class="MathJax_Preview">y=0</span><script type="math/tex">y=0</script></span> if they come up differently. Clearly, <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> are independent, but if I tell you <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>, they become coupled!</p>
</div>
<h4 id="bayes-balls-algorithm">Bayes-Balls Algorithm<a class="headerlink" href="#bayes-balls-algorithm" title="Permanent link">&para;</a></h4>
<p>An alternative algorithm for determining conditional independence is the <a href="https://metacademy.org/graphs/concepts/bayes_ball#focus=bayes_ball&amp;mode=learn">Bayes Balls</a> algorithm. To check if <span><span class="MathJax_Preview">x_A \bot x_B | x_C</span><script type="math/tex">x_A \bot x_B | x_C</script></span> we need to check if every variable in <span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> is d-seperated from every variable in <span><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span> conditioned on all variables in <span><span class="MathJax_Preview">C</span><script type="math/tex">C</script></span>. In other words, given that all the nodes in <span><span class="MathJax_Preview">x_C</span><script type="math/tex">x_C</script></span> are "clamped", when we "wiggle" nodes <span><span class="MathJax_Preview">x_A</span><script type="math/tex">x_A</script></span> can we change any of the nodes in <span><span class="MathJax_Preview">x_B</span><script type="math/tex">x_B</script></span>?</p>
<p>In general, the algorithm works as follows: We shade all nodes <span><span class="MathJax_Preview">x_C</span><script type="math/tex">x_C</script></span>, place "balls" at each node in <span><span class="MathJax_Preview">x_A</span><script type="math/tex">x_A</script></span> (or <span><span class="MathJax_Preview">x_B</span><script type="math/tex">x_B</script></span>), let them "bounce" around according to some rules, and then ask if any of the balls reach any of the nodes in <span><span class="MathJax_Preview">x_B</span><script type="math/tex">x_B</script></span> (or <span><span class="MathJax_Preview">x_A</span><script type="math/tex">x_A</script></span>).</p>
<p><em>The rules are as follows</em>:</p>
<p><img alt="" src="../../img/lecture_3_8.png" /></p>
<p>including the <em>boundary rules</em>:</p>
<p><img alt="" src="../../img/lecture_3_9.png" /></p>
<p>where <strong>arrows</strong> indicate paths the balls <em>can</em> travel, and <strong>arrows with bars</strong> indicate paths the balls <em>cannot</em> travel.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Notice balls can travel opposite to edge directions!</p>
</div>
<p>Here’s a trick for the explaining away case: If <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> <em>or any of its descendants</em> is <strong>shaded</strong>, the ball passes through.</p>
<p><img alt="" src="../../img/lecture_3_10.png" /></p>
<h5 id="canonical-micrographs">Canonical Micrographs<a class="headerlink" href="#canonical-micrographs" title="Permanent link">&para;</a></h5>
<p>For reference, here are some canonical micrographs and the Bayes Balls algorithmic rules that apply to them</p>
<p><img alt="" src="../../img/lecture_3_11.png" /></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>See <a href="https://www.youtube.com/watch?v=jgt0G2PkWl0">this video</a> for an easy way to remember all the rules.</p>
</div>
<h5 id="examples">Examples<a class="headerlink" href="#examples" title="Permanent link">&para;</a></h5>
<p><em>Question</em>: In the following graph, is <span><span class="MathJax_Preview">x_1 \bot x_6 | \{x_2, x_3\}</span><script type="math/tex">x_1 \bot x_6 | \{x_2, x_3\}</script></span>?</p>
<p><img alt="" src="../../img/lecture_3_12.png" /></p>
<p><em>Answer</em>:</p>
<p>Yes, by the Bayes Balls algorithm.</p>
<p><img alt="" src="../../img/lecture_3_13.png" /></p>
<p><em>Question</em>: In the following graph, is <span><span class="MathJax_Preview">x_2 \bot x_3 | \{x_1, x_6\}</span><script type="math/tex">x_2 \bot x_3 | \{x_1, x_6\}</script></span>?</p>
<p><img alt="" src="../../img/lecture_3_14.png" /></p>
<p><em>Answer</em>:</p>
<p>No, by the Bayes Balls algorithm.</p>
<p><img alt="" src="../../img/lecture_3_15.png" /></p>
<h3 id="plates">Plates<a class="headerlink" href="#plates" title="Permanent link">&para;</a></h3>
<p>Because Bayesian methods treat parameters as random variables, we would like to include them in the graphical model. One way to do this is to repeat all the iid observations explicitly and show the parameter only once. A better way is to use <strong>plates</strong>, in which repeated quantities that are iid are put in a box</p>
<p><img alt="" src="../../img/lecture_3_16.png" /></p>
<p>Plates are like “macros” that allow you to draw a very complicated graphical model with a simpler notation.
<em>The rules of plates are simple</em>: repeat every structure in a box a number of times given by the integer in the corner of the box (e.g. <span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>), updating the plate index variable (e.g. <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>) as you go. Duplicate every arrow going into the plate and every arrow leaving the plate by connecting the arrows to each copy of the structure.</p>
<p><img alt="" src="../../img/lecture_3_17.png" /></p>
<h4 id="nested-plates">Nested Plates<a class="headerlink" href="#nested-plates" title="Permanent link">&para;</a></h4>
<p>Plates can be nested, in which case their arrows get duplicated also, according to the rule: draw an arrow from every copy of the source node to every copy of the destination node.</p>
<p><img alt="" src="../../img/lecture_3_18.png" /></p>
<p>Plates can also cross (intersect), in which case the nodes at the intersection have multiple indices and get duplicated a number of times equal to the product of the duplication numbers on all the plates containing them.</p>
<h3 id="example-of-a-dagm-markov-chain">Example of a DAGM: Markov Chain<a class="headerlink" href="#example-of-a-dagm-markov-chain" title="Permanent link">&para;</a></h3>
<p><a href="https://en.wikipedia.org/wiki/Markov_chain">Markov chains</a> are a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.</p>
<p><img alt="" src="../../img/lecture_3_19.png" /></p>
<p>In other words, it is a model that satisfies the <a href="https://en.wikipedia.org/wiki/Markov_property">Markov property</a>, i.e., conditional on the present state of the system, its future and past states are independent.</p>
<p><img alt="" src="../../img/lecture_3_20.png" /></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>I don't really understand the difference between the two models given on the slides (and shown above). Are they both Markov chains? In the second model, the probability of an event depends not just on the previous node but on the previous node of the previous node. Jesse went over this only very briefly in lecture.</p>
</div>
<h3 id="unobserved-variables">Unobserved Variables<a class="headerlink" href="#unobserved-variables" title="Permanent link">&para;</a></h3>
<p>Certain variables in our models may be unobserved (<span><span class="MathJax_Preview">Q</span><script type="math/tex">Q</script></span> in the example given below), either some of the time or always, at training time or at test time.</p>
<p><img alt="" src="../../img/lecture_3_21.png" /></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Graphically, we will use shading to indicate observation</p>
</div>
<h4 id="partially-unobserved-missing-variables">Partially Unobserved (Missing) Variables<a class="headerlink" href="#partially-unobserved-missing-variables" title="Permanent link">&para;</a></h4>
<p>If variables are <em>occasionally unobserved</em> then they are <em>missing data</em>, e.g., undefined inputs, missing class labels, erroneous target values. In this case, we can still model the joint distribution, but we define a new cost function in which we sum out or marginalize the missing values at training or test time</p>
<p>\[\ell(\theta ; \mathcal D) = \sum_{\text{complete}} \log p(x^c, y^c | \theta) + \sum_{\text{missing}} \log p(x^m | \theta)\]
\[= \sum_{\text{complete}} \log p(x^c, y^c | \theta) + \sum_{\text{missing}} \log \sum_y p(x^m, y | \theta)\]</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Recall that <span><span class="MathJax_Preview">p(x) = \sum_q p(x, q)</span><script type="math/tex">p(x) = \sum_q p(x, q)</script></span>.</p>
</div>
<h4 id="latent-variables">Latent variables<a class="headerlink" href="#latent-variables" title="Permanent link">&para;</a></h4>
<p>What to do when a variable <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> is <em>always</em> unobserved? Depends on where it appears in our model. If we never condition on it when computing the probability of the variables we do observe, then we can just forget about it and integrate it out.</p>
<p>E.g., given <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>, <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> fit the model <span><span class="MathJax_Preview">p(z, y|x) = p(z|y)p(y|x, w)p(w)</span><script type="math/tex">p(z, y|x) = p(z|y)p(y|x, w)p(w)</script></span>. In other words if it is a leaf node.</p>
<p>However, if <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> is conditioned on, we need to model it.</p>
<p>E.g. given <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>, <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> fit the model <span><span class="MathJax_Preview">p(y|x) = \sum_z p(y|x, z)p(z)</span><script type="math/tex">p(y|x) = \sum_z p(y|x, z)p(z)</script></span>.</p>
<p><img alt="" src="../../img/lecture_3_22.png" /></p>
<h5 id="where-do-latent-variables-come-from">Where do latent variables come from?<a class="headerlink" href="#where-do-latent-variables-come-from" title="Permanent link">&para;</a></h5>
<p>Latent variables may appear naturally, from the structure of the problem (because something wasn’t measured, because of faulty sensors, occlusion, privacy, etc.). But we also may want to <em>intentionally</em> introduce latent variables to model complex dependencies between variables without looking at the dependencies between them directly. This can actually simplify the model (e.g., mixtures).</p>
<p><img alt="" src="../../img/lecture_3_23.png" /></p>
<h3 id="mixture-models">Mixture models<a class="headerlink" href="#mixture-models" title="Permanent link">&para;</a></h3>
<p>Think about the following two sets of data, and notice how there is some underlying structure <em>not dependent on x</em>.</p>
<p><img alt="" src="../../img/lecture_3_24.png" /></p>
<p>The most basic latent variable model might introduce a single discrete node, <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span>, in order to better model the data. This allows different submodels (experts) to contribute to the (conditional) density model in different parts of the space (known as a <a href="https://en.wikipedia.org/wiki/Mixture_of_experts">mixture of experts</a>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The basic idea is to divide &amp; conquer: use simple parts to build complex models (e.g., multimodal densities, or piecewise-linear regressions).</p>
</div>
<h4 id="mixture-densities">Mixture densities<a class="headerlink" href="#mixture-densities" title="Permanent link">&para;</a></h4>
<p>What if the class is <em>unobserved</em>? Then we sum it out</p>
<div>
<div class="MathJax_Preview">
p(x | \theta) = \sum_{k=1}^Kp(z=k | \theta_z)p(x|z=k, \theta_k) \\
= \sum_{k=1}^K\alpha_k p_k(x|\theta_k)
</div>
<script type="math/tex; mode=display">
p(x | \theta) = \sum_{k=1}^Kp(z=k | \theta_z)p(x|z=k, \theta_k) \\
= \sum_{k=1}^K\alpha_k p_k(x|\theta_k)
</script>
</div>
<p>where the <strong>mixing proportions</strong>, <span><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span> sum to 1, i.e. <span><span class="MathJax_Preview">\sum_k\alpha_k = 1</span><script type="math/tex">\sum_k\alpha_k = 1</script></span>. We can use Bayes' rule to compute the posterior probability of the mixture component given some data:</p>
<div>
<div class="MathJax_Preview">
p(z=k | x, \theta_z) = \frac{\alpha_k p_k(x|\theta_k)}{\sum_j\alpha_j p_j(x|\theta_j)}
</div>
<script type="math/tex; mode=display">
p(z=k | x, \theta_z) = \frac{\alpha_k p_k(x|\theta_k)}{\sum_j\alpha_j p_j(x|\theta_j)}
</script>
</div>
<p>these quantities are called <strong>responsibilities</strong>.</p>
<h6 id="example-gaussian-mixture-models">Example: Gaussian Mixture Models<a class="headerlink" href="#example-gaussian-mixture-models" title="Permanent link">&para;</a></h6>
<p>Consider a mixture of <span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> Gaussian componentns</p>
<div>
<div class="MathJax_Preview">
p(x | \theta) = \sum_k \alpha_k \mathcal N(x | \mu_k, \Sigma_k) \\
p(z = k | x, \theta) = \frac{\alpha_k \mathcal N(x | \mu_k, \Sigma_k)}{\sum_j \alpha_j \mathcal N(x | \mu_j, \Sigma_j)} \\
\ell(\theta ; \mathcal D) = \sum_n \log \sum_k \alpha_k \mathcal N(x^{(n)} | \mu_k, \Sigma_k) \\
</div>
<script type="math/tex; mode=display">
p(x | \theta) = \sum_k \alpha_k \mathcal N(x | \mu_k, \Sigma_k) \\
p(z = k | x, \theta) = \frac{\alpha_k \mathcal N(x | \mu_k, \Sigma_k)}{\sum_j \alpha_j \mathcal N(x | \mu_j, \Sigma_j)} \\
\ell(\theta ; \mathcal D) = \sum_n \log \sum_k \alpha_k \mathcal N(x^{(n)} | \mu_k, \Sigma_k) \\
</script>
</div>
<p><img alt="" src="../../img/lecture_3_25.png" /></p>
<ul>
<li>Density model: <span><span class="MathJax_Preview">p(x | \theta)</span><script type="math/tex">p(x | \theta)</script></span> is a familiarity signal.</li>
<li>Clustering: <span><span class="MathJax_Preview">p(z | x, \theta)</span><script type="math/tex">p(z | x, \theta)</script></span> is the assignment rule, <span><span class="MathJax_Preview">- \ell(\theta)</span><script type="math/tex">- \ell(\theta)</script></span> is the cost.</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>I didn't really understand this example.</p>
</div>
<h6 id="example-mixtures-of-experts">Example: Mixtures of Experts<a class="headerlink" href="#example-mixtures-of-experts" title="Permanent link">&para;</a></h6>
<p><a href="https://en.wikipedia.org/wiki/Mixture_of_experts">Mixtures of experts</a>, also known as conditional mixtures are exactly like a class-conditional model, but the class is unobserved and so we sum it out:</p>
<div>
<div class="MathJax_Preview">
p(y | x, \theta) = \sum_{k=1}^Kp(z=k|x, \theta_z)p(y|z=k, x, \theta_K) \\
= \sum_k \alpha_k (x | \theta_z)p_k(y | x, \theta_k) \\
</div>
<script type="math/tex; mode=display">
p(y | x, \theta) = \sum_{k=1}^Kp(z=k|x, \theta_z)p(y|z=k, x, \theta_K) \\
= \sum_k \alpha_k (x | \theta_z)p_k(y | x, \theta_k) \\
</script>
</div>
<p>where <span><span class="MathJax_Preview">\sum_k \alpha_k (x) = 1 \; \forall x</span><script type="math/tex">\sum_k \alpha_k (x) = 1 \; \forall x</script></span>. This is a harder problem than the previous example, as we must learn <span><span class="MathJax_Preview">\alpha(x)</span><script type="math/tex">\alpha(x)</script></span>, often called the <strong>gating function</strong> (unless we chose <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> to be independent of <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>). However, we can still use Bayes' rule to compute the posterior probability of the mixture components given some data:</p>
<div>
<div class="MathJax_Preview">
p(z = k | x, y, \theta) = \frac{\alpha_k(x) p_k(y| x, \theta_k)}{\sum_j\alpha_j(x) p_j(y|x_j, \theta_j)}
</div>
<script type="math/tex; mode=display">
p(z = k | x, y, \theta) = \frac{\alpha_k(x) p_k(y| x, \theta_k)}{\sum_j\alpha_j(x) p_j(y|x_j, \theta_j)}
</script>
</div>
<h6 id="example-mixtures-of-linear-regression-experts">Example: Mixtures of Linear Regression Experts<a class="headerlink" href="#example-mixtures-of-linear-regression-experts" title="Permanent link">&para;</a></h6>
<p>In this model, each expert generates data according to a linear function of the input plus additive Gaussian noise</p>
<div>
<div class="MathJax_Preview">
p(y | x, \theta) = \sum_k \alpha_k \mathcal N(y | \beta_k^Tx, \sigma_k^2)
</div>
<script type="math/tex; mode=display">
p(y | x, \theta) = \sum_k \alpha_k \mathcal N(y | \beta_k^Tx, \sigma_k^2)
</script>
</div>
<p>where the gating function can be a softmax classifier</p>
<div>
<div class="MathJax_Preview">
\alpha_k(x) = p(z=k | x) = \frac{e^{\eta_k^Tx}}{\sum_je^{\eta_k^Tx}}
</div>
<script type="math/tex; mode=display">
\alpha_k(x) = p(z=k | x) = \frac{e^{\eta_k^Tx}}{\sum_je^{\eta_k^Tx}}
</script>
</div>
<p>Remember: we are <em>not</em> modeling the density of the inputs <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>.</p>
<h4 id="gradient-learning-with-mixtures">Gradient learning with mixtures<a class="headerlink" href="#gradient-learning-with-mixtures" title="Permanent link">&para;</a></h4>
<p>We can learn mixture densities using gradient descent on the likelihood as usual.</p>
<div>
<div class="MathJax_Preview">
\ell(\theta) = \log p(x | \theta) = \sum_k \alpha_kp_k(x_k | \theta_k) \\
\Rightarrow \frac{\partial \ell}{\partial \theta} = \frac{1}{p(x | \theta)} \sum_k \alpha_k \frac{\partial p_k(x | \theta)}{\partial \theta} \\
= \sum_k \alpha_k \frac{1}{p(x | \theta)}p_k(x | \theta_k)\frac{\partial \log p_k (x | \theta_k)}{\partial \theta} \\
= \sum_k \alpha_k \frac{p_k(x | \theta_k)}{p(x | \theta)} \frac{\partial \ell_k}{\partial \theta_k} \\
= \sum_k \alpha_k r_k \frac{\partial \ell_k}{\partial \theta_k}
</div>
<script type="math/tex; mode=display">
\ell(\theta) = \log p(x | \theta) = \sum_k \alpha_kp_k(x_k | \theta_k) \\
\Rightarrow \frac{\partial \ell}{\partial \theta} = \frac{1}{p(x | \theta)} \sum_k \alpha_k \frac{\partial p_k(x | \theta)}{\partial \theta} \\
= \sum_k \alpha_k \frac{1}{p(x | \theta)}p_k(x | \theta_k)\frac{\partial \log p_k (x | \theta_k)}{\partial \theta} \\
= \sum_k \alpha_k \frac{p_k(x | \theta_k)}{p(x | \theta)} \frac{\partial \ell_k}{\partial \theta_k} \\
= \sum_k \alpha_k r_k \frac{\partial \ell_k}{\partial \theta_k}
</script>
</div>
<p>In other words, the gradient is the responsibility weighted sum of the individual log likelihood gradients</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>We used two tricks here to derive the gradient, <span><span class="MathJax_Preview">\frac{\partial \log f(\theta)}{\partial \theta} = \frac{1}{f(\theta)} \cdot \frac{\partial f(\theta)}{\partial \theta}</span><script type="math/tex">\frac{\partial \log f(\theta)}{\partial \theta} = \frac{1}{f(\theta)} \cdot \frac{\partial f(\theta)}{\partial \theta}</script></span> and <span><span class="MathJax_Preview"> \frac{\partial f(\theta)}{\partial \theta} = f(\theta) \cdot \frac{\partial \log f(\theta)}{\partial \theta} </span><script type="math/tex"> \frac{\partial f(\theta)}{\partial \theta} = f(\theta) \cdot \frac{\partial \log f(\theta)}{\partial \theta} </script></span></p>
</div>
<h3 id="hidden-markov-models-hmms">Hidden Markov Models (HMMs)<a class="headerlink" href="#hidden-markov-models-hmms" title="Permanent link">&para;</a></h3>
<p><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model"><strong>Hidden Markov Model (HMM)</strong></a> is a statistical Markov model in which the system being modeled is assumed to be a <a href="https://en.wikipedia.org/wiki/Markov_process">Markov process</a> with unobserved (i.e. hidden) states. It is a very popular type of latent variable model</p>
<p><img alt="" src="../../img/lecture_3_26.png" /></p>
<p>where</p>
<ul>
<li><span><span class="MathJax_Preview">Z_t</span><script type="math/tex">Z_t</script></span> are <em>hidden states</em> taking on one of <span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> discrete values</li>
<li><span><span class="MathJax_Preview">X_t</span><script type="math/tex">X_t</script></span> are <em>observed variables</em> taking on values in any space</li>
</ul>
<p>the joint probability represented by the graph factorizes according to</p>
<div>
<div class="MathJax_Preview">
p(X_{1:T}, Z_{1:T}) = p(Z_{1:T})p(X_{1:T} | Z_{1:T}) = p(Z_1) \prod_{t=2}^T p(Z_t | Z_{t-1}) \prod_{t=1}^T p(X_t | Z_t)
</div>
<script type="math/tex; mode=display">
p(X_{1:T}, Z_{1:T}) = p(Z_{1:T})p(X_{1:T} | Z_{1:T}) = p(Z_1) \prod_{t=2}^T p(Z_t | Z_{t-1}) \prod_{t=1}^T p(X_t | Z_t)
</script>
</div>
<h2 id="appendix">Appendix<a class="headerlink" href="#appendix" title="Permanent link">&para;</a></h2>
<h3 id="useful-resources">Useful Resources<a class="headerlink" href="#useful-resources" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://metacademy.org/graphs/concepts/bayes_ball#focus=bayes_ball&amp;mode=learn">Metacademy lesson on Bayes Balls</a>. In fact, that link will bring you to a short course on a couple important concepts for this corse, including conditional probability, conditional independence, Bayesian networks and d-separation.</li>
<li><a href="https://www.youtube.com/watch?v=jgt0G2PkWl0">A video</a> on how to memorize the Bayes Balls rules (this is linked in the above course).</li>
</ul>
<h3 id="glossary-of-terms">Glossary of Terms<a class="headerlink" href="#glossary-of-terms" title="Permanent link">&para;</a></h3>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../week_2/" title="Week 2" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Week 2
              </span>
            </div>
          </a>
        
        
          <a href="../week_4/" title="Week 4" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Week 4
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
  <div class="md-footer-social">
    <link rel="stylesheet" href="../../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/JohnGiorgi" class="md-footer-social__link fa fa-github"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/application.b41f3d20.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"../.."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
    
      
    
  </body>
</html>