



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="https://johngiorgi.github.io/CSC412-2506-course-notes/. /lectures/week_9/">
      
      
        <meta name="author" content="John Giorgi">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.2.0">
    
    
      
        <title>Week 9 - CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/application.572ca0f0.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../../assets/javascripts/modernizr.8c900955.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="../../styles/extra.css">
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#week-9-hidden-markov-models" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://johngiorgi.github.io/CSC412-2506-course-notes/. " title="CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning
              </span>
              <span class="md-header-nav__topic">
                Week 9
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      JohnGiorgi/CSC412-2506-course-notes
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://johngiorgi.github.io/CSC412-2506-course-notes/. " title="CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      JohnGiorgi/CSC412-2506-course-notes
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../.." title="About" class="md-nav__link">
      About
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      Lectures
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Lectures
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../week_1/" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_2/" title="Week 2" class="md-nav__link">
      Week 2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_3/" title="Week 3" class="md-nav__link">
      Week 3
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_4/" title="Week 4" class="md-nav__link">
      Week 4
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_5/" title="Week 5" class="md-nav__link">
      Week 5
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_6/" title="Week 6" class="md-nav__link">
      Week 6
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_8/" title="Week 8" class="md-nav__link">
      Week 8
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Week 9
      </label>
    
    <a href="./" title="Week 9" class="md-nav__link md-nav__link--active">
      Week 9
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#assigned-reading" title="Assigned Reading" class="md-nav__link">
    Assigned Reading
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overview" title="Overview" class="md-nav__link">
    Overview
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sequential-data" title="Sequential data" class="md-nav__link">
    Sequential data
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generalizing-to-high-order-chains" title="Generalizing to high-order chains" class="md-nav__link">
    Generalizing to high-order chains
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameterization" title="Parameterization" class="md-nav__link">
    Parameterization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#aside" title="Aside" class="md-nav__link">
    Aside
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hidden-markov-models" title="Hidden Markov Models" class="md-nav__link">
    Hidden Markov Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simple-example" title="Simple Example" class="md-nav__link">
    Simple Example
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hmms-main-task" title="HMMs Main Task" class="md-nav__link">
    HMMs Main Task
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#forward-backward-algorithm" title="Forward-backward algorithm" class="md-nav__link">
    Forward-backward algorithm
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#forward-recursion" title="Forward Recursion" class="md-nav__link">
    Forward Recursion
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward-recursion" title="Backward Recursion" class="md-nav__link">
    Backward Recursion
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#viterbi-algorithm" title="Viterbi Algorithm" class="md-nav__link">
    Viterbi Algorithm
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_10/" title="Week 10" class="md-nav__link">
      Week 10
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_11/" title="Week 11" class="md-nav__link">
      Week 11
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_12/" title="Week 12" class="md-nav__link">
      Week 12
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_13/" title="Week 13" class="md-nav__link">
      Week 13
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Tutorials
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Tutorials
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/week_1/" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../sample_midterm/" title="Sample Midterm" class="md-nav__link">
      Sample Midterm
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../sample_final/" title="Sample Final" class="md-nav__link">
      Sample Final
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#assigned-reading" title="Assigned Reading" class="md-nav__link">
    Assigned Reading
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overview" title="Overview" class="md-nav__link">
    Overview
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sequential-data" title="Sequential data" class="md-nav__link">
    Sequential data
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generalizing-to-high-order-chains" title="Generalizing to high-order chains" class="md-nav__link">
    Generalizing to high-order chains
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameterization" title="Parameterization" class="md-nav__link">
    Parameterization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#aside" title="Aside" class="md-nav__link">
    Aside
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hidden-markov-models" title="Hidden Markov Models" class="md-nav__link">
    Hidden Markov Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simple-example" title="Simple Example" class="md-nav__link">
    Simple Example
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hmms-main-task" title="HMMs Main Task" class="md-nav__link">
    HMMs Main Task
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#forward-backward-algorithm" title="Forward-backward algorithm" class="md-nav__link">
    Forward-backward algorithm
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#forward-recursion" title="Forward Recursion" class="md-nav__link">
    Forward Recursion
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward-recursion" title="Backward Recursion" class="md-nav__link">
    Backward Recursion
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#viterbi-algorithm" title="Viterbi Algorithm" class="md-nav__link">
    Viterbi Algorithm
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/edit/master/docs/lectures/week_9.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="week-9-hidden-markov-models">Week 9: Hidden Markov Models<a class="headerlink" href="#week-9-hidden-markov-models" title="Permanent link">&para;</a></h1>
<h3 id="assigned-reading">Assigned Reading<a class="headerlink" href="#assigned-reading" title="Permanent link">&para;</a></h3>
<ul>
<li>Murphy: Chapter 18</li>
</ul>
<h3 id="overview">Overview<a class="headerlink" href="#overview" title="Permanent link">&para;</a></h3>
<ul>
<li>Markov Chains</li>
<li>Hidden Markov Models</li>
<li>Forward / Backward Algorithm</li>
<li>Viterbi Algorithm</li>
</ul>
<h2 id="sequential-data">Sequential data<a class="headerlink" href="#sequential-data" title="Permanent link">&para;</a></h2>
<p>Let us turn our attention to sequential data</p>
<div>
<div class="MathJax_Preview">
x_{1:T} = \{x_1, ..., x_T\}
</div>
<script type="math/tex; mode=display">
x_{1:T} = \{x_1, ..., x_T\}
</script>
</div>
<p>Broadly speaking, <strong>sequential data</strong> can be <a href="https://en.wikipedia.org/wiki/Time_series">time-series</a> (e.g. stock prices, speech, video analysis) or ordered (e.g. textual data, gene sequences). So far, we have assumed our data to be i.i.d, however this is a poor assumption for sequential data, as there is often clear dependencies between data points.</p>
<p>Recall the general joint factorization via the chain rule</p>
<div>
<div class="MathJax_Preview">
p(x_{1:T}) = \prod_{t=1}^T p(x_t | x_{t-1}, ..., x_1)
</div>
<script type="math/tex; mode=display">
p(x_{1:T}) = \prod_{t=1}^T p(x_t | x_{t-1}, ..., x_1)
</script>
</div>
<p>But this quickly becomes intractable for high-dimensional data, which is why we made the i.i.d in the first place.</p>
<p>We want temporal dependence without the intractability of computing the chain rule, so we make the simplifying assumption that our data can be modeled as a <strong>first-order <a href="https://en.wikipedia.org/wiki/Markov_chain">Markov chain</a></strong></p>
<div>
<div class="MathJax_Preview">
p(x_t | x_{1:t-1}) = p(x_t | x_{t-1})
</div>
<script type="math/tex; mode=display">
p(x_t | x_{1:t-1}) = p(x_t | x_{t-1})
</script>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>In plain english: each observation is assumed to be independent of all previous observations except most recent.</p>
</div>
<p>This assumption greatly simplifies our joint distribution</p>
<div>
<div class="MathJax_Preview">
p(x_{1:T}) = \prod_{t=1}^T p(x_t | x_{t-1})
</div>
<script type="math/tex; mode=display">
p(x_{1:T}) = \prod_{t=1}^T p(x_t | x_{t-1})
</script>
</div>
<div class="admonition aside">
<p class="admonition-title">Aside</p>
<p>Last week we talked about the Markov chain Monte carlo sampling method Metropolis-Hastings. It is called this because the way you draw samples is a Markov chain.</p>
</div>
<p>A useful distinction to make at this point is between stationary and non-stationary distributions that generate our data</p>
<ul>
<li><strong>Stationary distribution</strong>: the distribution generating the data does not change through time</li>
<li><strong>Non-stationary distribution</strong>: the distribution generating the data is a function of time</li>
</ul>
<p>We are only going to consider the case of a stationary distribution (because it greatly simplifies the math). In this case, we use the same distribution at every timestep, sometimes called a <a href="https://en.wikipedia.org/wiki/Markov_chain#Variations">homogenous Markov chain</a>. Therefore,</p>
<div>
<div class="MathJax_Preview">
p(x_t | x_{t-1}) \quad \forall t
</div>
<script type="math/tex; mode=display">
p(x_t | x_{t-1}) \quad \forall t
</script>
</div>
<p>notice how the process generating the data is independent of <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>.</p>
<h3 id="generalizing-to-high-order-chains">Generalizing to high-order chains<a class="headerlink" href="#generalizing-to-high-order-chains" title="Permanent link">&para;</a></h3>
<p>The first-order assumption is still very restrictive. There are many examples where this would be a poor modeling choice (such as when modeling natural language, where long-term dependencies occur often). We can generalize to high-order dependence trivially</p>
<p><em>second-order</em></p>
<div>
<div class="MathJax_Preview">
p(x_t | x_{1:t-1}) = p(x_t | x_{t-1}, x_{t-2})
</div>
<script type="math/tex; mode=display">
p(x_t | x_{1:t-1}) = p(x_t | x_{t-1}, x_{t-2})
</script>
</div>
<p><span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span><em>-order</em></p>
<div>
<div class="MathJax_Preview">
p(x_t | x_{1:t-1}) = p(x_t | x_{t-m:t-1})
</div>
<script type="math/tex; mode=display">
p(x_t | x_{1:t-1}) = p(x_t | x_{t-m:t-1})
</script>
</div>
<h4 id="parameterization">Parameterization<a class="headerlink" href="#parameterization" title="Permanent link">&para;</a></h4>
<p>How does the order of temporal dependence affect the number of parameters in our model?</p>
<p>Assume <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is a discrete random variable with <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> states. How many parameters are needed to parameterize</p>
<ul>
<li><span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span>: <span><span class="MathJax_Preview">k-1</span><script type="math/tex">k-1</script></span>, as the last state is implicit.</li>
<li><em>first-order chain</em>: <span><span class="MathJax_Preview">k(k-1)</span><script type="math/tex">k(k-1)</script></span>, as we need <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> number of parameters for each parameter of <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span></li>
<li><span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span><em>-order chain</em>: <span><span class="MathJax_Preview">k^m(k-1)</span><script type="math/tex">k^m(k-1)</script></span>, as we need <span><span class="MathJax_Preview">k^m</span><script type="math/tex">k^m</script></span> number of parameters for each parameter of <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>He hinted that this would be useful for assignment 2. Seems like it might be the answer to question 4a.</p>
</div>
<h4 id="aside">Aside<a class="headerlink" href="#aside" title="Permanent link">&para;</a></h4>
<p>So far, we have been thinking about models that operate in <em>discrete space</em> and <em>discrete time</em> but we could also think about models that operate in
<em>continuous space</em> or <em>continuous time</em>.</p>
<p><img alt="" src="../../img/lecture_8_1.png" /></p>
<h2 id="hidden-markov-models">Hidden Markov Models<a class="headerlink" href="#hidden-markov-models" title="Permanent link">&para;</a></h2>
<p><a href="https://duckduckgo.com/?q=hidden+markov+models&amp;t=ffab&amp;ia=web"><strong>Hidden Markov models (HMMs)</strong></a>, make another restrictive assumption: the state of our variables is <em>fully observed</em>.</p>
<p>HMMs hide the temporal dependence by keeping it in the <em>unobserved</em> state. No assumptions on the temporal dependence of observations is made. For each observation <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span>, we associate a corresponding unobserved hidden/latent variable <span><span class="MathJax_Preview">z_t</span><script type="math/tex">z_t</script></span></p>
<p><img alt="" src="../../img/lecture_8_2.png" /></p>
<p>The joint distribution of the model becomes</p>
<div>
<div class="MathJax_Preview">
p(x_{1:T}, z_{1:T}) = p(z_1)\prod_{t=2}^Tp(z_t | z_{t-1})\prod_{t=1}^Tp(x_t | z_t)
</div>
<script type="math/tex; mode=display">
p(x_{1:T}, z_{1:T}) = p(z_1)\prod_{t=2}^Tp(z_t | z_{t-1})\prod_{t=1}^Tp(x_t | z_t)
</script>
</div>
<p>Unlike simple Markov chains, the observations are not limited by a Markov assumption of any order. Assuming we have a <em>homogeneous</em> model, we only have to learn three distributions</p>
<ol>
<li><strong>Initial distribution</strong>: <span><span class="MathJax_Preview">\pi(i) = p(z_1 = i)</span><script type="math/tex">\pi(i) = p(z_1 = i)</script></span>. The probability of the first hidden variable being in state <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> (often denoted <span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span>)</li>
<li><strong>Transition distribution</strong>: <span><span class="MathJax_Preview">T(i, j) = p(z_{t + 1} = j | z_t = i) \quad i \in \{1, ..., k\}</span><script type="math/tex">T(i, j) = p(z_{t + 1} = j | z_t = i) \quad i \in \{1, ..., k\}</script></span>. The probability of moving from hidden state <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> to hidden state <span><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>.</li>
<li><strong>Emission probability</strong>: <span><span class="MathJax_Preview">\varepsilon_i(x) = p(x | z_t = i)</span><script type="math/tex">\varepsilon_i(x) = p(x | z_t = i)</script></span>. The probability of an observed random variable <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> given the state of the hidden variable that <em>"emitted"</em> it.</li>
</ol>
<h3 id="simple-example">Simple Example<a class="headerlink" href="#simple-example" title="Permanent link">&para;</a></h3>
<p>Say we have the following simple chain</p>
<p><img alt="" src="../../img/lecture_8_3.png" /></p>
<p>where</p>
<ul>
<li><span><span class="MathJax_Preview">x_t \in [N, Z, A]</span><script type="math/tex">x_t \in [N, Z, A]</script></span></li>
<li><span><span class="MathJax_Preview">z_t \in [H, S]</span><script type="math/tex">z_t \in [H, S]</script></span></li>
</ul>
<p>where our observed states are whether or not we are watching Netflix (<span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>), sleeping (<span><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span>), or working on the assignment (<span><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>) and our hidden states are whether we are happy (<span><span class="MathJax_Preview">H</span><script type="math/tex">H</script></span>) or sad (<span><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span>). Say futher that we are given the initial (<span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span>), transition (<span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span>), and emission probabilities (<span><span class="MathJax_Preview">\varepsilon</span><script type="math/tex">\varepsilon</script></span>)</p>
<p><center></p>
<table>
<thead>
<tr>
<th><span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>H</td>
<td>0.70</td>
</tr>
<tr>
<td>S</td>
<td>0.30</td>
</tr>
</tbody>
</table>
<p></center></p>
<p><center></p>
<table>
<thead>
<tr>
<th><span><span class="MathJax_Preview">\varepsilon</span><script type="math/tex">\varepsilon</script></span></th>
<th>N</th>
<th>Z</th>
<th>A</th>
</tr>
</thead>
<tbody>
<tr>
<td>H</td>
<td>0.40</td>
<td>0.50</td>
<td>0.10</td>
</tr>
<tr>
<td>S</td>
<td>0.10</td>
<td>0.30</td>
<td>0.60</td>
</tr>
</tbody>
</table>
<p></center></p>
<p><center></p>
<table>
<thead>
<tr>
<th>T</th>
<th>H</th>
<th>S</th>
</tr>
</thead>
<tbody>
<tr>
<td>H</td>
<td>0.80</td>
<td>0.20</td>
</tr>
<tr>
<td>S</td>
<td>0.10</td>
<td>0.90</td>
</tr>
</tbody>
</table>
<p></center></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is the <em>rows</em> of these tables that need to sum to 1, not the columns!</p>
</div>
<p>From these conditional probabilities, we can preform inference with the model, e.g.</p>
<div>
<div class="MathJax_Preview">
p(z_3 = H | z_1 = S) = p(z_3 = H | z_2 = H)p(z_2 = H | z_1 = S) + p(z_3 = H | z_2 = S)p(z_2 = S | z_1 = S) \\
= (0.80)(0.1) + (0.10)(0.90) \\
= 0.17 \\
</div>
<script type="math/tex; mode=display">
p(z_3 = H | z_1 = S) = p(z_3 = H | z_2 = H)p(z_2 = H | z_1 = S) + p(z_3 = H | z_2 = S)p(z_2 = S | z_1 = S) \\
= (0.80)(0.1) + (0.10)(0.90) \\
= 0.17 \\
</script>
</div>
<p>or</p>
<div>
<div class="MathJax_Preview">
p(x_3 = A | z_1 = S) = p(x_3 = A | z_3 = H)p(z_3 = H | z_1 = S) + p(x_3 = A | z_3 = S)p(z_3 = S | z_1 = S) \\
= (0.10)(0.17) + (0.60)(1 - 0.17) \\
= 0.515
</div>
<script type="math/tex; mode=display">
p(x_3 = A | z_1 = S) = p(x_3 = A | z_3 = H)p(z_3 = H | z_1 = S) + p(x_3 = A | z_3 = S)p(z_3 = S | z_1 = S) \\
= (0.10)(0.17) + (0.60)(1 - 0.17) \\
= 0.515
</script>
</div>
<h3 id="hmms-main-task">HMMs Main Task<a class="headerlink" href="#hmms-main-task" title="Permanent link">&para;</a></h3>
<p>The main tasks we perform with HMMs are as follows:</p>
<p><strong>1. Compute the probability of a <em>latent</em> sequence given an <em>observation</em> sequence</strong></p>
<p>That is, we want to be able to compute <span><span class="MathJax_Preview">p(z_{1:t} | x_{1:t})</span><script type="math/tex">p(z_{1:t} | x_{1:t})</script></span>. This is achieved with the <a href="https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm">Forward-Backward algorithm</a>.</p>
<p><strong>2. Infer the most likely sequence of hidden states</strong></p>
<p>That is, we want to be able to compute <span><span class="MathJax_Preview">Z^{\star} = \underset{z_{1:T}}{\operatorname{argmax}} p(z_{1:T} | x_{1:T})</span><script type="math/tex">Z^{\star} = \underset{z_{1:T}}{\operatorname{argmax}} p(z_{1:T} | x_{1:T})</script></span>. This is achieved using the <a href="https://en.wikipedia.org/wiki/Viterbi_algorithm">Viterbi algorithm</a>.</p>
<p><strong>3. Learn the parameters</strong></p>
<p>The parameters of a HMM are typically learned with the <a href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm">Baum-Welch algorithm</a>, a special case of the <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation–maximization algorithm</a> algorithm.</p>
<p>In this course, we will focus on the first two tasks.</p>
<h3 id="forward-backward-algorithm">Forward-backward algorithm<a class="headerlink" href="#forward-backward-algorithm" title="Permanent link">&para;</a></h3>
<p>The Forward-backward algorithm is used to efficiently estimate the <em>latent</em> sequence given an <em>observation</em> sequence under a HMM. That is, we want to compute</p>
<div>
<div class="MathJax_Preview">
p(z_{t} | x_{1:T}) \quad \forall_t \in [1, T]
</div>
<script type="math/tex; mode=display">
p(z_{t} | x_{1:T}) \quad \forall_t \in [1, T]
</script>
</div>
<p>assuming that we know the <em>initial</em> <span><span class="MathJax_Preview">p(z_1)</span><script type="math/tex">p(z_1)</script></span>, <em>transition</em> <span><span class="MathJax_Preview">p(z_t | z_{t-1})</span><script type="math/tex">p(z_t | z_{t-1})</script></span>, and <em>emission</em> <span><span class="MathJax_Preview">p(x_t | z_t)</span><script type="math/tex">p(x_t | z_t)</script></span> probabilities <span><span class="MathJax_Preview">\forall_t \in [1 ,T]</span><script type="math/tex">\forall_t \in [1 ,T]</script></span>. This task of hidden state inference breaks down into the following:</p>
<ul>
<li><strong>Filtering</strong>: compute posterior over <em>current</em> hidden state, <span><span class="MathJax_Preview">p(z_t | x_{1:t})</span><script type="math/tex">p(z_t | x_{1:t})</script></span>.</li>
<li><strong>Prediction</strong>: compute posterior over <em>future</em> hidden state, <span><span class="MathJax_Preview">p(z_{t+k} | x_{1:t})</span><script type="math/tex">p(z_{t+k} | x_{1:t})</script></span>.</li>
<li><strong>Smoothing</strong>: compute posterior over <em>past</em> hidden state, <span><span class="MathJax_Preview">p(z_n | x_{1:t}) \quad 1 \lt n \lt t</span><script type="math/tex">p(z_n | x_{1:t}) \quad 1 \lt n \lt t</script></span>.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Forward-backward algorithm is dynamic, i.e. results are stored as they are computed.</p>
</div>
<p>Our probability of interest, <span><span class="MathJax_Preview">p(z_{t} | x_{1:T})</span><script type="math/tex">p(z_{t} | x_{1:T})</script></span> is computed using a forward and backward recursion</p>
<ul>
<li><strong>Forward Recursion</strong>: <span><span class="MathJax_Preview">p(z_{t} | x_{1:t})</span><script type="math/tex">p(z_{t} | x_{1:t})</script></span></li>
<li><strong>Backward Recursion</strong>: <span><span class="MathJax_Preview">p(x_{1 + t : T} | z_t)</span><script type="math/tex">p(x_{1 + t : T} | z_t)</script></span></li>
</ul>
<p>We note that</p>
<div>
<div class="MathJax_Preview">
p(z_t | x_{1:T}) \propto p(z_t, x_{1:T}) \\
= p(z_t, x_{1:t})p(x_{t+1:T} | z_t, x_{1:t}) \\
= p(z_t, x_{1:t})p(x_{t+1:T} | z_t) \\
= (\text{Forward Recursion})(\text{Backward Recursion})
</div>
<script type="math/tex; mode=display">
p(z_t | x_{1:T}) \propto p(z_t, x_{1:T}) \\
= p(z_t, x_{1:t})p(x_{t+1:T} | z_t, x_{1:t}) \\
= p(z_t, x_{1:t})p(x_{t+1:T} | z_t) \\
= (\text{Forward Recursion})(\text{Backward Recursion})
</script>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The third line is arrived at by noting the conditional independence <span><span class="MathJax_Preview">x_{t+1:T} \bot x_{1:t} | z</span><script type="math/tex">x_{t+1:T} \bot x_{1:t} | z</script></span>. If it is not clear why this conditional independence holds, try to draw out the HMM conditioned on <span><span class="MathJax_Preview">z_t</span><script type="math/tex">z_t</script></span>.</p>
</div>
<h4 id="forward-recursion">Forward Recursion<a class="headerlink" href="#forward-recursion" title="Permanent link">&para;</a></h4>
<div>
<div class="MathJax_Preview">\begin{align}
p(z_t, x_{1:t}) &amp;= \sum^k_{z_{t-1} = 1}p(z_{t-1}, z_t, x_{1:t}) \\
&amp;= \sum^k_{z_{t-1} = 1} p(x_t | z_{t-1}, z_t, x_{1:t-1})p(z_t | z_{t-1}, x_{1:t-1})p(z_{t-1}, x_{1:t-1}) \\
&amp;\Rightarrow \alpha_t(z_t) = p(x_t | z_t) \sum^k_{z_{t-1} = 1} p(z_t | z_{t-1}) \alpha_{t-1}(z_{t-1})\\
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
p(z_t, x_{1:t}) &= \sum^k_{z_{t-1} = 1}p(z_{t-1}, z_t, x_{1:t}) \\
&= \sum^k_{z_{t-1} = 1} p(x_t | z_{t-1}, z_t, x_{1:t-1})p(z_t | z_{t-1}, x_{1:t-1})p(z_{t-1}, x_{1:t-1}) \\
&\Rightarrow \alpha_t(z_t) = p(x_t | z_t) \sum^k_{z_{t-1} = 1} p(z_t | z_{t-1}) \alpha_{t-1}(z_{t-1})\\
\end{align}</script>
</div>
<p>Notice that our forward recursion contains our emission, <span><span class="MathJax_Preview">p(x_t | z_t)</span><script type="math/tex">p(x_t | z_t)</script></span> and transition, <span><span class="MathJax_Preview">p(z_t | z_{t-1})</span><script type="math/tex">p(z_t | z_{t-1})</script></span> probabilities. If we recurse all the way down to <span><span class="MathJax_Preview">\alpha_1(z_1)</span><script type="math/tex">\alpha_1(z_1)</script></span>, we get</p>
<div>
<div class="MathJax_Preview">
\alpha_1(z_1) = p(z_1, x_1) = p(z_1)p(x_1 | z_1)
</div>
<script type="math/tex; mode=display">
\alpha_1(z_1) = p(z_1, x_1) = p(z_1)p(x_1 | z_1)
</script>
</div>
<p>the initial probability times the emission probability of the first observed state, as expected</p>
<h4 id="backward-recursion">Backward Recursion<a class="headerlink" href="#backward-recursion" title="Permanent link">&para;</a></h4>
<div>
<div class="MathJax_Preview">\begin{align}
p(x_{t+1:T} | z_t) &amp;= \sum_{z_{t+1}}^k p(z_{t+1}, x_{t+1:T} | z_t) \\
&amp;= \sum_{z_{t+1}}^k p(x_{t+2:T} | z_{t+1}, z_t, x_{t+1})p(x_{t+1} | z_{t+1}, z_t)p(z_{t+1} | z_t) \\
&amp;\Rightarrow \beta_t(z_t) = \sum_{z_{t+1}}^k p(x_{t+2:T} | z_{t+1})p(x_{t+1} | z_{t+1})p(z_{t+1} | z_t) \\
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
p(x_{t+1:T} | z_t) &= \sum_{z_{t+1}}^k p(z_{t+1}, x_{t+1:T} | z_t) \\
&= \sum_{z_{t+1}}^k p(x_{t+2:T} | z_{t+1}, z_t, x_{t+1})p(x_{t+1} | z_{t+1}, z_t)p(z_{t+1} | z_t) \\
&\Rightarrow \beta_t(z_t) = \sum_{z_{t+1}}^k p(x_{t+2:T} | z_{t+1})p(x_{t+1} | z_{t+1})p(z_{t+1} | z_t) \\
\end{align}</script>
</div>
<p>Notice that our backward recursion contains our emission, <span><span class="MathJax_Preview">p(x_{t+1} | z_{t+1})</span><script type="math/tex">p(x_{t+1} | z_{t+1})</script></span> and transition, <span><span class="MathJax_Preview">p(z_{t+1} | z_t)</span><script type="math/tex">p(z_{t+1} | z_t)</script></span> probabilities. If we recurse all the way down to <span><span class="MathJax_Preview">\beta_1(z_1)</span><script type="math/tex">\beta_1(z_1)</script></span>, we get</p>
<div>
<div class="MathJax_Preview">
\beta_1(z_1) = p(x_{3:T} | z_{2})p(x_{2} | z_{2})p(z_{2} | z_1) \\
</div>
<script type="math/tex; mode=display">
\beta_1(z_1) = p(x_{3:T} | z_{2})p(x_{2} | z_{2})p(z_{2} | z_1) \\
</script>
</div>
<h3 id="viterbi-algorithm">Viterbi Algorithm<a class="headerlink" href="#viterbi-algorithm" title="Permanent link">&para;</a></h3>
<div class="admonition error">
<p class="admonition-title">Error</p>
<p>This will not be on the exam, so I won't bother cleaning up my notes and putting them here!</p>
</div>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../week_8/" title="Week 8" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Week 8
              </span>
            </div>
          </a>
        
        
          <a href="../week_10/" title="Week 10" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Week 10
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
  <div class="md-footer-social">
    <link rel="stylesheet" href="../../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/JohnGiorgi" class="md-footer-social__link fa fa-github"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/application.b41f3d20.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"../.."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
    
      
    
  </body>
</html>