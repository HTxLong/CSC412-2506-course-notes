



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="https://johngiorgi.github.io/CSC412-2506-course-notes/. /lectures/week_11/">
      
      
        <meta name="author" content="John Giorgi">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.2.0">
    
    
      
        <title>Week 11 - CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/application.572ca0f0.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../../assets/javascripts/modernizr.8c900955.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="../../styles/extra.css">
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#week-11-variational-auto-encoders" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://johngiorgi.github.io/CSC412-2506-course-notes/. " title="CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning
              </span>
              <span class="md-header-nav__topic">
                Week 11
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      JohnGiorgi/CSC412-2506-course-notes
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://johngiorgi.github.io/CSC412-2506-course-notes/. " title="CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      JohnGiorgi/CSC412-2506-course-notes
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../.." title="About" class="md-nav__link">
      About
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      Lectures
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Lectures
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../week_1/" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_2/" title="Week 2" class="md-nav__link">
      Week 2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_3/" title="Week 3" class="md-nav__link">
      Week 3
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_4/" title="Week 4" class="md-nav__link">
      Week 4
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_5/" title="Week 5" class="md-nav__link">
      Week 5
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_6/" title="Week 6" class="md-nav__link">
      Week 6
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_8/" title="Week 8" class="md-nav__link">
      Week 8
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_9/" title="Week 9" class="md-nav__link">
      Week 9
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_10/" title="Week 10" class="md-nav__link">
      Week 10
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Week 11
      </label>
    
    <a href="./" title="Week 11" class="md-nav__link md-nav__link--active">
      Week 11
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#assigned-reading" title="Assigned Reading" class="md-nav__link">
    Assigned Reading
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#motivation-autoencoders-deterministic" title="Motivation: Autoencoders (Deterministic)" class="md-nav__link">
    Motivation: Autoencoders (Deterministic)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#problems-with-deterministic-autoencoders" title="Problems with Deterministic Autoencoders" class="md-nav__link">
    Problems with Deterministic Autoencoders
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#problem-1-proximity-in-data-space-does-not-mean-proximity-in-feature-space" title="Problem 1: Proximity in data space does not mean proximity in feature space" class="md-nav__link">
    Problem 1: Proximity in data space does not mean proximity in feature space
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#problem-2-how-to-measure-the-goodness-of-a-reconstruction" title="Problem 2: How to measure the goodness of a reconstruction?" class="md-nav__link">
    Problem 2: How to measure the goodness of a reconstruction?
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#variational-autoencoders-vaes" title="Variational Autoencoders (VAEs)" class="md-nav__link">
    Variational Autoencoders (VAEs)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-specifics" title="The specifics" class="md-nav__link">
    The specifics
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-does-a-vae-solve-the-problems-of-a-deterministic-autoencoder" title="Why does a VAE solve the problems of a deterministic autoencoder?" class="md-nav__link">
    Why does a VAE solve the problems of a deterministic autoencoder?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vae-pipeline" title="VAE Pipeline" class="md-nav__link">
    VAE Pipeline
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-example-mnist" title="Gradient Example: MNIST" class="md-nav__link">
    Gradient Example: MNIST
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-reparameterization-trick" title="The Reparameterization Trick" class="md-nav__link">
    The Reparameterization Trick
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#appendix" title="Appendix" class="md-nav__link">
    Appendix
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#useful-resources" title="Useful Resources" class="md-nav__link">
    Useful Resources
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#glossary-of-terms" title="Glossary of Terms" class="md-nav__link">
    Glossary of Terms
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_12.md" title="Week 12" class="md-nav__link">
      Week 12
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../week_13.md" title="Week 13" class="md-nav__link">
      Week 13
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Tutorials
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Tutorials
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/week_1/" title="Week 1" class="md-nav__link">
      Week 1
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../sample_midterm/" title="Sample Midterm" class="md-nav__link">
      Sample Midterm
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../sample_final/" title="Sample Final" class="md-nav__link">
      Sample Final
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#assigned-reading" title="Assigned Reading" class="md-nav__link">
    Assigned Reading
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#motivation-autoencoders-deterministic" title="Motivation: Autoencoders (Deterministic)" class="md-nav__link">
    Motivation: Autoencoders (Deterministic)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#problems-with-deterministic-autoencoders" title="Problems with Deterministic Autoencoders" class="md-nav__link">
    Problems with Deterministic Autoencoders
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#problem-1-proximity-in-data-space-does-not-mean-proximity-in-feature-space" title="Problem 1: Proximity in data space does not mean proximity in feature space" class="md-nav__link">
    Problem 1: Proximity in data space does not mean proximity in feature space
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#problem-2-how-to-measure-the-goodness-of-a-reconstruction" title="Problem 2: How to measure the goodness of a reconstruction?" class="md-nav__link">
    Problem 2: How to measure the goodness of a reconstruction?
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#variational-autoencoders-vaes" title="Variational Autoencoders (VAEs)" class="md-nav__link">
    Variational Autoencoders (VAEs)
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-specifics" title="The specifics" class="md-nav__link">
    The specifics
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-does-a-vae-solve-the-problems-of-a-deterministic-autoencoder" title="Why does a VAE solve the problems of a deterministic autoencoder?" class="md-nav__link">
    Why does a VAE solve the problems of a deterministic autoencoder?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vae-pipeline" title="VAE Pipeline" class="md-nav__link">
    VAE Pipeline
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-example-mnist" title="Gradient Example: MNIST" class="md-nav__link">
    Gradient Example: MNIST
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-reparameterization-trick" title="The Reparameterization Trick" class="md-nav__link">
    The Reparameterization Trick
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#appendix" title="Appendix" class="md-nav__link">
    Appendix
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#useful-resources" title="Useful Resources" class="md-nav__link">
    Useful Resources
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#glossary-of-terms" title="Glossary of Terms" class="md-nav__link">
    Glossary of Terms
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/JohnGiorgi/CSC412-2506-course-notes/edit/master/docs/lectures/week_11.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="week-11-variational-auto-encoders">Week 11: Variational Auto Encoders<a class="headerlink" href="#week-11-variational-auto-encoders" title="Permanent link">&para;</a></h1>
<h3 id="assigned-reading">Assigned Reading<a class="headerlink" href="#assigned-reading" title="Permanent link">&para;</a></h3>
<p>See <a href="http://www.cs.toronto.edu/~jessebett/CSC412/index.html">course website</a>.</p>
<h2 id="motivation-autoencoders-deterministic">Motivation: Autoencoders (Deterministic)<a class="headerlink" href="#motivation-autoencoders-deterministic" title="Permanent link">&para;</a></h2>
<p>An <a href="https://en.wikipedia.org/wiki/Autoencoder">autoencoders</a> takes some data as input and learns some latent state representation of the data. More specifically, autoencoders reconstruct their own input using an <strong>encoder</strong> and a <strong>decoder</strong>.</p>
<p><strong>Encoder</strong>: <span><span class="MathJax_Preview">g(x) = Z \in F \quad x \in X</span><script type="math/tex">g(x) = Z \in F \quad x \in X</script></span></p>
<p><strong>Decoder</strong>: <span><span class="MathJax_Preview">f(z) = \tilde x</span><script type="math/tex">f(z) = \tilde x</script></span></p>
<p>where <span><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> is the data space, and <span><span class="MathJax_Preview">F</span><script type="math/tex">F</script></span> is the feature space.</p>
<p>The <em>encoder</em>, <span><span class="MathJax_Preview">g(x)</span><script type="math/tex">g(x)</script></span>, takes in the input data (such as an image) and outputs a single value for each encoding dimension while the The <em>decoder</em>, <span><span class="MathJax_Preview">f(z)</span><script type="math/tex">f(z)</script></span> takes this encoding and attempts to recreate the original input.</p>
<p>Our goal is to learn <span><span class="MathJax_Preview">g, f</span><script type="math/tex">g, f</script></span> from labeled data (which is nearly always done with a <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural network</a>).</p>
<p><img alt="" src="../../img/lecture_10_1.png" /></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Much of this lecture comes from <a href="http://papers.nips.cc/paper/7268-sticking-the-landing-simple-lower-variance-gradient-estimators-for-variational-inference">this paper</a>, and forms the basis of all material on assignment 3.</p>
</div>
<p><span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> is the <em>code</em> the model attempts to compress a representation of the input, <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>, into. It is important that this code is a bottleneck, i.e. that</p>
<div>
<div class="MathJax_Preview">
\text{dim} \ F \lt \text{dim} \ X
</div>
<script type="math/tex; mode=display">
\text{dim} \ F \lt \text{dim} \ X
</script>
</div>
<p>as this forces the autoencoder to engage in dimensionality reduction, for example by learning how to ignore noise (otherwise, we would just learn the identify function). The big idea is that the code contains only the most salient features of the input, such that we can reconstruct the input from the code reliably</p>
<div>
<div class="MathJax_Preview">
\tilde x = f(g(x)) \approx x
</div>
<script type="math/tex; mode=display">
\tilde x = f(g(x)) \approx x
</script>
</div>
<h3 id="problems-with-deterministic-autoencoders">Problems with Deterministic Autoencoders<a class="headerlink" href="#problems-with-deterministic-autoencoders" title="Permanent link">&para;</a></h3>
<p>There are two main problems with deterministic autoencoders.</p>
<h4 id="problem-1-proximity-in-data-space-does-not-mean-proximity-in-feature-space">Problem 1: Proximity in data space does not mean proximity in feature space<a class="headerlink" href="#problem-1-proximity-in-data-space-does-not-mean-proximity-in-feature-space" title="Permanent link">&para;</a></h4>
<p>The embeddings (or codes) learned by the model are deterministic, i.e.</p>
<div>
<div class="MathJax_Preview">
g(x_1) = z_1 \Rightarrow f(z_1) = \tilde x_1 \\
g(x_2) = z_2 \Rightarrow f(z_2) = \tilde x_2 \\
</div>
<script type="math/tex; mode=display">
g(x_1) = z_1 \Rightarrow f(z_1) = \tilde x_1 \\
g(x_2) = z_2 \Rightarrow f(z_2) = \tilde x_2 \\
</script>
</div>
<p>but proximity in feature space is not enforced for inputs in close proximity in data space, i.e.</p>
<div>
<div class="MathJax_Preview">
z_1 \approx z_2 \not \Rightarrow x_1 \approx x_2
</div>
<script type="math/tex; mode=display">
z_1 \approx z_2 \not \Rightarrow x_1 \approx x_2
</script>
</div>
<p>The fundamental problem with autoencoders, for generation, is that the latent space they convert their inputs to and where their encoded vectors lie, <em>may not be continuous</em>, or allow easy interpolation. Indeed, given that these models are trained strictly to optimize reconstruction loss, the best strategy is often to encode each input into distinct clusters in the latent space to encode each type of data (class, etc) with discontinuities between clusters (as doing this will allow the decoder to easily reconstruct the input).</p>
<p>But when you’re building a generative model, you don’t want to prepare to replicate the same image you put in. You want to randomly sample from the latent space, or generate variations on an input image, from a continuous latent space.</p>
<p>If the space has discontinuities (eg. gaps between clusters) and you sample/generate a variation from there, the decoder will simply generate an unrealistic output, because the decoder has no idea how to deal with that region of the latent space. During training, it never saw encoded vectors coming from that region of latent space.</p>
<h4 id="problem-2-how-to-measure-the-goodness-of-a-reconstruction">Problem 2: How to measure the goodness of a reconstruction?<a class="headerlink" href="#problem-2-how-to-measure-the-goodness-of-a-reconstruction" title="Permanent link">&para;</a></h4>
<p>An important question is how to measure how well the model is able to reconstruct its inputs. Take the simple example of reconstructing handwritten digits</p>
<p><img alt="" src="../../img/lecture_10_2.png" /></p>
<p>In this case, the reconstruction looks quite good. However, if we chose a simple distance metric between inputs and reconstructions to measure the performance of our model, we would heavily penalize the left-shift in the reconstruction <span><span class="MathJax_Preview">\tilde x</span><script type="math/tex">\tilde x</script></span>.</p>
<p>The point of this example is that choosing an appropriate metric for evaluating model performance can be difficult, and that a miss-aligned objective can be disastrous.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The loss function in this setting is referred to as the <em>reconstruction loss</em>. For autoencoders, the loss function is usually either the mean-squared error or cross-entropy between the output and the input.</p>
</div>
<h2 id="variational-autoencoders-vaes">Variational Autoencoders (VAEs)<a class="headerlink" href="#variational-autoencoders-vaes" title="Permanent link">&para;</a></h2>
<p>The big idea behind <a href="https://en.wikipedia.org/wiki/Autoencoder#Variational_autoencoder_(VAE)"><strong>variational autoencoders (VAEs)</strong></a> is to encode inputs with uncertainty. Unlike normal autoencoders, the encoder of a VAE (also called the <em>recognition model</em>) outputs a <em>probability distribution</em> for each latent attribute, i.e., it encodes inputs <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> to a distribution over latent codes, <span><span class="MathJax_Preview">p(z | x)</span><script type="math/tex">p(z | x)</script></span>. With this modification, random sampling and interpolation become straightforward.</p>
<p>VAEs learn a <a href="https://en.wikipedia.org/wiki/Latent_variable_model">latent variable model</a> for their input data. Instead of the encoder learning an encoding vector of size <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>, it learns two vectors of size <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>: vector of means, <span><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span>, and another vector of standard deviations, <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span>.</p>
<p>Intuitively, the mean will control where the encoding of the input should be centered around while the standard deviation will control how much can the encoding vary from the mean. They form the parameters of a vector of random variables of length <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>, with the <span><span class="MathJax_Preview">i^{th}</span><script type="math/tex">i^{th}</script></span> element of <span><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> and <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> being the mean and standard deviation of the <span><span class="MathJax_Preview">i^{th}</span><script type="math/tex">i^{th}</script></span> random variable, <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span>, from which we sample, to obtain the sampled encoding which we pass onward to the decoder.</p>
<p>This stochastic generation means, that even for the same input, while the mean and standard deviations remain the same, the actual encoding will somewhat vary on every single pass simply due to sampling.</p>
<p><img alt="" src="https://cdn-images-1.medium.com/max/1600/1*96ho7qSyW0nKrLvSoZHOtA.png" /></p>
<p>As encodings are generated at random from anywhere inside the “circle” (the distribution), the decoder learns that not only is a single point in latent space referring to a sample of that class, but all nearby points refer to the same as well. This allows the decoder to not just decode single, specific encodings in the latent space (leaving the decodable latent space discontinuous), but ones that slightly vary too, as the decoder is exposed to a range of variations of the encoding of the same input during training.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Much of this content came from <a href="https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf">this</a> Medium post. Please give it some claps if you found it useful.</p>
</div>
<h3 id="the-specifics">The specifics<a class="headerlink" href="#the-specifics" title="Permanent link">&para;</a></h3>
<p>Our model is represented by the joint distribution over the latent codes and the input data <span><span class="MathJax_Preview">p_\theta(x, z)</span><script type="math/tex">p_\theta(x, z)</script></span>. Decomposing this distribution, we get</p>
<div>
<div class="MathJax_Preview">
p_\theta(x, z) = \text{prior} \times \text{likelihood} = p_\theta(z)p_\theta(x | z)
</div>
<script type="math/tex; mode=display">
p_\theta(x, z) = \text{prior} \times \text{likelihood} = p_\theta(z)p_\theta(x | z)
</script>
</div>
<p>assuming our model looks like</p>
<p><img alt="" src="../../img/lecture_10_3.png" /></p>
<div class="admonition cite">
<p class="admonition-title">Cite</p>
<p><a href="https://arxiv.org/abs/1312.6114">Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." arXiv preprint arXiv:1312.6114 (2013)</a>.</p>
</div>
<p>then learning the model is just inference:</p>
<div>
<div class="MathJax_Preview">
p_\theta(z | x) = \frac{p_\theta(z)p_\theta(x | z)}{p_\theta(x)}
</div>
<script type="math/tex; mode=display">
p_\theta(z | x) = \frac{p_\theta(z)p_\theta(x | z)}{p_\theta(x)}
</script>
</div>
<p>however, as we said last lecture, learning</p>
<div>
<div class="MathJax_Preview">
p_\theta(x) = \int p_\theta(x | z) p_\theta(z) dz
</div>
<script type="math/tex; mode=display">
p_\theta(x) = \int p_\theta(x | z) p_\theta(z) dz
</script>
</div>
<p>is intractable. Our solution was to introduce an approximate distribution with its own set of parameters, <span><span class="MathJax_Preview">q_\phi</span><script type="math/tex">q_\phi</script></span>, and learn these parameters such that</p>
<div>
<div class="MathJax_Preview">
q_\phi (z | x) \approx p_\theta(z | x)
</div>
<script type="math/tex; mode=display">
q_\phi (z | x) \approx p_\theta(z | x)
</script>
</div>
<p>which turned our inference problems into the optimization problem of minimizing the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a> between the true and approximate distributions</p>
<div>
<div class="MathJax_Preview">
D_{KL}(q_\phi(z | x) || p_\theta(z | x))
</div>
<script type="math/tex; mode=display">
D_{KL}(q_\phi(z | x) || p_\theta(z | x))
</script>
</div>
<p>finally, we demonstrated that minimizing <span><span class="MathJax_Preview">D_{KL}</span><script type="math/tex">D_{KL}</script></span> was equivalent to maximizing the ELBO, <span><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span></p>
<div>
<div class="MathJax_Preview">
\mathcal L(\theta, \phi ; x) = -\text{ELBO} = - E_{z_\phi \sim q_\phi} \log \frac{q_\phi(z | x)}{p_\theta(x, z)}
</div>
<script type="math/tex; mode=display">
\mathcal L(\theta, \phi ; x) = -\text{ELBO} = - E_{z_\phi \sim q_\phi} \log \frac{q_\phi(z | x)}{p_\theta(x, z)}
</script>
</div>
<p>We also talked about two other <a href="../week_10/#alternative-forms-of-elbo-and-intuitions">alternative forms or "intuitions" of the ELBO</a>:</p>
<div>
<div class="MathJax_Preview">\begin{align}
\mathcal L(\theta, \phi ; x) &amp;= E_{z_\phi \sim q_\phi} \Big [ \log p_\theta({x | z}) + \log p_\theta({z}) - \log {q_\phi(z | x)} \Big ] \tag*{intuition (1)} \\
&amp;= E_{z_\phi \sim q_\phi} \Big [ \log p_\theta({x | z}) \Big ] - D_{KL}(q_\phi(z | x) || p_\theta(z)) \tag*{intuition (3)}\\
\end{align}</div>
<script type="math/tex; mode=display">\begin{align}
\mathcal L(\theta, \phi ; x) &= E_{z_\phi \sim q_\phi} \Big [ \log p_\theta({x | z}) + \log p_\theta({z}) - \log {q_\phi(z | x)} \Big ] \tag*{intuition (1)} \\
&= E_{z_\phi \sim q_\phi} \Big [ \log p_\theta({x | z}) \Big ] - D_{KL}(q_\phi(z | x) || p_\theta(z)) \tag*{intuition (3)}\\
\end{align}</script>
</div>
<p>The second of which (intuition 3) is the loss function we use for training VAEs. Notice now that the first term corresponds to the <em>likelihood of our input under the distribution decoded from <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span></em> and the second term the <em>divergence of the approximate distribution posterior from the prior of the true distribution</em>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We talked last week about how the second terms acts a regularization, by enforcing the idea that our parameterization shouldn't move us too far from the true distribution. Alos note that this term as a simple, closed form if the posterior and prior are Gaussians.</p>
</div>
<p>The encoder and decoder in a VAE become:</p>
<p><strong>Encoder</strong>: <span><span class="MathJax_Preview">g(x_i) = \phi_i</span><script type="math/tex">g(x_i) = \phi_i</script></span></p>
<p><strong>Decoder</strong>: <span><span class="MathJax_Preview">f(z_i) = \theta_i</span><script type="math/tex">f(z_i) = \theta_i</script></span></p>
<p>Where <span><span class="MathJax_Preview">\phi_i</span><script type="math/tex">\phi_i</script></span> are the parameters of <span><span class="MathJax_Preview">q_\phi(z | x)</span><script type="math/tex">q_\phi(z | x)</script></span> (we are encoding a distribution, which is exactly just its parameters) and <span><span class="MathJax_Preview">\theta_i</span><script type="math/tex">\theta_i</script></span> are the parameters of <span><span class="MathJax_Preview">p_\theta(\tilde x | z_i)</span><script type="math/tex">p_\theta(\tilde x | z_i)</script></span>, the reconstruction likelihood.</p>
<h3 id="why-does-a-vae-solve-the-problems-of-a-deterministic-autoencoder">Why does a VAE solve the problems of a deterministic autoencoder?<a class="headerlink" href="#why-does-a-vae-solve-the-problems-of-a-deterministic-autoencoder" title="Permanent link">&para;</a></h3>
<p><em>Problem 1</em></p>
<p>Unlike the encoder of a normal autoencoder, which encodes each input as a distinct point and forms distinct clusters in the latent space to encode each type of data (class, etc) with discontinuities between clusters (as doing this will allow the decoder to easily reconstruct the input), the VAE generation model learns to reconstruct its inputs not only from the encoded points but also <em>from the area around them</em>. This allows the generation model to generate new data by sampling from an “area” instead of only being able to generate already seen data corresponding to the particular fixed encoded points.</p>
<p><em>Problem 2</em></p>
<p>The first term in the ELBO is the <em>reconstruction likelihood</em>, i.e. the likelihood that we would have re-constructed our data under our model. This serves as our measure of "goodness" of the reconstructed inputs.</p>
<h3 id="vae-pipeline">VAE Pipeline<a class="headerlink" href="#vae-pipeline" title="Permanent link">&para;</a></h3>
<ol>
<li>Run a given input, <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> through the encoder: <span><span class="MathJax_Preview">g(x_i) = \phi_i</span><script type="math/tex">g(x_i) = \phi_i</script></span> to get the parameters of the approximate distribution <span><span class="MathJax_Preview">q_{\phi_i}(z | x)</span><script type="math/tex">q_{\phi_i}(z | x)</script></span>.</li>
<li>Sample <span><span class="MathJax_Preview">z_i \sim q_{\phi_i}(z | x)</span><script type="math/tex">z_i \sim q_{\phi_i}(z | x)</script></span>. This is the code in our feature space <span><span class="MathJax_Preview">F</span><script type="math/tex">F</script></span>.</li>
<li>Run the sampled code through the decoder: <span><span class="MathJax_Preview">f(z_i) = \theta_i</span><script type="math/tex">f(z_i) = \theta_i</script></span> to get the parameters of the true distribution <span><span class="MathJax_Preview">p_{\theta_i}(x | z)</span><script type="math/tex">p_{\theta_i}(x | z)</script></span>.</li>
<li>Compute the loss function: <span><span class="MathJax_Preview">\mathcal L(x ; \theta, \phi) = - E_{z_\phi \sim q_\phi} \Big [ \log p_\theta({x | z}) \Big ] + D_{KL}(q_\phi(z | x) || p_\theta(z))</span><script type="math/tex">\mathcal L(x ; \theta, \phi) = - E_{z_\phi \sim q_\phi} \Big [ \log p_\theta({x | z}) \Big ] + D_{KL}(q_\phi(z | x) || p_\theta(z))</script></span></li>
<li>Use gradient-based optimization to backpropogate the loses <span><span class="MathJax_Preview">\nabla_\theta L</span><script type="math/tex">\nabla_\theta L</script></span>, <span><span class="MathJax_Preview">\nabla_\phi L</span><script type="math/tex">\nabla_\phi L</script></span></li>
</ol>
<p>Once a VAE is trained, we can sample new inputs</p>
<div>
<div class="MathJax_Preview">
\tilde x_i \sim p_\theta(x | z_i)
</div>
<script type="math/tex; mode=display">
\tilde x_i \sim p_\theta(x | z_i)
</script>
</div>
<p>We can also interpolate between inputs, using simple vector arithmetic.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This sampling is never performed during training.</p>
</div>
<h3 id="gradient-example-mnist">Gradient Example: MNIST<a class="headerlink" href="#gradient-example-mnist" title="Permanent link">&para;</a></h3>
<p>Lets walk through an example of computing the gradients for a VAE on MNIST.</p>
<p>We will choose our prior on <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> to be the standard Gaussian with zero mean and unit variance</p>
<div>
<div class="MathJax_Preview">
\mathcal{N}(0, I)
</div>
<script type="math/tex; mode=display">
\mathcal{N}(0, I)
</script>
</div>
<p>our likelihood function to be</p>
<div>
<div class="MathJax_Preview">
p_\theta(x | z) = \Big \{^{\text{Bernoulli if binarized}}_{\sigma (\text{Gaussian}) \text{ else}}
</div>
<script type="math/tex; mode=display">
p_\theta(x | z) = \Big \{^{\text{Bernoulli if binarized}}_{\sigma (\text{Gaussian}) \text{ else}}
</script>
</div>
<p>and our approximate distribution to be</p>
<div>
<div class="MathJax_Preview">
q_\phi(z | x) = \mathcal{N}(\mu(x), \sigma(x)I)
</div>
<script type="math/tex; mode=display">
q_\phi(z | x) = \mathcal{N}(\mu(x), \sigma(x)I)
</script>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Notice that our mean and variance are functions of the input.</p>
</div>
<p>Finally, we use neural networks as our encoder and decoder</p>
<p><strong>Encoder</strong>: <span><span class="MathJax_Preview">g_\phi(x_i) = \phi_i = [u_i, \log \sigma_i]</span><script type="math/tex">g_\phi(x_i) = \phi_i = [u_i, \log \sigma_i]</script></span></p>
<p><strong>Decoder</strong>: <span><span class="MathJax_Preview">f_\theta(z_i) = \theta_i</span><script type="math/tex">f_\theta(z_i) = \theta_i</script></span></p>
<p>Where <span><span class="MathJax_Preview">\theta_i</span><script type="math/tex">\theta_i</script></span> are the Bernoulli variables for each pixel in the input. To get our reconstructed input, we simply evaluate</p>
<div>
<div class="MathJax_Preview">
\tilde x_i \sim p_{\theta_i}(x | z)
</div>
<script type="math/tex; mode=display">
\tilde x_i \sim p_{\theta_i}(x | z)
</script>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We log the variance in the encoder (<span><span class="MathJax_Preview">\log \sigma_i</span><script type="math/tex">\log \sigma_i</script></span>) in order to force outputs to be positive. This allows the neural network to learn parameters in an unconstrained space.</p>
</div>
<p>The entire model looks like:</p>
<p><img alt="" src="https://mohitjainweb.files.wordpress.com/2018/10/variational-autoencoder.png?w=700" /></p>
<div class="admonition cite">
<p class="admonition-title">Cite</p>
<p><a href="https://mohitjain.me/2018/10/26/variational-autoencoder/">Variational Autoencoder Explained</a>.</p>
</div>
<p>Where inputs <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> are encoded to vectors <span><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> and <span><span class="MathJax_Preview">\log \sigma_i</span><script type="math/tex">\log \sigma_i</script></span>, which parameterize <span><span class="MathJax_Preview">q_\phi(z | x)</span><script type="math/tex">q_\phi(z | x)</script></span>. Before decoding, we draw a sample <span><span class="MathJax_Preview">z \sim q_\phi(z | x) = \mathcal{N}(\mu(x), \sigma(x)I)</span><script type="math/tex">z \sim q_\phi(z | x) = \mathcal{N}(\mu(x), \sigma(x)I)</script></span> and evaluate its likelihood under the model with <span><span class="MathJax_Preview">p_\theta(x | z)</span><script type="math/tex">p_\theta(x | z)</script></span>. We compute the loss function <span><span class="MathJax_Preview">\mathcal L(\theta, \phi ; x)</span><script type="math/tex">\mathcal L(\theta, \phi ; x)</script></span> and propagate its derivative with respect to <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> and <span><span class="MathJax_Preview">\phi</span><script type="math/tex">\phi</script></span>, <span><span class="MathJax_Preview">\nabla_\theta L</span><script type="math/tex">\nabla_\theta L</script></span>, <span><span class="MathJax_Preview">\nabla_\phi L</span><script type="math/tex">\nabla_\phi L</script></span>, through the network during training.</p>
<h3 id="the-reparameterization-trick">The Reparameterization Trick<a class="headerlink" href="#the-reparameterization-trick" title="Permanent link">&para;</a></h3>
<p>The decoder generates a reconstruction by first sampling from the distribution <span><span class="MathJax_Preview">q_\phi(z | x)</span><script type="math/tex">q_\phi(z | x)</script></span>. This sampling process introduces a major problem: gradients are blocked from flowing into the encoder, and hence it will not train. To solve this problem, the <a href="https://medium.com/@llionj/the-reparameterization-trick-4ff30fe92954">reparameterization trick</a> is used.</p>
<p>The trick goes as follows: Instead of sampling <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> directly from its distribution (e.g. <span><span class="MathJax_Preview">z \sim \mathcal{N}(\mu(x), \sigma(x)I)</span><script type="math/tex">z \sim \mathcal{N}(\mu(x), \sigma(x)I)</script></span>) we express <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> as a deterministic variable</p>
<div>
<div class="MathJax_Preview">
z = g_\phi(\varepsilon, x)
</div>
<script type="math/tex; mode=display">
z = g_\phi(\varepsilon, x)
</script>
</div>
<p>where <span><span class="MathJax_Preview">\varepsilon</span><script type="math/tex">\varepsilon</script></span> is an auxiliary independent randome variable a <span><span class="MathJax_Preview">g_\phi</span><script type="math/tex">g_\phi</script></span> converts <span><span class="MathJax_Preview">\varepsilon</span><script type="math/tex">\varepsilon</script></span> to <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span>.</p>
<p>In the case of where our approximate distribution is chosen to be the multivariate normal with diagonal covariance, the reparameterization trick gives:</p>
<div>
<div class="MathJax_Preview">
z = \mu + \sigma * \varepsilon \quad \text{where } \varepsilon \sim \mathcal N(0, I)
</div>
<script type="math/tex; mode=display">
z = \mu + \sigma * \varepsilon \quad \text{where } \varepsilon \sim \mathcal N(0, I)
</script>
</div>
<p>with this reparameterization, gradients can now flow through the entire network</p>
<p><img alt="" src="../../img/lecture_10_4.png" /></p>
<h2 id="appendix">Appendix<a class="headerlink" href="#appendix" title="Permanent link">&para;</a></h2>
<h3 id="useful-resources">Useful Resources<a class="headerlink" href="#useful-resources" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://blog.keras.io/building-autoencoders-in-keras.html">Keras Blog</a> on autoencoders.</li>
<li><a href="https://mohitjain.me/2018/10/26/variational-autoencoder/">Blog</a> on VAEs.</li>
<li><a href="https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf">Blog</a> on the intuitive understanding of VAEs.</li>
<li>The <a href="https://arxiv.org/abs/1312.6114">original VAE paper</a> (which assignment 3 is based on) and a <a href="https://www.youtube.com/watch?time_continue=2&amp;v=9zKuYvjFFS8">video</a> explanation.</li>
<li><a href="https://medium.com/@llionj/the-reparameterization-trick-4ff30fe92954">Blog</a> on the reparameterization trick.</li>
</ul>
<h3 id="glossary-of-terms">Glossary of Terms<a class="headerlink" href="#glossary-of-terms" title="Permanent link">&para;</a></h3>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../week_10/" title="Week 10" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Week 10
              </span>
            </div>
          </a>
        
        
          <a href="../../tutorials/week_1/" title="Week 1" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Week 1
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
  <div class="md-footer-social">
    <link rel="stylesheet" href="../../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/JohnGiorgi" class="md-footer-social__link fa fa-github"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/application.b41f3d20.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"../.."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
    
      
    
  </body>
</html>