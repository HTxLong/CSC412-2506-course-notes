{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning Course Website Course Information Sheet When and Where Lectures: Tuesdays 15:00-17:00 in SS 2117 Tutorials: Thursday 13:00-14:00 in SS 2117","title":"About"},{"location":"#csc4122506-winter-2019-probabilistic-learning-and-reasoning","text":"Course Website Course Information Sheet","title":"CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning"},{"location":"#when-and-where","text":"Lectures: Tuesdays 15:00-17:00 in SS 2117 Tutorials: Thursday 13:00-14:00 in SS 2117","title":"When and Where"},{"location":"sample_midterm_answers/","text":"Sample Midterm (Answers) Question 2 Consider the following directed graphical model: (a) List all variables that are independent of A A given evidence on B B By Bayes' Balls I \\bot A | B I \\bot A | B (b) Write down the factorized normalized joint distribution that this graphical model represents. p(A, ..., I) = p(A | B, C)P(B | D)P(C | E, F)P(D | G)P(E | G)P(F | H)P(G)P(H)P(I | G, H) p(A, ..., I) = p(A | B, C)P(B | D)P(C | E, F)P(D | G)P(E | G)P(F | H)P(G)P(H)P(I | G, H) (c) If each node is a single discrete random variable in {1, ..., K} {1, ..., K} how many distinct joint states can the model take? That is, how many different configurations can the variables in this model be set? For each node (random variable) there is k k states. We therefore have k^n k^n possible configurations per node ( x_i x_i ) where k k is the number of states and n n the number of parent nodes ( x_{\\pi_i} x_{\\pi_i} ) Graphically: Together, there are k^{3 + 2 + 3 + 2 + 2 + 2 + 1 + 1 + 3} = k^{19} k^{3 + 2 + 3 + 2 + 2 + 2 + 1 + 1 + 3} = k^{19} possible different configurations of the variables in this model.","title":"Sample Midterm (Answers)"},{"location":"sample_midterm_answers/#sample-midterm-answers","text":"","title":"Sample Midterm (Answers)"},{"location":"sample_midterm_answers/#question-2","text":"Consider the following directed graphical model: (a) List all variables that are independent of A A given evidence on B B By Bayes' Balls I \\bot A | B I \\bot A | B (b) Write down the factorized normalized joint distribution that this graphical model represents. p(A, ..., I) = p(A | B, C)P(B | D)P(C | E, F)P(D | G)P(E | G)P(F | H)P(G)P(H)P(I | G, H) p(A, ..., I) = p(A | B, C)P(B | D)P(C | E, F)P(D | G)P(E | G)P(F | H)P(G)P(H)P(I | G, H) (c) If each node is a single discrete random variable in {1, ..., K} {1, ..., K} how many distinct joint states can the model take? That is, how many different configurations can the variables in this model be set? For each node (random variable) there is k k states. We therefore have k^n k^n possible configurations per node ( x_i x_i ) where k k is the number of states and n n the number of parent nodes ( x_{\\pi_i} x_{\\pi_i} ) Graphically: Together, there are k^{3 + 2 + 3 + 2 + 2 + 2 + 1 + 1 + 3} = k^{19} k^{3 + 2 + 3 + 2 + 2 + 2 + 1 + 1 + 3} = k^{19} possible different configurations of the variables in this model.","title":"Question 2"},{"location":"lectures/week_1/","text":"Week 1: Introduction Assigned Reading Murphy: Chapters 1 and 2 Chapter 2 of David Mackay's textbook Overview Course information Overview of ML with examples Ungraded, anonymous background quiz Textbook and Resources There is no required textbook, but optional reading will be assigned each week Kevin Murphy (2012), Machine Learning: A Probabilistic Perspective . David MacKay (2003) Information Theory, Inference, and Learning Algorithms . The David MacKay textbook is recommended, although 100% of tested material come from class. In this course, lecture slides are more a supplement then main content. The most important stuff will be done on the blackboard, so it is important to come to class with pen and paper. Tutorial slides are relevant, but probably won't be tested. Assignments Assignments must be your own individual work. You can collaborate with up to 2 other students. You should name these people in your submission. Code should be readable . Make sure to put all plots and important results in your PDF submission. Related Courses CSC411: List of methods, (K-NN, Decision trees), more focus on computation STA302: Linear regression and classical stats ECE521: Similar material, more focus on computation STA414: Mostly same material, slightly more emphasis on theory than coding CSC321: Neural networks - about 30% overlap Stats vs Machine Learning Statisticians look at the data, consider the problem, and design a model we can understand. They Analyze methods to give guarantees Want to make few assumptions In machine learning , We only care about making good predictions! The basic idea is to learn a general procedure that works for lots of datasets. Often, there is no way around making assumptions, so we make our model large enough to hopefully learn something close to the truth. We can't use bounds in practice, so we evaluate and empirically choose model details. Sometimes, we end up with interpretable models anyways! In short, statistics starts with a model based on the data , machine learning aims to learn a model from the data. Types of Learning Unsupervised Learning: Given unlabeled data instances x_1 x_1 , x_2 x_2 , x_3 x_3 ... build a statistical model of x x , which can be used for making predictions, decisions. Supervised Learning: Given input-output pairs (x,y) (x,y) the goal is to predict correct output given a new input. Semi-supervised Learning: We are given only a limited amount of (x, y) (x, y) pairs, but lots of unlabeled x x 's Active learning and RL: Also get to choose actions that influence (x, y) (x, y) pairs, but lots of unlabeled x x \u2019s. future information + reward. Can just use basic decision theory. Note that these are all just special cases of estimating distributions from data: p(y | x) p(y | x) , p(x) p(x) , p(x, y) p(x, y) ! Finding Structure in Data With a big enough dataset, we can identify structure in the data. Take a large newswire corpus, for example. A simple model based on the word counts of webpages \\[P(x) = \\frac{1}{Z} \\sum_h \\exp [x^TWh]\\] could learn to discretize data into topics. In this case, our topics are our hidden (or latent ) variables. Note Sometimes latent variables correspond to aspects of physical reality, which could in principle be measured, but may not be for practical reasons. In this situation, the term hidden variables is commonly used (reflecting the fact that the variables are \"really there\", but hidden). Matrix Factorization Lets take a look at a specific example which uses matrix factorization for collaborative filtering . Part of the winning solution in the Netflix contest started with a Netflix dataset of 480,189 users, 17,770 movies and over 100 million ratings. The job was essentially to \"fill-in\" the missing information in a table that looked something like the following (hence the collaborative in collaborative filtering): After the modal was learned, it was clear that the latent representations it learned closley mapped what we might call genre : Multiple Kinds of Data in One Model My modeling the joint distribution of our data p(x, y) p(x, y) , we can incorporate multiple types of data under one model. In this example, our dataset consists of both images and text. Once the joint distribution is learned, we could provide a word and ask the model to sample from the learned distribution and return a picture (or vice versa!): Info Nguyen A, Dosovitskiy A, Yosinski J, Brox T, Clune J (2016). Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. Advances in Neural Information Processing Systems 29 In fact, this is the key idea behind image captioning models: Latent Representations Once learned, latent representations of our data allow us to do some powerful things. For example, neural networks that are able to fill-in occuluded (or missing) portions of digital images: Info Pixel Recurrent Neural Networks. Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu Because our latent space is really a vector space , we have access to all the mathematical operations that are defined on vectors, such as addition and subtraction . Furthermore, out latent representations (which themselves are just vector learned during model training) can be decoded into images (or words, or molecules, etc!). For example, if we were to learn latent representations of human faces, we could add and subtract these representations to create entirely new representations: Latent Representations of Structured Data Some data is structured in a way that is semantically meaningful. Put another way, there is a \"grammar\" to the data. Take the example of molecules representing pharmaceutical drugs. In this case, it is much more difficult to interpolate between two valid structures than it would be to interpolate between images of human faces, for example, because of the grammar of organic chemistry. Simplifying this point, two molecules that look extremely similar could in fact have wildly different behavior in the human body. The take-home point is that we need different methods for learning latent representations of structured data than unstructured data Course Themes This course will follow the broad theme of starting with a simple model and then adding to it. Example Linear regression and principal component analysis (PCA) are special cases of almost everything. A few \"lego bricks\" are enough to build most models (e.g. gaussians, categorical variables, linear transforms and neural networks). While the exact for of each distribution / function shouldn't mater much, your model should have a million parameters in it somewhere (the real world is messy!) Model checking is hard, but important. Learning algorithms are especially hard to debug. Computation Later assignments will involve a bit of programming. You can use whatever language you want, but python and numpy are recommended. For fitting and inference in high-dimensional models, gradient-based methods are basically the only game in town Lots of methods conflate model and fitting algorithm, we will try to separate these. ML as a Bag of Tricks Fast special cases Extensible family K-means Mixture of Gaussians Kernel Density Estimation Latent variable models SVMs Gaussian processes Boosting Deep neural nets Random Forests Bayesian neural nets K-Nearest Neighbours ?? Regularization as a Bag of Tricks Fast special cases Extensible family Early stopping Ensembling L2 Regularization Stochastic variational inference Gradient noise Dropout Expectation-Maximization A Language of Models Our goal will be to develop a language of models, a toolbox . For example, hidden Markov models, mixture of Gaussians, and logistic regression are all examples from a language of models. We will try to show a larger family, and point out common special cases. Using this language, you will be able to build you own custom models. In fact, we can talk about this family of models using very few ideas. Really, all we need are deep probabilistic latent-variable models and some decision theory . Russel and Norvig\u2019s parts of AI Extensible family Machine learning Natural language processing Knowledge representation Deep probabilistic latent-variable models + decision theory Automated reasoning Computer vision Robotics Advantages of probabilistic latent-variable models Data-efficient Learning Automatic regularization, can take advantage of more information. Model is aware what it doesn\u2019t know. E.g. after many cat images model updates get small. Showing a dog image for the first time would lead to large updates.\u0000 Compos-able models Models are built like lego blocks. E.g. you could incorporate a data corruption model. Handle Missing Corrupted Data Latent-variable models can easily handle missing and corrupted data, without the standard hacks of guessing missing values using averages. Predictive Uncertainty Necessary for decision-making. Bad models can confidently give us bad predictions. So can good modals! The key is to be able to express uncertainty. Conditional Predictions Be able to condition predictions can be powerful. E.g. if brexit happens, the value of the pound will fall. Active Learning What data would be expected to increase our confidence about a prediction. Disadvantages of Probabilistic Latent-variable Models Intractable integral over latent variables. Integrating over many dimensions is difficult and sometimes intractable. Probabilistic Graphical Models Vs. Neural Networks Imagine we had the following data we may try to model this data by fitting a mixture of Gaussians, as so which seems perfectly reasonable in this case. However, imagine instead we had the following data the use of a mixture model may not be appropriate in this case, as it fits the data poorly and reports too many clusters but a neural network who's job is to come up with a convincing distribution over where you can expect to see data does much better this brings us to a comparison of probabilistic graphical models and deep learning Probabilistic graphical models Deep learning \u2795 structured representations \u2796 neural net \"goo\" \u2795 priors and uncertainty \u2796 difficult parameterization \u2795 data and computational efficiency \u2796 can require lots of data \u2796 rigid assumptions may not fit \u2795 flexible \u2796 feature engineering \u2795 feature learning \u2796top-down inference \u2795 recognition networks \u2757 Left off on slide 35 The Unreasonable Easiness of Deep Learning The deep learning recipe involves defining an objective function (i.e. a probability of data given parameters) and optimizing the parameters to maximize the object. Gradients are computed automatically, you just need to define a model by some computation. Differentiable models Differentiable models, in general, follow these general principals 1. Model Distributions implicitly by a variable pushed through a deep net \\[y = f_{\\theta}(x) ; x \\sim \\mathcal N(0, I) \\] 2. Approximate intractable distribution by a tractable distribution parameterized by a deep net \\[p(y | x) = \\mathcal N(y | \\mu = f_{\\theta}(x), \\Sigma = g_{\\theta}(x))] ; x \\sim \\mathcal N(0, I) \\] 3. Optimize all parameters using stochastic gradient descent Modeling Idea Graphical models and neural networks have complimentary strengths, and can be combined. One such way to combine these models is by using structured prior distributions formulated as graphical models with highly nonlinear observation models implemented using neural networks . By pushing these structured prior distributions through a neural network we can get a model which takes exploits the best of both worlds This idea can be extended to supervised or unsupervised learning Learning Outcomes Know standard algorithms (bag of tricks), when to use them, and their limitations. For basic applications and baselines. Know main elements of language of deep probabilistic models (bag of bricks: distributions, expectations, latent variables, neural networks) and how to combine them. For custom applications + research. Know standard computational tools (Monte Carlo, Stochastic optimization, regularization, automatic differentiation). For fitting models. Tentative List of Topics Linear methods for regression + classification Bayesian linear regression Probabilistic Generative and Discriminative models - Regularization methods Stochastic Optimization and Neural Networks Graphical model notation and exact inference Mixture Models, Bayesian Networks Model Comparison and marginal likelihood Stochastic Variational Inference Time series and recurrent models Gaussian processes Variational Autoencoders Machine-learning-centric History of Probabilistic Models 1940s - 1960s Motivating probability and Bayesian inference 1980s - 2000s Bayesian machine learning with MCMC 1990s - 2000s Graphical models with exact inference 1990s - present Bayesian Nonparametrics with MCMC (Indian Buffet process, Chinese restaurant process) 1990s - 2000s Bayesian ML with mean-field variational inference 2000s - present Probabilistic Programming 2000s - 2013 Deep undirected graphical models (RBMs, pretraining) 2010s - present Stan - Bayesian Data Analysis with HMC 2000s - 2013 Autoencoders, denoising autoencoders 2000s - present Invertible density estimation 2013 - present Stochastic variational inference, variational autoencoders 2014 - present Generative adversarial nets, Real NVP, Pixelnet 2016 - present Lego-style deep generative models (attend, infer, repeat) Appendix Useful Resources Blog + tutorial on matrix factorization for movie recommendation. Glow an interactive OpenAI blog on Generative Models. It appears that a few of these slides were taken straight from this video. Summary of notation in probability and statistics . Glossary of Terms","title":"Week 1"},{"location":"lectures/week_1/#week-1-introduction","text":"","title":"Week 1: Introduction"},{"location":"lectures/week_1/#assigned-reading","text":"Murphy: Chapters 1 and 2 Chapter 2 of David Mackay's textbook","title":"Assigned Reading"},{"location":"lectures/week_1/#overview","text":"Course information Overview of ML with examples Ungraded, anonymous background quiz","title":"Overview"},{"location":"lectures/week_1/#textbook-and-resources","text":"There is no required textbook, but optional reading will be assigned each week Kevin Murphy (2012), Machine Learning: A Probabilistic Perspective . David MacKay (2003) Information Theory, Inference, and Learning Algorithms . The David MacKay textbook is recommended, although 100% of tested material come from class. In this course, lecture slides are more a supplement then main content. The most important stuff will be done on the blackboard, so it is important to come to class with pen and paper. Tutorial slides are relevant, but probably won't be tested.","title":"Textbook and Resources"},{"location":"lectures/week_1/#assignments","text":"Assignments must be your own individual work. You can collaborate with up to 2 other students. You should name these people in your submission. Code should be readable . Make sure to put all plots and important results in your PDF submission.","title":"Assignments"},{"location":"lectures/week_1/#related-courses","text":"CSC411: List of methods, (K-NN, Decision trees), more focus on computation STA302: Linear regression and classical stats ECE521: Similar material, more focus on computation STA414: Mostly same material, slightly more emphasis on theory than coding CSC321: Neural networks - about 30% overlap","title":"Related Courses"},{"location":"lectures/week_1/#stats-vs-machine-learning","text":"Statisticians look at the data, consider the problem, and design a model we can understand. They Analyze methods to give guarantees Want to make few assumptions In machine learning , We only care about making good predictions! The basic idea is to learn a general procedure that works for lots of datasets. Often, there is no way around making assumptions, so we make our model large enough to hopefully learn something close to the truth. We can't use bounds in practice, so we evaluate and empirically choose model details. Sometimes, we end up with interpretable models anyways! In short, statistics starts with a model based on the data , machine learning aims to learn a model from the data.","title":"Stats vs Machine Learning"},{"location":"lectures/week_1/#types-of-learning","text":"Unsupervised Learning: Given unlabeled data instances x_1 x_1 , x_2 x_2 , x_3 x_3 ... build a statistical model of x x , which can be used for making predictions, decisions. Supervised Learning: Given input-output pairs (x,y) (x,y) the goal is to predict correct output given a new input. Semi-supervised Learning: We are given only a limited amount of (x, y) (x, y) pairs, but lots of unlabeled x x 's Active learning and RL: Also get to choose actions that influence (x, y) (x, y) pairs, but lots of unlabeled x x \u2019s. future information + reward. Can just use basic decision theory. Note that these are all just special cases of estimating distributions from data: p(y | x) p(y | x) , p(x) p(x) , p(x, y) p(x, y) !","title":"Types of Learning"},{"location":"lectures/week_1/#finding-structure-in-data","text":"With a big enough dataset, we can identify structure in the data. Take a large newswire corpus, for example. A simple model based on the word counts of webpages \\[P(x) = \\frac{1}{Z} \\sum_h \\exp [x^TWh]\\] could learn to discretize data into topics. In this case, our topics are our hidden (or latent ) variables. Note Sometimes latent variables correspond to aspects of physical reality, which could in principle be measured, but may not be for practical reasons. In this situation, the term hidden variables is commonly used (reflecting the fact that the variables are \"really there\", but hidden).","title":"Finding Structure in Data"},{"location":"lectures/week_1/#matrix-factorization","text":"Lets take a look at a specific example which uses matrix factorization for collaborative filtering . Part of the winning solution in the Netflix contest started with a Netflix dataset of 480,189 users, 17,770 movies and over 100 million ratings. The job was essentially to \"fill-in\" the missing information in a table that looked something like the following (hence the collaborative in collaborative filtering): After the modal was learned, it was clear that the latent representations it learned closley mapped what we might call genre :","title":"Matrix Factorization"},{"location":"lectures/week_1/#multiple-kinds-of-data-in-one-model","text":"My modeling the joint distribution of our data p(x, y) p(x, y) , we can incorporate multiple types of data under one model. In this example, our dataset consists of both images and text. Once the joint distribution is learned, we could provide a word and ask the model to sample from the learned distribution and return a picture (or vice versa!): Info Nguyen A, Dosovitskiy A, Yosinski J, Brox T, Clune J (2016). Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. Advances in Neural Information Processing Systems 29 In fact, this is the key idea behind image captioning models:","title":"Multiple Kinds of Data in One Model"},{"location":"lectures/week_1/#latent-representations","text":"Once learned, latent representations of our data allow us to do some powerful things. For example, neural networks that are able to fill-in occuluded (or missing) portions of digital images: Info Pixel Recurrent Neural Networks. Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu Because our latent space is really a vector space , we have access to all the mathematical operations that are defined on vectors, such as addition and subtraction . Furthermore, out latent representations (which themselves are just vector learned during model training) can be decoded into images (or words, or molecules, etc!). For example, if we were to learn latent representations of human faces, we could add and subtract these representations to create entirely new representations:","title":"Latent Representations"},{"location":"lectures/week_1/#latent-representations-of-structured-data","text":"Some data is structured in a way that is semantically meaningful. Put another way, there is a \"grammar\" to the data. Take the example of molecules representing pharmaceutical drugs. In this case, it is much more difficult to interpolate between two valid structures than it would be to interpolate between images of human faces, for example, because of the grammar of organic chemistry. Simplifying this point, two molecules that look extremely similar could in fact have wildly different behavior in the human body. The take-home point is that we need different methods for learning latent representations of structured data than unstructured data","title":"Latent Representations of Structured Data"},{"location":"lectures/week_1/#course-themes","text":"This course will follow the broad theme of starting with a simple model and then adding to it. Example Linear regression and principal component analysis (PCA) are special cases of almost everything. A few \"lego bricks\" are enough to build most models (e.g. gaussians, categorical variables, linear transforms and neural networks). While the exact for of each distribution / function shouldn't mater much, your model should have a million parameters in it somewhere (the real world is messy!) Model checking is hard, but important. Learning algorithms are especially hard to debug.","title":"Course Themes"},{"location":"lectures/week_1/#computation","text":"Later assignments will involve a bit of programming. You can use whatever language you want, but python and numpy are recommended. For fitting and inference in high-dimensional models, gradient-based methods are basically the only game in town Lots of methods conflate model and fitting algorithm, we will try to separate these.","title":"Computation"},{"location":"lectures/week_1/#ml-as-a-bag-of-tricks","text":"Fast special cases Extensible family K-means Mixture of Gaussians Kernel Density Estimation Latent variable models SVMs Gaussian processes Boosting Deep neural nets Random Forests Bayesian neural nets K-Nearest Neighbours ??","title":"ML as a Bag of Tricks"},{"location":"lectures/week_1/#regularization-as-a-bag-of-tricks","text":"Fast special cases Extensible family Early stopping Ensembling L2 Regularization Stochastic variational inference Gradient noise Dropout Expectation-Maximization","title":"Regularization as a Bag of Tricks"},{"location":"lectures/week_1/#a-language-of-models","text":"Our goal will be to develop a language of models, a toolbox . For example, hidden Markov models, mixture of Gaussians, and logistic regression are all examples from a language of models. We will try to show a larger family, and point out common special cases. Using this language, you will be able to build you own custom models. In fact, we can talk about this family of models using very few ideas. Really, all we need are deep probabilistic latent-variable models and some decision theory . Russel and Norvig\u2019s parts of AI Extensible family Machine learning Natural language processing Knowledge representation Deep probabilistic latent-variable models + decision theory Automated reasoning Computer vision Robotics","title":"A Language of Models"},{"location":"lectures/week_1/#advantages-of-probabilistic-latent-variable-models","text":"Data-efficient Learning Automatic regularization, can take advantage of more information. Model is aware what it doesn\u2019t know. E.g. after many cat images model updates get small. Showing a dog image for the first time would lead to large updates.\u0000 Compos-able models Models are built like lego blocks. E.g. you could incorporate a data corruption model. Handle Missing Corrupted Data Latent-variable models can easily handle missing and corrupted data, without the standard hacks of guessing missing values using averages. Predictive Uncertainty Necessary for decision-making. Bad models can confidently give us bad predictions. So can good modals! The key is to be able to express uncertainty. Conditional Predictions Be able to condition predictions can be powerful. E.g. if brexit happens, the value of the pound will fall. Active Learning What data would be expected to increase our confidence about a prediction.","title":"Advantages of probabilistic latent-variable models"},{"location":"lectures/week_1/#disadvantages-of-probabilistic-latent-variable-models","text":"Intractable integral over latent variables. Integrating over many dimensions is difficult and sometimes intractable.","title":"Disadvantages of Probabilistic Latent-variable Models"},{"location":"lectures/week_1/#probabilistic-graphical-models-vs-neural-networks","text":"Imagine we had the following data we may try to model this data by fitting a mixture of Gaussians, as so which seems perfectly reasonable in this case. However, imagine instead we had the following data the use of a mixture model may not be appropriate in this case, as it fits the data poorly and reports too many clusters but a neural network who's job is to come up with a convincing distribution over where you can expect to see data does much better this brings us to a comparison of probabilistic graphical models and deep learning Probabilistic graphical models Deep learning \u2795 structured representations \u2796 neural net \"goo\" \u2795 priors and uncertainty \u2796 difficult parameterization \u2795 data and computational efficiency \u2796 can require lots of data \u2796 rigid assumptions may not fit \u2795 flexible \u2796 feature engineering \u2795 feature learning \u2796top-down inference \u2795 recognition networks \u2757 Left off on slide 35","title":"Probabilistic Graphical Models Vs. Neural Networks"},{"location":"lectures/week_1/#the-unreasonable-easiness-of-deep-learning","text":"The deep learning recipe involves defining an objective function (i.e. a probability of data given parameters) and optimizing the parameters to maximize the object. Gradients are computed automatically, you just need to define a model by some computation.","title":"The Unreasonable Easiness of Deep Learning"},{"location":"lectures/week_1/#differentiable-models","text":"Differentiable models, in general, follow these general principals 1. Model Distributions implicitly by a variable pushed through a deep net \\[y = f_{\\theta}(x) ; x \\sim \\mathcal N(0, I) \\] 2. Approximate intractable distribution by a tractable distribution parameterized by a deep net \\[p(y | x) = \\mathcal N(y | \\mu = f_{\\theta}(x), \\Sigma = g_{\\theta}(x))] ; x \\sim \\mathcal N(0, I) \\] 3. Optimize all parameters using stochastic gradient descent","title":"Differentiable models"},{"location":"lectures/week_1/#modeling-idea","text":"Graphical models and neural networks have complimentary strengths, and can be combined. One such way to combine these models is by using structured prior distributions formulated as graphical models with highly nonlinear observation models implemented using neural networks . By pushing these structured prior distributions through a neural network we can get a model which takes exploits the best of both worlds This idea can be extended to supervised or unsupervised learning","title":"Modeling Idea"},{"location":"lectures/week_1/#learning-outcomes","text":"Know standard algorithms (bag of tricks), when to use them, and their limitations. For basic applications and baselines. Know main elements of language of deep probabilistic models (bag of bricks: distributions, expectations, latent variables, neural networks) and how to combine them. For custom applications + research. Know standard computational tools (Monte Carlo, Stochastic optimization, regularization, automatic differentiation). For fitting models.","title":"Learning Outcomes"},{"location":"lectures/week_1/#tentative-list-of-topics","text":"Linear methods for regression + classification Bayesian linear regression Probabilistic Generative and Discriminative models - Regularization methods Stochastic Optimization and Neural Networks Graphical model notation and exact inference Mixture Models, Bayesian Networks Model Comparison and marginal likelihood Stochastic Variational Inference Time series and recurrent models Gaussian processes Variational Autoencoders","title":"Tentative List of Topics"},{"location":"lectures/week_1/#machine-learning-centric-history-of-probabilistic-models","text":"1940s - 1960s Motivating probability and Bayesian inference 1980s - 2000s Bayesian machine learning with MCMC 1990s - 2000s Graphical models with exact inference 1990s - present Bayesian Nonparametrics with MCMC (Indian Buffet process, Chinese restaurant process) 1990s - 2000s Bayesian ML with mean-field variational inference 2000s - present Probabilistic Programming 2000s - 2013 Deep undirected graphical models (RBMs, pretraining) 2010s - present Stan - Bayesian Data Analysis with HMC 2000s - 2013 Autoencoders, denoising autoencoders 2000s - present Invertible density estimation 2013 - present Stochastic variational inference, variational autoencoders 2014 - present Generative adversarial nets, Real NVP, Pixelnet 2016 - present Lego-style deep generative models (attend, infer, repeat)","title":"Machine-learning-centric History of Probabilistic Models"},{"location":"lectures/week_1/#appendix","text":"","title":"Appendix"},{"location":"lectures/week_1/#useful-resources","text":"Blog + tutorial on matrix factorization for movie recommendation. Glow an interactive OpenAI blog on Generative Models. It appears that a few of these slides were taken straight from this video. Summary of notation in probability and statistics .","title":"Useful Resources"},{"location":"lectures/week_1/#glossary-of-terms","text":"","title":"Glossary of Terms"},{"location":"lectures/week_2/","text":"Week 2: Introduction to Probabilistic Models Assigned Reading Murphy: Chapters 3, 4, 7-9 (excluding * sections) Chapter 3 of David Mackay's textbook Overview Overview of probabilistic models Sufficient statistics Likelihood Maximum likelihood estimation (MLE) Classification Overview of probabilistic models In general, we have variables that can be observed or unobserved ( latent variables are always unobserved!). The job of a probabilistic model is to relate the variables (observed or unobserved). More specifically, a probabilistic learns a joint probability distribution over variables, e.g. p(x_1, x_2, ..., x_N) p(x_1, x_2, ..., x_N) . Because the distributions are parameterized, learning is essentially joint density estimation . In a generative model , we assume our data is generated by some distribution then try to learn that distribution . The joint distribution (or joint density function) is the central object we use to define our model. From this perspective, we can think about the basic tasks we care about in machine learning (ML) as operations on joint distributions. One such task is classification Model : p(X, C) p(X, C) Task : p(C=c | x) = \\frac{p(c, x)}{p(x)} p(C=c | x) = \\frac{p(c, x)}{p(x)} where X X are our inputs, C C our classes and p(x) p(x) is the probability of our data (sometimes called the evidence ). p(x) p(x) can be re-written as the marginal probability p(C=c | x) = \\frac{p(x, c)}{\\sum_i p(x, c_i)} p(C=c | x) = \\frac{p(x, c)}{\\sum_i p(x, c_i)} What happens if c c is never observed? Then we call this clustering . Clustering allows us to compute p(C=c|x) p(C=c|x) (\"the probability that the input belongs to some cluster\") even if c c is unobserved. p(c | x) = \\frac{p(c, x)}{p(x)} ; c \\text{ is unobserved} p(c | x) = \\frac{p(c, x)}{p(x)} ; c \\text{ is unobserved} If our inputs and classes are continuous, we call this regression p(y | x) = \\frac{p(x, y)}{p(x)} = \\frac{p(x, y)}{\\int p(x,y)dy} p(y | x) = \\frac{p(x, y)}{p(x)} = \\frac{p(x, y)}{\\int p(x,y)dy} In general, if a variable is always observed, we may not want to model its density (regression / classification) if a variable is never observed (always unobserved) then we call it a hidden or latent variable and we may want to model its density (clustering, density estimation) In fact, we can mostly classify (no pun intended) the problems we care about into four types: Classification : p(c | x) = \\frac{p(c, x)}{p(x)} = \\frac{p(c, x)}{\\sum_c p(c, x)} p(c | x) = \\frac{p(c, x)}{p(x)} = \\frac{p(c, x)}{\\sum_c p(c, x)} Clustering : p(c | x) = \\frac{p(c, x)}{p(x)} ; c \\text{ is unobserved} p(c | x) = \\frac{p(c, x)}{p(x)} ; c \\text{ is unobserved} Regression : p(y | x) = \\frac{p(y, x)}{p(x)} = \\frac{p(y, x)}{\\int p(x)} p(y | x) = \\frac{p(y, x)}{p(x)} = \\frac{p(y, x)}{\\int p(x)} Density Estimation : p(y | x) = \\frac{p(y, x)}{p(x)} ; y \\text{ is unobserved} p(y | x) = \\frac{p(y, x)}{p(x)} ; y \\text{ is unobserved} Operations on Probabilistic Models The fundamental operations we will perform on a probabilistic modal are: Generate Data : For this you need to know how to sample from local models (directed) or how to do Gibbs or other sampling (undirected). Compute probabilities : When all nodes are either observed or marginalized the result is a single number which is the probability of the configuration. Inference : Compute expectations of some things given others which are observed or marginalized. Learning : Set the parameters of the joint distribution given some (partially) observed data to maximize the probability of seeing the data. Goals of a Probabilistic Model We want to build prediction systems automatically based on data, and as little as possible on expert information. In this course, we\u2019ll use probability to combine evidence from data and make predictions. We\u2019ll use graphical models as a visual shorthand language to express and reason about families of model assumptions, structures, dependencies and information flow, without specifying exact distributional forms or parameters. In this case learning means setting parameters of distributions given a model structure . Note \"Structure learning\" is also possible but we won\u2019t consider it now. More specifically, we want two things of our probabilistic model: Compact representation : we don't want parameters to scale poorly with dimensionality of the data. Efficient computation : we need to be able to compute the marginal and conditional probability from the joint distribution efficiently. Multiple observations, Complete IID data A single observation of the data, X X is rarely useful on its own. Generally we have data including many observations, which creates a set of random variables: \\mathcal D = \\{x^{(1)}, ..., x^{(M)}\\} \\mathcal D = \\{x^{(1)}, ..., x^{(M)}\\} To achieve our above-listed goals, we will make assumptions. Often, we assume the following: 1. Observations are independently and identically distributed (i.i.d) according to the joint distribution of the model. This reduces the computation of the joint probability to the product of the individual probabilities of observations We don't always assume full independence. Sometimes, we assume only some level of independence between variables (e.g. var 3 depends on var 1 but not on var 2 ). 2. We observe all random variables in the domain on each observation (i.e. complete data, or fully observed model). Important We typically shade the nodes in a probabilistic graphical model to indicate they are observed . (Later we will work with unshaded nodes corresponding to missing data or latent variables.) Learning Parameters for a Distribution Lets take an example with discrete random variables . \\[ T: \\text{Temperature} ; t = \\text{\"hot\" or \"cold\"} \\] \\[ W: \\text{Weather} ; w = \\text{\"sunny\" or \"raining\"} \\] We know that \\[ P(T=h) = 0.40 \\] \\[ P(T=c) = 0.60 \\] \\[ P(W=s) = 0.70 \\] \\[ P(W=r) = 0.30 \\] and that these states define a valid probability distribution, so P(X) \\ge 0 ; \\sum_x P(x) = 1 P(X) \\ge 0 ; \\sum_x P(x) = 1 We could create a parameterized, probabilistic model, P(T, W) P(T, W) over the states \\[P(T | \\theta_T) ; \\theta_T = \\begin{bmatrix} 0.4 \\\\ 0.6 \\end{bmatrix}\\] \\[P(W | \\theta_W) ; \\theta_W = \\begin{bmatrix} 0.7 \\\\ 0.3 \\end{bmatrix}\\] Notice that \\theta_T \\theta_T and \\theta_W \\theta_W are the probability distributions of our random variables. Our parameters define the probability of the data explicitly and store it in a vector . We can represent the joint distribution P(T, W) P(T, W) , our model as: T W P h s 0.28 c r 0.18 h r 0.12 c s 0.42 from the joint distribution (which is again, essentially our model) we can compute the marginals P(T=h) = \\sum_w P(T=h, W=w) = 0.40 P(T=h) = \\sum_w P(T=h, W=w) = 0.40 P(T=c) = \\sum_w P(T=c, W=w) = 0.60 P(T=c) = \\sum_w P(T=c, W=w) = 0.60 P(W=s) = \\sum_t P(W=s, T=t) = 0.70 P(W=s) = \\sum_t P(W=s, T=t) = 0.70 P(W=r) = \\sum_t P(W=r, T=t) = 0.30 P(W=r) = \\sum_t P(W=r, T=t) = 0.30 we could also ask questions about conditional probabilities, like P(W = s | T = c) = \\frac{P(s,c)}{P(T=c)} = \\frac{P(s,c)}{\\sum_w P(T=c, W=w)} = \\frac{0.42}{0.60} = 0.64 P(W = s | T = c) = \\frac{P(s,c)}{P(T=c)} = \\frac{P(s,c)}{\\sum_w P(T=c, W=w)} = \\frac{0.42}{0.60} = 0.64 Why did we do this? The whole point of the above example was to show that from a probabilistic model, which itself is just a joint distribution represented as a matrix (or tensor), we can compute both the marginal and conditional probabilities. This will allow us to compute probabilities, generate data and perform inference. Joint Dimensionality Lets take our previous example and expand on it. Firstly, it is helpful to think of the joint distribution as a grid with k^n k^n squares, where n n is our number of variables and k k our states. For our running example, this means our joint distribution is parameterized by a 4 dimensional vector, containing the probabilities of seeing any pair of states. We could of course, add more random variables to our model. Imagine we add B B , for whether or not we bike into work and H H , for overall health , each with two states. The dimensionality of our parameters then becomes k^n = 2^4 k^n = 2^4 It is important to note that our joint distribution will be computed based on the assumptions we make about independence between variables. For example, we could assume that while T T and W W are independent from one another, H H is dependent on both T T and W W as well as B B . From the chain rule, we get P(T, H, B, W) = P(T)P(W)P(H|T, W)P(H|B) P(T, H, B, W) = P(T)P(W)P(H|T, W)P(H|B) Likelihood function So far, we have focused on the probability function p(x|\\theta) p(x|\\theta) which assigns a probability (density) to any joint configuration of variables x x given fixed parameters \\theta \\theta . But our goal is to learn \\theta \\theta , which we do not start with and which is not fixed . This is the opposite of how we want to think. Really, we have some fixed data and we want to find parameters \\theta \\theta which maximize the likelihood of that data. Note We are asking \"given x x , how do I choose \\theta \\theta ?\". To do this, we define some function of \\theta \\theta for a fixed x x \\ell(\\theta ; x) = \\log p(x|\\theta) \\ell(\\theta ; x) = \\log p(x|\\theta) which we call the log likelihood function . Note The likelihood function is essentially a notational trick in order to make it easy to talk about our data as a function of our parameters. The process of learning is choosing \\theta \\theta to minimize some cost or loss function, L(\\theta) L(\\theta) which includes \\ell (\\theta) \\ell (\\theta) . This can be done in a couple of ways, including: Maximum likelihood estimation (MLE) : L(\\theta) = \\ell (\\theta; \\mathcal D) L(\\theta) = \\ell (\\theta; \\mathcal D) Maximum a posteriori (MAP) : L(\\theta) = \\ell (\\theta; \\mathcal D) + r(\\theta) L(\\theta) = \\ell (\\theta; \\mathcal D) + r(\\theta) Maximum likelihood estimation The basic idea behind maximum likelihood estimation (MLE) is to pick values for our parameters which were most likely to have generated the data we saw \\theta_{MLE}^* = \\underset{\\theta}{\\operatorname{argmax}} \\ell(\\theta ; D) \\theta_{MLE}^* = \\underset{\\theta}{\\operatorname{argmax}} \\ell(\\theta ; D) Note MLE is commonly used in statistics, and often leads to \"intuitive\", \"appealing\" or \"natural\" estimators. For IID data \\[p(\\mathcal D | \\theta) = \\prod_m p(x^{(m)} | \\theta)\\] \\[\\ell (\\theta ; D) = \\sum_m \\log p(x^{(m)} | \\theta)\\] The IID assumption turns the log likelihood into a sum , making the derivative easy to compute term by term. Note The negative log likelihood, NLL(\\theta ; D) NLL(\\theta ; D) , simply introduces a negative sign so our optimization problem becomes a minimization, that is, maximizing \\ell (\\theta ; D) \\ell (\\theta ; D) is equivalent to minimizing NLL(\\theta ; D) NLL(\\theta ; D) . Sufficient statistics A statistic is a (possibly vector valued) deterministic function of a (set of) random variable(s). A sufficient statistic is a statistic that conveys exactly the same information about the data generating process that created that data as the entire data itself. In other words, once we know the sufficient statistic, T(x) T(x) , then our inferences are the same as would be obtained from our entire data. More formally, we say that T(X) T(X) is a sufficient statistic for X X if T(x^{(1)}) = T(x^{(2)}) \\Rightarrow L(\\theta ; x^{(1)}) = L(\\theta; x^{(2)}) \\forall \\theta T(x^{(1)}) = T(x^{(2)}) \\Rightarrow L(\\theta ; x^{(1)}) = L(\\theta; x^{(2)}) \\forall \\theta Put another way P(\\theta | T(X)) = P(\\theta | X) P(\\theta | T(X)) = P(\\theta | X) Note Why is the useful? Well, if we have a particular large data sample, a lot of the data may be redundant. If we knew the sufficient statistic for that sample, we could use it in place of the full data sample. Equivalently (by the Neyman factorization theorem) we can write P(\\theta | T(X)) = h(x, T(x))g(T(x), \\theta) P(\\theta | T(X)) = h(x, T(x))g(T(x), \\theta) An example is the exponential family p(x | \\eta) = h(x)\\exp\\{\\eta^TT(x)-A(\\eta)\\} p(x | \\eta) = h(x)\\exp\\{\\eta^TT(x)-A(\\eta)\\} Sufficient statistics example: Bernoulli Trials Let us take the example of flipping a fair coin. This process that generates our data is can be modeled as a Bernoulli distribution X \\backsim \\text{Ber}(\\theta) X \\backsim \\text{Ber}(\\theta) where X X is a random variable and x_i x_i represents the result of the ith coin flip \\[x_i = 0 \\text{ , if tails}\\] \\[x_i = 1 \\text{ , if heads}\\] the likelihood (assuming independence between flips of the coin) is \\[L = \\prod_{i=1}^N \\theta^{x_i}(1-\\theta)^{1-x_i}\\] \\[= \\theta^{\\sum_{i=1}^N x_i}(1-\\theta)^{N-\\sum_{i=1}^N x_i}\\] So we notice here that our likelihood depends on \\sum_{i=1}^N x_i \\sum_{i=1}^N x_i . In other words, our data only enters the likelihood in this particular form. This tells us that if we know this summary statistic, which we will call T(x) = \\sum_{i=1}^N x_i T(x) = \\sum_{i=1}^N x_i then essentially we know everything that is useful from our sample to do inference. To perform inference with T(x) T(x) , we define the log likelihood \\[\\ell(\\theta ; X) = \\log p(X | \\theta)\\] \\[ = T(X) \\log \\theta - (N - T(X)) \\log(1-\\theta) \\] then we take the derivative and set it to 0 to find the maximum \\frac{\\partial \\ell}{\\partial \\theta} = 0 = \\frac{T(X)}{N} \\frac{\\partial \\ell}{\\partial \\theta} = 0 = \\frac{T(X)}{N} This is our maximum likelihood estimation of the parameters \\theta \\theta , \\theta^{\\star}_{MLE} \\theta^{\\star}_{MLE} . Note See Lecture 2 slides 10-13 for more examples. Summary of Probabilistic Models In general, learning the parameters of a probabilistic model depends on whether our variables are observed or partially observed, continuous or discrete Continuous Discrete Fully observed variables Bespoke estimates from calculus Normalized counts Partially observed variables Variational inference, recognition networks, MCMC Message passing, variable elimination, junction tree Appendix Useful Resources Helpful video on sufficient statistics. Glossary of Terms","title":"Week 2"},{"location":"lectures/week_2/#week-2-introduction-to-probabilistic-models","text":"","title":"Week 2: Introduction to Probabilistic Models"},{"location":"lectures/week_2/#assigned-reading","text":"Murphy: Chapters 3, 4, 7-9 (excluding * sections) Chapter 3 of David Mackay's textbook","title":"Assigned Reading"},{"location":"lectures/week_2/#overview","text":"Overview of probabilistic models Sufficient statistics Likelihood Maximum likelihood estimation (MLE) Classification","title":"Overview"},{"location":"lectures/week_2/#overview-of-probabilistic-models","text":"In general, we have variables that can be observed or unobserved ( latent variables are always unobserved!). The job of a probabilistic model is to relate the variables (observed or unobserved). More specifically, a probabilistic learns a joint probability distribution over variables, e.g. p(x_1, x_2, ..., x_N) p(x_1, x_2, ..., x_N) . Because the distributions are parameterized, learning is essentially joint density estimation . In a generative model , we assume our data is generated by some distribution then try to learn that distribution . The joint distribution (or joint density function) is the central object we use to define our model. From this perspective, we can think about the basic tasks we care about in machine learning (ML) as operations on joint distributions. One such task is classification Model : p(X, C) p(X, C) Task : p(C=c | x) = \\frac{p(c, x)}{p(x)} p(C=c | x) = \\frac{p(c, x)}{p(x)} where X X are our inputs, C C our classes and p(x) p(x) is the probability of our data (sometimes called the evidence ). p(x) p(x) can be re-written as the marginal probability p(C=c | x) = \\frac{p(x, c)}{\\sum_i p(x, c_i)} p(C=c | x) = \\frac{p(x, c)}{\\sum_i p(x, c_i)} What happens if c c is never observed? Then we call this clustering . Clustering allows us to compute p(C=c|x) p(C=c|x) (\"the probability that the input belongs to some cluster\") even if c c is unobserved. p(c | x) = \\frac{p(c, x)}{p(x)} ; c \\text{ is unobserved} p(c | x) = \\frac{p(c, x)}{p(x)} ; c \\text{ is unobserved} If our inputs and classes are continuous, we call this regression p(y | x) = \\frac{p(x, y)}{p(x)} = \\frac{p(x, y)}{\\int p(x,y)dy} p(y | x) = \\frac{p(x, y)}{p(x)} = \\frac{p(x, y)}{\\int p(x,y)dy} In general, if a variable is always observed, we may not want to model its density (regression / classification) if a variable is never observed (always unobserved) then we call it a hidden or latent variable and we may want to model its density (clustering, density estimation) In fact, we can mostly classify (no pun intended) the problems we care about into four types: Classification : p(c | x) = \\frac{p(c, x)}{p(x)} = \\frac{p(c, x)}{\\sum_c p(c, x)} p(c | x) = \\frac{p(c, x)}{p(x)} = \\frac{p(c, x)}{\\sum_c p(c, x)} Clustering : p(c | x) = \\frac{p(c, x)}{p(x)} ; c \\text{ is unobserved} p(c | x) = \\frac{p(c, x)}{p(x)} ; c \\text{ is unobserved} Regression : p(y | x) = \\frac{p(y, x)}{p(x)} = \\frac{p(y, x)}{\\int p(x)} p(y | x) = \\frac{p(y, x)}{p(x)} = \\frac{p(y, x)}{\\int p(x)} Density Estimation : p(y | x) = \\frac{p(y, x)}{p(x)} ; y \\text{ is unobserved} p(y | x) = \\frac{p(y, x)}{p(x)} ; y \\text{ is unobserved}","title":"Overview of probabilistic models"},{"location":"lectures/week_2/#operations-on-probabilistic-models","text":"The fundamental operations we will perform on a probabilistic modal are: Generate Data : For this you need to know how to sample from local models (directed) or how to do Gibbs or other sampling (undirected). Compute probabilities : When all nodes are either observed or marginalized the result is a single number which is the probability of the configuration. Inference : Compute expectations of some things given others which are observed or marginalized. Learning : Set the parameters of the joint distribution given some (partially) observed data to maximize the probability of seeing the data.","title":"Operations on Probabilistic Models"},{"location":"lectures/week_2/#goals-of-a-probabilistic-model","text":"We want to build prediction systems automatically based on data, and as little as possible on expert information. In this course, we\u2019ll use probability to combine evidence from data and make predictions. We\u2019ll use graphical models as a visual shorthand language to express and reason about families of model assumptions, structures, dependencies and information flow, without specifying exact distributional forms or parameters. In this case learning means setting parameters of distributions given a model structure . Note \"Structure learning\" is also possible but we won\u2019t consider it now. More specifically, we want two things of our probabilistic model: Compact representation : we don't want parameters to scale poorly with dimensionality of the data. Efficient computation : we need to be able to compute the marginal and conditional probability from the joint distribution efficiently.","title":"Goals of a Probabilistic Model"},{"location":"lectures/week_2/#multiple-observations-complete-iid-data","text":"A single observation of the data, X X is rarely useful on its own. Generally we have data including many observations, which creates a set of random variables: \\mathcal D = \\{x^{(1)}, ..., x^{(M)}\\} \\mathcal D = \\{x^{(1)}, ..., x^{(M)}\\} To achieve our above-listed goals, we will make assumptions. Often, we assume the following: 1. Observations are independently and identically distributed (i.i.d) according to the joint distribution of the model. This reduces the computation of the joint probability to the product of the individual probabilities of observations We don't always assume full independence. Sometimes, we assume only some level of independence between variables (e.g. var 3 depends on var 1 but not on var 2 ). 2. We observe all random variables in the domain on each observation (i.e. complete data, or fully observed model). Important We typically shade the nodes in a probabilistic graphical model to indicate they are observed . (Later we will work with unshaded nodes corresponding to missing data or latent variables.)","title":"Multiple observations, Complete IID data"},{"location":"lectures/week_2/#learning-parameters-for-a-distribution","text":"Lets take an example with discrete random variables . \\[ T: \\text{Temperature} ; t = \\text{\"hot\" or \"cold\"} \\] \\[ W: \\text{Weather} ; w = \\text{\"sunny\" or \"raining\"} \\] We know that \\[ P(T=h) = 0.40 \\] \\[ P(T=c) = 0.60 \\] \\[ P(W=s) = 0.70 \\] \\[ P(W=r) = 0.30 \\] and that these states define a valid probability distribution, so P(X) \\ge 0 ; \\sum_x P(x) = 1 P(X) \\ge 0 ; \\sum_x P(x) = 1 We could create a parameterized, probabilistic model, P(T, W) P(T, W) over the states \\[P(T | \\theta_T) ; \\theta_T = \\begin{bmatrix} 0.4 \\\\ 0.6 \\end{bmatrix}\\] \\[P(W | \\theta_W) ; \\theta_W = \\begin{bmatrix} 0.7 \\\\ 0.3 \\end{bmatrix}\\] Notice that \\theta_T \\theta_T and \\theta_W \\theta_W are the probability distributions of our random variables. Our parameters define the probability of the data explicitly and store it in a vector . We can represent the joint distribution P(T, W) P(T, W) , our model as: T W P h s 0.28 c r 0.18 h r 0.12 c s 0.42 from the joint distribution (which is again, essentially our model) we can compute the marginals P(T=h) = \\sum_w P(T=h, W=w) = 0.40 P(T=h) = \\sum_w P(T=h, W=w) = 0.40 P(T=c) = \\sum_w P(T=c, W=w) = 0.60 P(T=c) = \\sum_w P(T=c, W=w) = 0.60 P(W=s) = \\sum_t P(W=s, T=t) = 0.70 P(W=s) = \\sum_t P(W=s, T=t) = 0.70 P(W=r) = \\sum_t P(W=r, T=t) = 0.30 P(W=r) = \\sum_t P(W=r, T=t) = 0.30 we could also ask questions about conditional probabilities, like P(W = s | T = c) = \\frac{P(s,c)}{P(T=c)} = \\frac{P(s,c)}{\\sum_w P(T=c, W=w)} = \\frac{0.42}{0.60} = 0.64 P(W = s | T = c) = \\frac{P(s,c)}{P(T=c)} = \\frac{P(s,c)}{\\sum_w P(T=c, W=w)} = \\frac{0.42}{0.60} = 0.64","title":"Learning Parameters for a Distribution"},{"location":"lectures/week_2/#why-did-we-do-this","text":"The whole point of the above example was to show that from a probabilistic model, which itself is just a joint distribution represented as a matrix (or tensor), we can compute both the marginal and conditional probabilities. This will allow us to compute probabilities, generate data and perform inference.","title":"Why did we do this?"},{"location":"lectures/week_2/#joint-dimensionality","text":"Lets take our previous example and expand on it. Firstly, it is helpful to think of the joint distribution as a grid with k^n k^n squares, where n n is our number of variables and k k our states. For our running example, this means our joint distribution is parameterized by a 4 dimensional vector, containing the probabilities of seeing any pair of states. We could of course, add more random variables to our model. Imagine we add B B , for whether or not we bike into work and H H , for overall health , each with two states. The dimensionality of our parameters then becomes k^n = 2^4 k^n = 2^4 It is important to note that our joint distribution will be computed based on the assumptions we make about independence between variables. For example, we could assume that while T T and W W are independent from one another, H H is dependent on both T T and W W as well as B B . From the chain rule, we get P(T, H, B, W) = P(T)P(W)P(H|T, W)P(H|B) P(T, H, B, W) = P(T)P(W)P(H|T, W)P(H|B)","title":"Joint Dimensionality"},{"location":"lectures/week_2/#likelihood-function","text":"So far, we have focused on the probability function p(x|\\theta) p(x|\\theta) which assigns a probability (density) to any joint configuration of variables x x given fixed parameters \\theta \\theta . But our goal is to learn \\theta \\theta , which we do not start with and which is not fixed . This is the opposite of how we want to think. Really, we have some fixed data and we want to find parameters \\theta \\theta which maximize the likelihood of that data. Note We are asking \"given x x , how do I choose \\theta \\theta ?\". To do this, we define some function of \\theta \\theta for a fixed x x \\ell(\\theta ; x) = \\log p(x|\\theta) \\ell(\\theta ; x) = \\log p(x|\\theta) which we call the log likelihood function . Note The likelihood function is essentially a notational trick in order to make it easy to talk about our data as a function of our parameters. The process of learning is choosing \\theta \\theta to minimize some cost or loss function, L(\\theta) L(\\theta) which includes \\ell (\\theta) \\ell (\\theta) . This can be done in a couple of ways, including: Maximum likelihood estimation (MLE) : L(\\theta) = \\ell (\\theta; \\mathcal D) L(\\theta) = \\ell (\\theta; \\mathcal D) Maximum a posteriori (MAP) : L(\\theta) = \\ell (\\theta; \\mathcal D) + r(\\theta) L(\\theta) = \\ell (\\theta; \\mathcal D) + r(\\theta)","title":"Likelihood function"},{"location":"lectures/week_2/#maximum-likelihood-estimation","text":"The basic idea behind maximum likelihood estimation (MLE) is to pick values for our parameters which were most likely to have generated the data we saw \\theta_{MLE}^* = \\underset{\\theta}{\\operatorname{argmax}} \\ell(\\theta ; D) \\theta_{MLE}^* = \\underset{\\theta}{\\operatorname{argmax}} \\ell(\\theta ; D) Note MLE is commonly used in statistics, and often leads to \"intuitive\", \"appealing\" or \"natural\" estimators. For IID data \\[p(\\mathcal D | \\theta) = \\prod_m p(x^{(m)} | \\theta)\\] \\[\\ell (\\theta ; D) = \\sum_m \\log p(x^{(m)} | \\theta)\\] The IID assumption turns the log likelihood into a sum , making the derivative easy to compute term by term. Note The negative log likelihood, NLL(\\theta ; D) NLL(\\theta ; D) , simply introduces a negative sign so our optimization problem becomes a minimization, that is, maximizing \\ell (\\theta ; D) \\ell (\\theta ; D) is equivalent to minimizing NLL(\\theta ; D) NLL(\\theta ; D) .","title":"Maximum likelihood estimation"},{"location":"lectures/week_2/#sufficient-statistics","text":"A statistic is a (possibly vector valued) deterministic function of a (set of) random variable(s). A sufficient statistic is a statistic that conveys exactly the same information about the data generating process that created that data as the entire data itself. In other words, once we know the sufficient statistic, T(x) T(x) , then our inferences are the same as would be obtained from our entire data. More formally, we say that T(X) T(X) is a sufficient statistic for X X if T(x^{(1)}) = T(x^{(2)}) \\Rightarrow L(\\theta ; x^{(1)}) = L(\\theta; x^{(2)}) \\forall \\theta T(x^{(1)}) = T(x^{(2)}) \\Rightarrow L(\\theta ; x^{(1)}) = L(\\theta; x^{(2)}) \\forall \\theta Put another way P(\\theta | T(X)) = P(\\theta | X) P(\\theta | T(X)) = P(\\theta | X) Note Why is the useful? Well, if we have a particular large data sample, a lot of the data may be redundant. If we knew the sufficient statistic for that sample, we could use it in place of the full data sample. Equivalently (by the Neyman factorization theorem) we can write P(\\theta | T(X)) = h(x, T(x))g(T(x), \\theta) P(\\theta | T(X)) = h(x, T(x))g(T(x), \\theta) An example is the exponential family p(x | \\eta) = h(x)\\exp\\{\\eta^TT(x)-A(\\eta)\\} p(x | \\eta) = h(x)\\exp\\{\\eta^TT(x)-A(\\eta)\\}","title":"Sufficient statistics"},{"location":"lectures/week_2/#sufficient-statistics-example-bernoulli-trials","text":"Let us take the example of flipping a fair coin. This process that generates our data is can be modeled as a Bernoulli distribution X \\backsim \\text{Ber}(\\theta) X \\backsim \\text{Ber}(\\theta) where X X is a random variable and x_i x_i represents the result of the ith coin flip \\[x_i = 0 \\text{ , if tails}\\] \\[x_i = 1 \\text{ , if heads}\\] the likelihood (assuming independence between flips of the coin) is \\[L = \\prod_{i=1}^N \\theta^{x_i}(1-\\theta)^{1-x_i}\\] \\[= \\theta^{\\sum_{i=1}^N x_i}(1-\\theta)^{N-\\sum_{i=1}^N x_i}\\] So we notice here that our likelihood depends on \\sum_{i=1}^N x_i \\sum_{i=1}^N x_i . In other words, our data only enters the likelihood in this particular form. This tells us that if we know this summary statistic, which we will call T(x) = \\sum_{i=1}^N x_i T(x) = \\sum_{i=1}^N x_i then essentially we know everything that is useful from our sample to do inference. To perform inference with T(x) T(x) , we define the log likelihood \\[\\ell(\\theta ; X) = \\log p(X | \\theta)\\] \\[ = T(X) \\log \\theta - (N - T(X)) \\log(1-\\theta) \\] then we take the derivative and set it to 0 to find the maximum \\frac{\\partial \\ell}{\\partial \\theta} = 0 = \\frac{T(X)}{N} \\frac{\\partial \\ell}{\\partial \\theta} = 0 = \\frac{T(X)}{N} This is our maximum likelihood estimation of the parameters \\theta \\theta , \\theta^{\\star}_{MLE} \\theta^{\\star}_{MLE} . Note See Lecture 2 slides 10-13 for more examples.","title":"Sufficient statistics example: Bernoulli Trials"},{"location":"lectures/week_2/#summary-of-probabilistic-models","text":"In general, learning the parameters of a probabilistic model depends on whether our variables are observed or partially observed, continuous or discrete Continuous Discrete Fully observed variables Bespoke estimates from calculus Normalized counts Partially observed variables Variational inference, recognition networks, MCMC Message passing, variable elimination, junction tree","title":"Summary of Probabilistic Models"},{"location":"lectures/week_2/#appendix","text":"","title":"Appendix"},{"location":"lectures/week_2/#useful-resources","text":"Helpful video on sufficient statistics.","title":"Useful Resources"},{"location":"lectures/week_2/#glossary-of-terms","text":"","title":"Glossary of Terms"},{"location":"lectures/week_3/","text":"Week 3: Directed Graphical Models Assigned Reading Murphy: Chapters 10-12 (excluding * sections) Kevin Murphy's page on graphical models Roger Grosse's slides on backprop Overview Graphical notations Conditional independence Bayes Balls Latent variables Common motifs Graphical model notation The joint distribution of N N random variables can be computed by the chain rule p(x_{1, ..., N}) = p(x_1)p(x_2|x_1)p(x_3 | x_2, x_1) \\ldots p(x_{1, ..., N}) = p(x_1)p(x_2|x_1)p(x_3 | x_2, x_1) \\ldots this is true for any joint distribution over any random variables . More formally, in probability the chain rule for two random variables is p(x, y) = p(x | y)p(y) p(x, y) = p(x | y)p(y) and for N N random variables p(\\cap^N_{i=1}) = \\prod_{k=1}^N p(x_k | \\cap^{k-1}_{j=1} x_j) p(\\cap^N_{i=1}) = \\prod_{k=1}^N p(x_k | \\cap^{k-1}_{j=1} x_j) Note Note that this is a bit of an abuse of notation, but p(x_k | \\cap^{k-1}_{j=1} x_j) p(x_k | \\cap^{k-1}_{j=1} x_j) will collpase to p(x_1) p(x_1) when k k = 1. Graphical, we might represent a model p(x_i, x_{\\pi_i}) = p(x_{\\pi_i})p(x_i | x_{\\pi_i}) p(x_i, x_{\\pi_i}) = p(x_{\\pi_i})p(x_i | x_{\\pi_i}) as where nodes represent random variables arrows mean \"conditioned on\", e.g. \" x_i x_i is conditioned on x_{\\pi_1} x_{\\pi_1} \". For example, the graphical model p(x_{1, ..., 6}) p(x_{1, ..., 6}) is represented as This is what the model looks like with no assumptions on the conditional dependence between variables (said otherwise, we assume full conditional dependence of the joint distribution as per the chain rule). This model will scale poorly (exponential with the number of parameters, or k^n k^n where k k are states and n n are random variables, or nodes.). We can simplify the model by building in our assumptions about the conditional probabilities. More explicitly, a directed graphical model implies a restricted factorization of the joint distribution. Conditional Independence Let X X be the set of nodes in our graph (the random variables of our model), then two (sets of) variables X_A X_A , X_B X_B are conditionally independent given a third variable X_C X_C \\[(X_A \\perp X_B | X_C)\\] if \\Leftrightarrow p(X_A, X_B | X_C) = p(X_A | X_C)p(X_B | X_C) \\; (\\star) \\Leftrightarrow p(X_A, X_B | X_C) = p(X_A | X_C)p(X_B | X_C) \\; (\\star) \\Leftrightarrow p(X_A | X_B, X_C) = p(X_A | X_C) \\; (\\star\\star) \\Leftrightarrow p(X_A | X_B, X_C) = p(X_A | X_C) \\; (\\star\\star) for all X_c X_c . Note \\star\\star \\star\\star is especially important, and we use this several times throughout the lecture. Only a subset of all distributions respect any given (nontrivial) conditional independence statement. The subset of distributions that respect all the CI assumptions we make is the family of distributions consistent with our assumptions. Probabilistic graphical models are a powerful, elegant and simple way to specify such a family. Directed acyclic graphical models (DAGM) A directed acyclic graphical model over N N random variables looks like p(x_{1, ..., N}) = \\prod_i^Np(x_i | x_{\\pi_i}) p(x_{1, ..., N}) = \\prod_i^Np(x_i | x_{\\pi_i}) where x_i x_i is a random variable (node in the graphical model) and x_{\\pi_i} x_{\\pi_i} are the parents of this node. In other words, the joint distribution of a DAGM factors into a product of local conditional distributions , where each random variable (or node) is conditionally dependent on its parent node(s), which could be empty. Tip The Wikipedia entry on Graphical models is helpful, particularly the section on Bayesian networks . Notice the difference between a DAGM and the chain rule for probability we introduced early: we are conditioning on parent nodes as opposed to every node . Therefore, the model that represents this distribution is exponential in the fan-in of each node (the number of nodes in the parent set), instead of in N N . Independence assumptions on DAGMs Lets look again at the graphical model p(x_{1, ..., 6}) p(x_{1, ..., 6}) we introduced above. First, lets sort the DAGM topologically. The conditional independence of our random variables becomes x_i \\bot x_{\\widetilde{\\pi_i}} | x_{\\pi_i} x_i \\bot x_{\\widetilde{\\pi_i}} | x_{\\pi_i} so random variables x_i x_i and x_{\\widetilde{\\pi_i}} x_{\\widetilde{\\pi_i}} are conditionally independent of each other but conditionally dependent on their parent nodes x_{\\pi_i} x_{\\pi_i} . Note To topological sort or order a DAGM means to sort all parents before their children. Lastly, lets place some assumptions on the conditional dependence of our random variables. Say our model looks like What have the assumptions done to our joint distribution represented by our model? p(x_{1, ..., 6}) = p(x_1)p(x_2 | x_1)p(x_3 | x_1)p(x_4 | x_2)p(x_5 | x_3)p(x_6 | x_2, x_5) p(x_{1, ..., 6}) = p(x_1)p(x_2 | x_1)p(x_3 | x_1)p(x_4 | x_2)p(x_5 | x_3)p(x_6 | x_2, x_5) Cleary our assumptions on conditional independence have vastly simplified the model. Now Suppose each is x_i x_i is a binary random variable. Our assumptions on conditional independence also reduce the dimensionality of our model Missing Edges Missing edges imply conditional independence . Recall that from the chain rule, we can get (for any joint distribution) p(\\cap^N_{i=1}) = \\prod_{k=1}^N p(x_k | \\cap^{k-1}_{j=1} x_j) p(\\cap^N_{i=1}) = \\prod_{k=1}^N p(x_k | \\cap^{k-1}_{j=1} x_j) If our joint distribution is represented by a DAGM, however, then some of the conditioned variables can be dropped. This is equivalent to enforcing conditional independence. D-Separation D-separation , or directed-separation is a notion of connectedness in DAGMs in which two (sets of) variables may or may not be connected conditioned on a third (set of) variable(s); where D-connection implies conditional dependence and d-separation implies conditional independence . In particular, we say that x_A \\bot x_B | x_C x_A \\bot x_B | x_C if every variable in A A is d-separated from every variable in B B conditioned on all the variables in C C . We will look at two methods for checking if an independence is true: A depth-first search algorithm and Bayes Balls . DFS Algorithm for checking independence To check if an independence is true, we can cycle through each node in A A , do a depth-first search to reach every node in B B , and examine the path between them. If all of the paths are d-separated, then we can assert x_A \\bot x_B | x_C x_A \\bot x_B | x_C Thus, it will be sufficient to consider triples of nodes. Note It is not totally clear to me why it is sufficient to consider triples of nodes. This is simply stated \"as is\" on the lecture slides. Lets go through some of the most common triples. Tip It was suggested in class that these types of examples make for really good midterm questions! 1. Chain Question : When we condition on y y , are x x and z z independent? Answer : From the graph, we get P(x, y, z) = P(x)P(y|x)P(z|y) P(x, y, z) = P(x)P(y|x)P(z|y) which implies \\begin{align} P(z | x, y) = \\frac{P(x, y, z)}{P(x, y)} \\\\ = \\frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\\\ = P(z | y) \\\\ \\end{align} \\begin{align} P(z | x, y) &= \\frac{P(x, y, z)}{P(x, y)} \\\\ &= \\frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\\\ &= P(z | y) \\\\ \\end{align} \\therefore \\therefore P(z | x, y) = P(z | y) P(z | x, y) = P(z | y) and so by \\star\\star \\star\\star , x \\bot z | y x \\bot z | y . Tip It is helpful to think about x x as the past, y y as the present and z z as the future when working with chains such as this one. 2. Common Cause Where we think of y y as the \"common cause\" of the two independent effects x x and z z . Question : When we condition on y y , are x x and z z independent? Answer : From the graph, we get P(x, y, z) = P(y)P(x|y)P(z|y) P(x, y, z) = P(y)P(x|y)P(z|y) which implies \\begin{align} P(x, z | y) = \\frac{P(x, y, z)}{P(y)} \\\\ = \\frac{P(y)P(x|y)P(z|y)}{P(y)} \\\\ = P(x|y)P(z|y) \\\\ \\end{align} \\begin{align} P(x, z | y) &= \\frac{P(x, y, z)}{P(y)} \\\\ &= \\frac{P(y)P(x|y)P(z|y)}{P(y)} \\\\ &= P(x|y)P(z|y) \\\\ \\end{align} \\therefore \\therefore P(x, z| y) = P(x|y)P(z|y) P(x, z| y) = P(x|y)P(z|y) and so by \\star \\star , x \\bot z | y x \\bot z | y . 3. Explaining Away Question : When we condition on y y , are x x and z z independent? Answer : From the graph, we get P(x, y, z) = P(x)P(z)P(y|x, z) P(x, y, z) = P(x)P(z)P(y|x, z) which implies \\begin{align} P(z | x, y) = \\frac{P(x)P(z)P(y | x, z)}{P(x)P(y|x)} \\\\ = \\frac{P(z)P(y | x, z)}{P(y|x)} \\\\ \\not = P(z|y) \\\\ \\end{align} \\begin{align} P(z | x, y) &= \\frac{P(x)P(z)P(y | x, z)}{P(x)P(y|x)} \\\\ &= \\frac{P(z)P(y | x, z)}{P(y|x)} \\\\ &\\not = P(z|y) \\\\ \\end{align} \\therefore \\therefore P(z | x, y) \\not = P(z|y) P(z | x, y) \\not = P(z|y) and so by \\star\\star \\star\\star , x \\not \\bot z | y x \\not \\bot z | y . In fact, x x and z z are marginally independent , but given y y they are conditionally independent . This important effect is called explaining away ( Berkson\u2019s paradox ). Example Imaging flipping two coins independently, represented by events x x and z z . Furthermore, let y=1 y=1 if the coins come up the same and y=0 y=0 if they come up differently. Clearly, x x and z z are independent, but if I tell you y y , they become coupled! Bayes-Balls Algorithm An alternative algorithm for determining conditional independence is the Bayes Balls algorithm. To check if x_A \\bot x_B | x_C x_A \\bot x_B | x_C we need to check if every variable in A A is d-seperated from every variable in B B conditioned on all variables in C C . In other words, given that all the nodes in x_C x_C are \"clamped\", when we \"wiggle\" nodes x_A x_A can we change any of the nodes in x_B x_B ? In general, the algorithm works as follows: We shade all nodes x_C x_C , place \"balls\" at each node in x_A x_A (or x_B x_B ), let them \"bounce\" around according to some rules, and then ask if any of the balls reach any of the nodes in x_B x_B (or x_A x_A ). The rules are as follows : including the boundary rules : where arrows indicate paths the balls can travel, and arrows with bars indicate paths the balls cannot travel. Note Notice balls can travel opposite to edge directions! Here\u2019s a trick for the explaining away case: If y y or any of its descendants is shaded , the ball passes through. Canonical Micrographs For reference, here are some canonical micrographs and the Bayes Balls algorithmic rules that apply to them Tip See this video for an easy way to remember all the rules. Examples Question : In the following graph, is x_1 \\bot x_6 | \\{x_2, x_3\\} x_1 \\bot x_6 | \\{x_2, x_3\\} ? Answer : Yes, by the Bayes Balls algorithm. Question : In the following graph, is x_2 \\bot x_3 | \\{x_1, x_6\\} x_2 \\bot x_3 | \\{x_1, x_6\\} ? Answer : No, by the Bayes Balls algorithm. Plates Because Bayesian methods treat parameters as random variables, we would like to include them in the graphical model. One way to do this is to repeat all the iid observations explicitly and show the parameter only once. A better way is to use plates , in which repeated quantities that are iid are put in a box Plates are like \u201cmacros\u201d that allow you to draw a very complicated graphical model with a simpler notation. The rules of plates are simple : repeat every structure in a box a number of times given by the integer in the corner of the box (e.g. N N ), updating the plate index variable (e.g. n n ) as you go. Duplicate every arrow going into the plate and every arrow leaving the plate by connecting the arrows to each copy of the structure. Nested Plates Plates can be nested, in which case their arrows get duplicated also, according to the rule: draw an arrow from every copy of the source node to every copy of the destination node. Plates can also cross (intersect), in which case the nodes at the intersection have multiple indices and get duplicated a number of times equal to the product of the duplication numbers on all the plates containing them. Example of a DAGM: Markov Chain Markov chains are a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. In other words, it is a model that satisfies the Markov property , i.e., conditional on the present state of the system, its future and past states are independent. Warning I don't really understand the difference between the two models given on the slides (and shown above). Are they both Markov chains? In the second model, the probability of an event depends not just on the previous node but on the previous node of the previous node. Jesse went over this only very briefly in lecture. Unobserved Variables Certain variables in our models may be unobserved ( Q Q in the example given below), either some of the time or always, at training time or at test time. Note Graphically, we will use shading to indicate observation Partially Unobserved (Missing) Variables If variables are occasionally unobserved then they are missing data , e.g., undefined inputs, missing class labels, erroneous target values. In this case, we can still model the joint distribution, but we define a new cost function in which we sum out or marginalize the missing values at training or test time \\[\\ell(\\theta ; \\mathcal D) = \\sum_{\\text{complete}} \\log p(x^c, y^c | \\theta) + \\sum_{\\text{missing}} \\log p(x^m | \\theta)\\] \\[= \\sum_{\\text{complete}} \\log p(x^c, y^c | \\theta) + \\sum_{\\text{missing}} \\log \\sum_y p(x^m, y | \\theta)\\] Note Recall that p(x) = \\sum_q p(x, q) p(x) = \\sum_q p(x, q) . Latent What to do when a variable z z is always unobserved? Depends on where it appears in our model. If we never condition on it when computing the probability of the variables we do observe, then we can just forget about it and integrate it out. E.g., given y y , x x fit the model p(z, y|x) = p(z|y)p(y|x, w)p(w) p(z, y|x) = p(z|y)p(y|x, w)p(w) . In other words if it is a leaf node. However, if z z is conditioned on, we need to model it. E.g. given y y , x x fit the model p(y|x) = \\sum_z p(y|x, z)p(z) p(y|x) = \\sum_z p(y|x, z)p(z) . Where do latent variables come from? Latent variables may appear naturally, from the structure of the problem (because something wasn\u2019t measured, because of faulty sensors, occlusion, privacy, etc.). But we also may want to intentionally introduce latent variables to model complex dependencies between variables without looking at the dependencies between them directly. This can actually simplify the model (e.g., mixtures). Mixture models Think about the following two sets of data, and notice how there is some underlying structure not dependent on x . The most basic latent variable model might introduce a single discrete node, z z , in order to better model the data. This allows different submodels (experts) to contribute to the (conditional) density model in different parts of the space (known as a mixture of experts ). Note The basic idea is to divide conquer: use simple parts to build complex models (e.g., multimodal densities, or piecewise-linear regressions). Mixture densities What if the class is unobserved ? Then we sum it out p(x | \\theta) = \\sum_{k=1}^Kp(z=k | \\theta_z)p(x|z=k, \\theta_k) \\\\ = \\sum_{k=1}^K\\alpha_k p_k(x|\\theta_k) p(x | \\theta) = \\sum_{k=1}^Kp(z=k | \\theta_z)p(x|z=k, \\theta_k) \\\\ = \\sum_{k=1}^K\\alpha_k p_k(x|\\theta_k) where the mixing proportions , \\alpha_k \\alpha_k sum to 1, i.e. \\sum_k\\alpha_k = 1 \\sum_k\\alpha_k = 1 . We can use Bayes' rule to compute the posterior probability of the mixture component given some data: p(z=k | x, \\theta_z) = \\frac{\\alpha_k p_k(x|\\theta_k)}{\\sum_j\\alpha_j p_j(x|\\theta_j)} p(z=k | x, \\theta_z) = \\frac{\\alpha_k p_k(x|\\theta_k)}{\\sum_j\\alpha_j p_j(x|\\theta_j)} these quantities are called responsibilities . Example: Gaussian Mixture Models Consider a mixture of K K Gaussian componentns p(x | \\theta) = \\sum_k \\alpha_k \\mathcal N(x | \\mu_k, \\Sigma_k) \\\\ p(z = k | x, \\theta) = \\frac{\\alpha_k \\mathcal N(x | \\mu_k, \\Sigma_k)}{\\sum_j \\alpha_j \\mathcal N(x | \\mu_j, \\Sigma_j)} \\\\ \\ell(\\theta ; \\mathcal D) = \\sum_n \\log \\sum_k \\alpha_k \\mathcal N(x^{(n)} | \\mu_k, \\Sigma_k) \\\\ p(x | \\theta) = \\sum_k \\alpha_k \\mathcal N(x | \\mu_k, \\Sigma_k) \\\\ p(z = k | x, \\theta) = \\frac{\\alpha_k \\mathcal N(x | \\mu_k, \\Sigma_k)}{\\sum_j \\alpha_j \\mathcal N(x | \\mu_j, \\Sigma_j)} \\\\ \\ell(\\theta ; \\mathcal D) = \\sum_n \\log \\sum_k \\alpha_k \\mathcal N(x^{(n)} | \\mu_k, \\Sigma_k) \\\\ Density model: p(x | \\theta) p(x | \\theta) is a familiarity signal. Clustering: p(z | x, \\theta) p(z | x, \\theta) is the assignment rule, - \\ell(\\theta) - \\ell(\\theta) is the cost. Warning I didn't really understand this example. Example: Mixtures of Experts Mixtures of experts , also known as conditional mixtures are exactly like a class-conditional model, but the class is unobserved and so we sum it out: p(y | x, \\theta) = \\sum_{k=1}^Kp(z=k|x, \\theta_z)p(y|z=k, x, \\theta_K) \\\\ = \\sum_k \\alpha_k (x | \\theta_z)p_k(y | x, \\theta_k) \\\\ p(y | x, \\theta) = \\sum_{k=1}^Kp(z=k|x, \\theta_z)p(y|z=k, x, \\theta_K) \\\\ = \\sum_k \\alpha_k (x | \\theta_z)p_k(y | x, \\theta_k) \\\\ where \\sum_k \\alpha_k (x) = 1 \\; \\forall x \\sum_k \\alpha_k (x) = 1 \\; \\forall x . This is a harder problem than the previous example, as we must learn \\alpha(x) \\alpha(x) , often called the gating function (unless we chose z z to be independent of x x ). However, we can still use Bayes' rule to compute the posterior probability of the mixture components given some data: p(z = k | x, y, \\theta) = \\frac{\\alpha_k(x) p_k(y| x, \\theta_k)}{\\sum_j\\alpha_j(x) p_j(y|x_j, \\theta_j)} p(z = k | x, y, \\theta) = \\frac{\\alpha_k(x) p_k(y| x, \\theta_k)}{\\sum_j\\alpha_j(x) p_j(y|x_j, \\theta_j)} Example: Mixtures of Linear Regression Experts In this model, each expert generates data according to a linear function of the input plus additive Gaussian noise p(y | x, \\theta) = \\sum_k \\alpha_k \\mathcal N(y | \\beta_k^Tx, \\sigma_k^2) p(y | x, \\theta) = \\sum_k \\alpha_k \\mathcal N(y | \\beta_k^Tx, \\sigma_k^2) where the gating function can be a softmax classifier \\alpha_k(x) = p(z=k | x) = \\frac{e^{\\eta_k^Tx}}{\\sum_je^{\\eta_k^Tx}} \\alpha_k(x) = p(z=k | x) = \\frac{e^{\\eta_k^Tx}}{\\sum_je^{\\eta_k^Tx}} Remember: we are not modeling the density of the inputs x x . Gradient learning with mixtures Error Left off at Gradient learning with Mixtures Appendix Useful Resources Metacademy lesson on Bayes Balls . In fact, that link will bring you to a short course on a couple important concepts for this corse, including conditional probability, conditional independence, Bayesian networks and d-separation. A video on how to memorize the Bayes Balls rules (this is linked in the above course). Glossary of Terms","title":"Week 3"},{"location":"lectures/week_3/#week-3-directed-graphical-models","text":"","title":"Week 3: Directed Graphical Models"},{"location":"lectures/week_3/#assigned-reading","text":"Murphy: Chapters 10-12 (excluding * sections) Kevin Murphy's page on graphical models Roger Grosse's slides on backprop","title":"Assigned Reading"},{"location":"lectures/week_3/#overview","text":"Graphical notations Conditional independence Bayes Balls Latent variables Common motifs","title":"Overview"},{"location":"lectures/week_3/#graphical-model-notation","text":"The joint distribution of N N random variables can be computed by the chain rule p(x_{1, ..., N}) = p(x_1)p(x_2|x_1)p(x_3 | x_2, x_1) \\ldots p(x_{1, ..., N}) = p(x_1)p(x_2|x_1)p(x_3 | x_2, x_1) \\ldots this is true for any joint distribution over any random variables . More formally, in probability the chain rule for two random variables is p(x, y) = p(x | y)p(y) p(x, y) = p(x | y)p(y) and for N N random variables p(\\cap^N_{i=1}) = \\prod_{k=1}^N p(x_k | \\cap^{k-1}_{j=1} x_j) p(\\cap^N_{i=1}) = \\prod_{k=1}^N p(x_k | \\cap^{k-1}_{j=1} x_j) Note Note that this is a bit of an abuse of notation, but p(x_k | \\cap^{k-1}_{j=1} x_j) p(x_k | \\cap^{k-1}_{j=1} x_j) will collpase to p(x_1) p(x_1) when k k = 1. Graphical, we might represent a model p(x_i, x_{\\pi_i}) = p(x_{\\pi_i})p(x_i | x_{\\pi_i}) p(x_i, x_{\\pi_i}) = p(x_{\\pi_i})p(x_i | x_{\\pi_i}) as where nodes represent random variables arrows mean \"conditioned on\", e.g. \" x_i x_i is conditioned on x_{\\pi_1} x_{\\pi_1} \". For example, the graphical model p(x_{1, ..., 6}) p(x_{1, ..., 6}) is represented as This is what the model looks like with no assumptions on the conditional dependence between variables (said otherwise, we assume full conditional dependence of the joint distribution as per the chain rule). This model will scale poorly (exponential with the number of parameters, or k^n k^n where k k are states and n n are random variables, or nodes.). We can simplify the model by building in our assumptions about the conditional probabilities. More explicitly, a directed graphical model implies a restricted factorization of the joint distribution.","title":"Graphical model notation"},{"location":"lectures/week_3/#conditional-independence","text":"Let X X be the set of nodes in our graph (the random variables of our model), then two (sets of) variables X_A X_A , X_B X_B are conditionally independent given a third variable X_C X_C \\[(X_A \\perp X_B | X_C)\\] if \\Leftrightarrow p(X_A, X_B | X_C) = p(X_A | X_C)p(X_B | X_C) \\; (\\star) \\Leftrightarrow p(X_A, X_B | X_C) = p(X_A | X_C)p(X_B | X_C) \\; (\\star) \\Leftrightarrow p(X_A | X_B, X_C) = p(X_A | X_C) \\; (\\star\\star) \\Leftrightarrow p(X_A | X_B, X_C) = p(X_A | X_C) \\; (\\star\\star) for all X_c X_c . Note \\star\\star \\star\\star is especially important, and we use this several times throughout the lecture. Only a subset of all distributions respect any given (nontrivial) conditional independence statement. The subset of distributions that respect all the CI assumptions we make is the family of distributions consistent with our assumptions. Probabilistic graphical models are a powerful, elegant and simple way to specify such a family.","title":"Conditional Independence"},{"location":"lectures/week_3/#directed-acyclic-graphical-models-dagm","text":"A directed acyclic graphical model over N N random variables looks like p(x_{1, ..., N}) = \\prod_i^Np(x_i | x_{\\pi_i}) p(x_{1, ..., N}) = \\prod_i^Np(x_i | x_{\\pi_i}) where x_i x_i is a random variable (node in the graphical model) and x_{\\pi_i} x_{\\pi_i} are the parents of this node. In other words, the joint distribution of a DAGM factors into a product of local conditional distributions , where each random variable (or node) is conditionally dependent on its parent node(s), which could be empty. Tip The Wikipedia entry on Graphical models is helpful, particularly the section on Bayesian networks . Notice the difference between a DAGM and the chain rule for probability we introduced early: we are conditioning on parent nodes as opposed to every node . Therefore, the model that represents this distribution is exponential in the fan-in of each node (the number of nodes in the parent set), instead of in N N .","title":"Directed acyclic graphical models (DAGM)"},{"location":"lectures/week_3/#independence-assumptions-on-dagms","text":"Lets look again at the graphical model p(x_{1, ..., 6}) p(x_{1, ..., 6}) we introduced above. First, lets sort the DAGM topologically. The conditional independence of our random variables becomes x_i \\bot x_{\\widetilde{\\pi_i}} | x_{\\pi_i} x_i \\bot x_{\\widetilde{\\pi_i}} | x_{\\pi_i} so random variables x_i x_i and x_{\\widetilde{\\pi_i}} x_{\\widetilde{\\pi_i}} are conditionally independent of each other but conditionally dependent on their parent nodes x_{\\pi_i} x_{\\pi_i} . Note To topological sort or order a DAGM means to sort all parents before their children. Lastly, lets place some assumptions on the conditional dependence of our random variables. Say our model looks like What have the assumptions done to our joint distribution represented by our model? p(x_{1, ..., 6}) = p(x_1)p(x_2 | x_1)p(x_3 | x_1)p(x_4 | x_2)p(x_5 | x_3)p(x_6 | x_2, x_5) p(x_{1, ..., 6}) = p(x_1)p(x_2 | x_1)p(x_3 | x_1)p(x_4 | x_2)p(x_5 | x_3)p(x_6 | x_2, x_5) Cleary our assumptions on conditional independence have vastly simplified the model. Now Suppose each is x_i x_i is a binary random variable. Our assumptions on conditional independence also reduce the dimensionality of our model","title":"Independence assumptions on DAGMs"},{"location":"lectures/week_3/#missing-edges","text":"Missing edges imply conditional independence . Recall that from the chain rule, we can get (for any joint distribution) p(\\cap^N_{i=1}) = \\prod_{k=1}^N p(x_k | \\cap^{k-1}_{j=1} x_j) p(\\cap^N_{i=1}) = \\prod_{k=1}^N p(x_k | \\cap^{k-1}_{j=1} x_j) If our joint distribution is represented by a DAGM, however, then some of the conditioned variables can be dropped. This is equivalent to enforcing conditional independence.","title":"Missing Edges"},{"location":"lectures/week_3/#d-separation","text":"D-separation , or directed-separation is a notion of connectedness in DAGMs in which two (sets of) variables may or may not be connected conditioned on a third (set of) variable(s); where D-connection implies conditional dependence and d-separation implies conditional independence . In particular, we say that x_A \\bot x_B | x_C x_A \\bot x_B | x_C if every variable in A A is d-separated from every variable in B B conditioned on all the variables in C C . We will look at two methods for checking if an independence is true: A depth-first search algorithm and Bayes Balls .","title":"D-Separation"},{"location":"lectures/week_3/#dfs-algorithm-for-checking-independence","text":"To check if an independence is true, we can cycle through each node in A A , do a depth-first search to reach every node in B B , and examine the path between them. If all of the paths are d-separated, then we can assert x_A \\bot x_B | x_C x_A \\bot x_B | x_C Thus, it will be sufficient to consider triples of nodes. Note It is not totally clear to me why it is sufficient to consider triples of nodes. This is simply stated \"as is\" on the lecture slides. Lets go through some of the most common triples. Tip It was suggested in class that these types of examples make for really good midterm questions! 1. Chain Question : When we condition on y y , are x x and z z independent? Answer : From the graph, we get P(x, y, z) = P(x)P(y|x)P(z|y) P(x, y, z) = P(x)P(y|x)P(z|y) which implies \\begin{align} P(z | x, y) = \\frac{P(x, y, z)}{P(x, y)} \\\\ = \\frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\\\ = P(z | y) \\\\ \\end{align} \\begin{align} P(z | x, y) &= \\frac{P(x, y, z)}{P(x, y)} \\\\ &= \\frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\\\ &= P(z | y) \\\\ \\end{align} \\therefore \\therefore P(z | x, y) = P(z | y) P(z | x, y) = P(z | y) and so by \\star\\star \\star\\star , x \\bot z | y x \\bot z | y . Tip It is helpful to think about x x as the past, y y as the present and z z as the future when working with chains such as this one. 2. Common Cause Where we think of y y as the \"common cause\" of the two independent effects x x and z z . Question : When we condition on y y , are x x and z z independent? Answer : From the graph, we get P(x, y, z) = P(y)P(x|y)P(z|y) P(x, y, z) = P(y)P(x|y)P(z|y) which implies \\begin{align} P(x, z | y) = \\frac{P(x, y, z)}{P(y)} \\\\ = \\frac{P(y)P(x|y)P(z|y)}{P(y)} \\\\ = P(x|y)P(z|y) \\\\ \\end{align} \\begin{align} P(x, z | y) &= \\frac{P(x, y, z)}{P(y)} \\\\ &= \\frac{P(y)P(x|y)P(z|y)}{P(y)} \\\\ &= P(x|y)P(z|y) \\\\ \\end{align} \\therefore \\therefore P(x, z| y) = P(x|y)P(z|y) P(x, z| y) = P(x|y)P(z|y) and so by \\star \\star , x \\bot z | y x \\bot z | y . 3. Explaining Away Question : When we condition on y y , are x x and z z independent? Answer : From the graph, we get P(x, y, z) = P(x)P(z)P(y|x, z) P(x, y, z) = P(x)P(z)P(y|x, z) which implies \\begin{align} P(z | x, y) = \\frac{P(x)P(z)P(y | x, z)}{P(x)P(y|x)} \\\\ = \\frac{P(z)P(y | x, z)}{P(y|x)} \\\\ \\not = P(z|y) \\\\ \\end{align} \\begin{align} P(z | x, y) &= \\frac{P(x)P(z)P(y | x, z)}{P(x)P(y|x)} \\\\ &= \\frac{P(z)P(y | x, z)}{P(y|x)} \\\\ &\\not = P(z|y) \\\\ \\end{align} \\therefore \\therefore P(z | x, y) \\not = P(z|y) P(z | x, y) \\not = P(z|y) and so by \\star\\star \\star\\star , x \\not \\bot z | y x \\not \\bot z | y . In fact, x x and z z are marginally independent , but given y y they are conditionally independent . This important effect is called explaining away ( Berkson\u2019s paradox ). Example Imaging flipping two coins independently, represented by events x x and z z . Furthermore, let y=1 y=1 if the coins come up the same and y=0 y=0 if they come up differently. Clearly, x x and z z are independent, but if I tell you y y , they become coupled!","title":"DFS Algorithm for checking independence"},{"location":"lectures/week_3/#bayes-balls-algorithm","text":"An alternative algorithm for determining conditional independence is the Bayes Balls algorithm. To check if x_A \\bot x_B | x_C x_A \\bot x_B | x_C we need to check if every variable in A A is d-seperated from every variable in B B conditioned on all variables in C C . In other words, given that all the nodes in x_C x_C are \"clamped\", when we \"wiggle\" nodes x_A x_A can we change any of the nodes in x_B x_B ? In general, the algorithm works as follows: We shade all nodes x_C x_C , place \"balls\" at each node in x_A x_A (or x_B x_B ), let them \"bounce\" around according to some rules, and then ask if any of the balls reach any of the nodes in x_B x_B (or x_A x_A ). The rules are as follows : including the boundary rules : where arrows indicate paths the balls can travel, and arrows with bars indicate paths the balls cannot travel. Note Notice balls can travel opposite to edge directions! Here\u2019s a trick for the explaining away case: If y y or any of its descendants is shaded , the ball passes through.","title":"Bayes-Balls Algorithm"},{"location":"lectures/week_3/#canonical-micrographs","text":"For reference, here are some canonical micrographs and the Bayes Balls algorithmic rules that apply to them Tip See this video for an easy way to remember all the rules.","title":"Canonical Micrographs"},{"location":"lectures/week_3/#examples","text":"Question : In the following graph, is x_1 \\bot x_6 | \\{x_2, x_3\\} x_1 \\bot x_6 | \\{x_2, x_3\\} ? Answer : Yes, by the Bayes Balls algorithm. Question : In the following graph, is x_2 \\bot x_3 | \\{x_1, x_6\\} x_2 \\bot x_3 | \\{x_1, x_6\\} ? Answer : No, by the Bayes Balls algorithm.","title":"Examples"},{"location":"lectures/week_3/#plates","text":"Because Bayesian methods treat parameters as random variables, we would like to include them in the graphical model. One way to do this is to repeat all the iid observations explicitly and show the parameter only once. A better way is to use plates , in which repeated quantities that are iid are put in a box Plates are like \u201cmacros\u201d that allow you to draw a very complicated graphical model with a simpler notation. The rules of plates are simple : repeat every structure in a box a number of times given by the integer in the corner of the box (e.g. N N ), updating the plate index variable (e.g. n n ) as you go. Duplicate every arrow going into the plate and every arrow leaving the plate by connecting the arrows to each copy of the structure.","title":"Plates"},{"location":"lectures/week_3/#nested-plates","text":"Plates can be nested, in which case their arrows get duplicated also, according to the rule: draw an arrow from every copy of the source node to every copy of the destination node. Plates can also cross (intersect), in which case the nodes at the intersection have multiple indices and get duplicated a number of times equal to the product of the duplication numbers on all the plates containing them.","title":"Nested Plates"},{"location":"lectures/week_3/#example-of-a-dagm-markov-chain","text":"Markov chains are a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. In other words, it is a model that satisfies the Markov property , i.e., conditional on the present state of the system, its future and past states are independent. Warning I don't really understand the difference between the two models given on the slides (and shown above). Are they both Markov chains? In the second model, the probability of an event depends not just on the previous node but on the previous node of the previous node. Jesse went over this only very briefly in lecture.","title":"Example of a DAGM: Markov Chain"},{"location":"lectures/week_3/#unobserved-variables","text":"Certain variables in our models may be unobserved ( Q Q in the example given below), either some of the time or always, at training time or at test time. Note Graphically, we will use shading to indicate observation","title":"Unobserved Variables"},{"location":"lectures/week_3/#partially-unobserved-missing-variables","text":"If variables are occasionally unobserved then they are missing data , e.g., undefined inputs, missing class labels, erroneous target values. In this case, we can still model the joint distribution, but we define a new cost function in which we sum out or marginalize the missing values at training or test time \\[\\ell(\\theta ; \\mathcal D) = \\sum_{\\text{complete}} \\log p(x^c, y^c | \\theta) + \\sum_{\\text{missing}} \\log p(x^m | \\theta)\\] \\[= \\sum_{\\text{complete}} \\log p(x^c, y^c | \\theta) + \\sum_{\\text{missing}} \\log \\sum_y p(x^m, y | \\theta)\\] Note Recall that p(x) = \\sum_q p(x, q) p(x) = \\sum_q p(x, q) .","title":"Partially Unobserved (Missing) Variables"},{"location":"lectures/week_3/#latent","text":"What to do when a variable z z is always unobserved? Depends on where it appears in our model. If we never condition on it when computing the probability of the variables we do observe, then we can just forget about it and integrate it out. E.g., given y y , x x fit the model p(z, y|x) = p(z|y)p(y|x, w)p(w) p(z, y|x) = p(z|y)p(y|x, w)p(w) . In other words if it is a leaf node. However, if z z is conditioned on, we need to model it. E.g. given y y , x x fit the model p(y|x) = \\sum_z p(y|x, z)p(z) p(y|x) = \\sum_z p(y|x, z)p(z) .","title":"Latent"},{"location":"lectures/week_3/#where-do-latent-variables-come-from","text":"Latent variables may appear naturally, from the structure of the problem (because something wasn\u2019t measured, because of faulty sensors, occlusion, privacy, etc.). But we also may want to intentionally introduce latent variables to model complex dependencies between variables without looking at the dependencies between them directly. This can actually simplify the model (e.g., mixtures).","title":"Where do latent variables come from?"},{"location":"lectures/week_3/#mixture-models","text":"Think about the following two sets of data, and notice how there is some underlying structure not dependent on x . The most basic latent variable model might introduce a single discrete node, z z , in order to better model the data. This allows different submodels (experts) to contribute to the (conditional) density model in different parts of the space (known as a mixture of experts ). Note The basic idea is to divide conquer: use simple parts to build complex models (e.g., multimodal densities, or piecewise-linear regressions).","title":"Mixture models"},{"location":"lectures/week_3/#mixture-densities","text":"What if the class is unobserved ? Then we sum it out p(x | \\theta) = \\sum_{k=1}^Kp(z=k | \\theta_z)p(x|z=k, \\theta_k) \\\\ = \\sum_{k=1}^K\\alpha_k p_k(x|\\theta_k) p(x | \\theta) = \\sum_{k=1}^Kp(z=k | \\theta_z)p(x|z=k, \\theta_k) \\\\ = \\sum_{k=1}^K\\alpha_k p_k(x|\\theta_k) where the mixing proportions , \\alpha_k \\alpha_k sum to 1, i.e. \\sum_k\\alpha_k = 1 \\sum_k\\alpha_k = 1 . We can use Bayes' rule to compute the posterior probability of the mixture component given some data: p(z=k | x, \\theta_z) = \\frac{\\alpha_k p_k(x|\\theta_k)}{\\sum_j\\alpha_j p_j(x|\\theta_j)} p(z=k | x, \\theta_z) = \\frac{\\alpha_k p_k(x|\\theta_k)}{\\sum_j\\alpha_j p_j(x|\\theta_j)} these quantities are called responsibilities .","title":"Mixture densities"},{"location":"lectures/week_3/#example-gaussian-mixture-models","text":"Consider a mixture of K K Gaussian componentns p(x | \\theta) = \\sum_k \\alpha_k \\mathcal N(x | \\mu_k, \\Sigma_k) \\\\ p(z = k | x, \\theta) = \\frac{\\alpha_k \\mathcal N(x | \\mu_k, \\Sigma_k)}{\\sum_j \\alpha_j \\mathcal N(x | \\mu_j, \\Sigma_j)} \\\\ \\ell(\\theta ; \\mathcal D) = \\sum_n \\log \\sum_k \\alpha_k \\mathcal N(x^{(n)} | \\mu_k, \\Sigma_k) \\\\ p(x | \\theta) = \\sum_k \\alpha_k \\mathcal N(x | \\mu_k, \\Sigma_k) \\\\ p(z = k | x, \\theta) = \\frac{\\alpha_k \\mathcal N(x | \\mu_k, \\Sigma_k)}{\\sum_j \\alpha_j \\mathcal N(x | \\mu_j, \\Sigma_j)} \\\\ \\ell(\\theta ; \\mathcal D) = \\sum_n \\log \\sum_k \\alpha_k \\mathcal N(x^{(n)} | \\mu_k, \\Sigma_k) \\\\ Density model: p(x | \\theta) p(x | \\theta) is a familiarity signal. Clustering: p(z | x, \\theta) p(z | x, \\theta) is the assignment rule, - \\ell(\\theta) - \\ell(\\theta) is the cost. Warning I didn't really understand this example.","title":"Example: Gaussian Mixture Models"},{"location":"lectures/week_3/#example-mixtures-of-experts","text":"Mixtures of experts , also known as conditional mixtures are exactly like a class-conditional model, but the class is unobserved and so we sum it out: p(y | x, \\theta) = \\sum_{k=1}^Kp(z=k|x, \\theta_z)p(y|z=k, x, \\theta_K) \\\\ = \\sum_k \\alpha_k (x | \\theta_z)p_k(y | x, \\theta_k) \\\\ p(y | x, \\theta) = \\sum_{k=1}^Kp(z=k|x, \\theta_z)p(y|z=k, x, \\theta_K) \\\\ = \\sum_k \\alpha_k (x | \\theta_z)p_k(y | x, \\theta_k) \\\\ where \\sum_k \\alpha_k (x) = 1 \\; \\forall x \\sum_k \\alpha_k (x) = 1 \\; \\forall x . This is a harder problem than the previous example, as we must learn \\alpha(x) \\alpha(x) , often called the gating function (unless we chose z z to be independent of x x ). However, we can still use Bayes' rule to compute the posterior probability of the mixture components given some data: p(z = k | x, y, \\theta) = \\frac{\\alpha_k(x) p_k(y| x, \\theta_k)}{\\sum_j\\alpha_j(x) p_j(y|x_j, \\theta_j)} p(z = k | x, y, \\theta) = \\frac{\\alpha_k(x) p_k(y| x, \\theta_k)}{\\sum_j\\alpha_j(x) p_j(y|x_j, \\theta_j)}","title":"Example: Mixtures of Experts"},{"location":"lectures/week_3/#example-mixtures-of-linear-regression-experts","text":"In this model, each expert generates data according to a linear function of the input plus additive Gaussian noise p(y | x, \\theta) = \\sum_k \\alpha_k \\mathcal N(y | \\beta_k^Tx, \\sigma_k^2) p(y | x, \\theta) = \\sum_k \\alpha_k \\mathcal N(y | \\beta_k^Tx, \\sigma_k^2) where the gating function can be a softmax classifier \\alpha_k(x) = p(z=k | x) = \\frac{e^{\\eta_k^Tx}}{\\sum_je^{\\eta_k^Tx}} \\alpha_k(x) = p(z=k | x) = \\frac{e^{\\eta_k^Tx}}{\\sum_je^{\\eta_k^Tx}} Remember: we are not modeling the density of the inputs x x .","title":"Example: Mixtures of Linear Regression Experts"},{"location":"lectures/week_3/#gradient-learning-with-mixtures","text":"Error Left off at Gradient learning with Mixtures","title":"Gradient learning with mixtures"},{"location":"lectures/week_3/#appendix","text":"","title":"Appendix"},{"location":"lectures/week_3/#useful-resources","text":"Metacademy lesson on Bayes Balls . In fact, that link will bring you to a short course on a couple important concepts for this corse, including conditional probability, conditional independence, Bayesian networks and d-separation. A video on how to memorize the Bayes Balls rules (this is linked in the above course).","title":"Useful Resources"},{"location":"lectures/week_3/#glossary-of-terms","text":"","title":"Glossary of Terms"},{"location":"lectures/week_4/","text":"Week 4: Undirected Graphical Models Assigned Reading Murphy: Chapters 19-19.5 Overview Will fill out when I get my notebook back. Directed Graphical Models (a Review) So far, we have seen directed acyclic graph models (DAGMs) . These models represent large joint distributions using local relationships specified by the graph, where each random variable is a node and the edges specify the conditional dependence between random variables (and therefore missing edges imply conditional independence). Graphically, these models looked like The graph factorized according to the local conditional probabilities p(x_{1, ..., N}) = \\prod_i^Np(x_i | x_{\\pi_i}) p(x_{1, ..., N}) = \\prod_i^Np(x_i | x_{\\pi_i}) where x_{\\pi_i} x_{\\pi_i} are the parents of node x_i x_i . Each node is conditionally independent of its non-descendents given its parents \\ \\{x_i \\bot x_{\\tilde\\pi_i} | x_{\\pi_i}\\} \\quad \\forall_i \\ \\{x_i \\bot x_{\\tilde\\pi_i} | x_{\\pi_i}\\} \\quad \\forall_i Note Recall that this is simply a topological ordering of the graph (i.e. parents have lower numbers than their children) For discrete variables, each node stores a conditional probability table (CPT) of size k^n k^n , where k k is the number of discrete states and n n the number of conditionally dependent nodes. Are DAGMs always useful? For some problems, it is not always clear how to choose the direction for the edges in our DAGMs. Take the example of modeling dependencies in an image our assumptions lead to unatural conditional independence between random variables. Take for example, the Markov blanket of node X_8 X_8 mb(8) = \\{3, 7\\} \\cup \\{9, 13\\} \\cup \\{12, 4\\} mb(8) = \\{3, 7\\} \\cup \\{9, 13\\} \\cup \\{12, 4\\} Note The Markov blanket contains the parents, children and co-parents of a node. More generally, it is the set of all variables that shield the node from the rest of the network. An alternative to DAGMs, is undirected graphical models (UGMs). Undirected Graphical Models Undirected graphical models (UDGMs), also called Markov random fields (MRFs) or Markov networks, is a set of random variables described by an undirected graph. As in DAGMs, the nodes in the graph represent random variables . However, in contrast to DAGMs, edges represent probabilistic interactions between neighboring variables (as opposed to conditional dependence). Dependencies in UGMs In DGMs, we used conditional probabilities to represent the distribution of nodes given their parents. In UGMs, we use a more symmetric parameterization that captures the affinities between related variables: def . Global Markov Property : X_A \\bot X_B | X_C X_A \\bot X_B | X_C iff C separates A from B (i.e. there is no path in the graph between A and B that doesn't go through C). def . Markov Blanket (local property) : The set of nodes that renders a node t t conditionally independent of all the other nodes in the graph t \\bot \\mathcal V \\setminus cl(t) | mb(t) t \\bot \\mathcal V \\setminus cl(t) | mb(t) def . Pairwise (Markov) Property : The set of nodes that renders a node t t conditionally independent of all the other nodes in the graph s \\bot t | \\mathcal V \\setminus \\{s, t\\} \\Leftrightarrow G_{st} = 0 s \\bot t | \\mathcal V \\setminus \\{s, t\\} \\Leftrightarrow G_{st} = 0 where G \\Rightarrow L \\Rightarrow P \\Rightarrow P \\quad p(x) 0 G \\Rightarrow L \\Rightarrow P \\Rightarrow P \\quad p(x) > 0 Simple example Global: \\{1, 2\\} \\bot \\{6, 7\\} | \\{3, 4, 5\\} \\{1, 2\\} \\bot \\{6, 7\\} | \\{3, 4, 5\\} Local: 1 \\bot \\text{rest} | \\{2, 3\\} 1 \\bot \\text{rest} | \\{2, 3\\} Pairwise: 1 \\bot 7 | \\text{rest} 1 \\bot 7 | \\text{rest} Image example Error I don't know how to solve the examples on the lecture slides (slide 11). Leaving blank for now. Not all UGMs can be represented as DGMs Take the follow UGM for example (a) and our attempts at encoding this as a DGM (b, c). First, note the two conditional independencies of our UGM in (a): A \\bot C|D,B A \\bot C|D,B B \\bot D|A,C B \\bot D|A,C In (b), we are able to encode the first independence, but not the second (i.e., our DGM implies that B is dependent on D given A and C). In (c), we are again able to encode the first independence, but our model also implies that B and D are marginally independent. Not all DGMs can be represented as UGMs It is also true that not all DGMs can be represented as UGMs. One such example is the 'V-structure' that we saw in the explaining away case in lecture 3 . An undirected model is unable to capture the marginal independence, X \\bot Y X \\bot Y that holds at the same time as \\neg (X \\bot Y | Z ) \\neg (X \\bot Y | Z ) . Cliques A clique in an undirected graph is a subset of its vertices such that every two vertices in the subset are connected by an edge (i.e., the subgraph induced by the clique is complete ). def . The maximal clique is a clique that cannot be extended by including one more adjacent vertex. def . The maximum clique is a clique of the largest possible size in a given graph. For example, in the following graph a maximal clique is show in blue, while the maximum clique is shown in green. Parameterization of an UGM Let x = (x_1, ..., x_m) x = (x_1, ..., x_m) be the set of all random variables in our graph. Unlike in DGMs, there is no topological ordering associated with an undirected graph, and so we cannot use the chain rule to represent p(x) p(x) . Therefore, instead of associating conditional probabilities to each node, we associate potential functions or factors with each maximal clique in the graph. For a given clique c c , we define the potential function or factor \\psi_c(x_c | \\theta_c) \\psi_c(x_c | \\theta_c) to be any non-negative function, where x_c x_c is some subset of variables in x x . The joint distribution is the proportional to the product of clique potentials . Note Any positive distribution whose conditional independencies are represented with an UGM can be represented this way. More formally , A positive distribution p(x) 0 p(x) > 0 satisfies the conditional independence properties of an undirected graph G G iff p p can be represented as a product of factors, one per maximal clique, i.e., p(x | \\theta) = \\frac{1}{Z(\\theta)}\\prod_{c \\in \\mathcal C}\\psi_c(x_c | \\theta) p(x | \\theta) = \\frac{1}{Z(\\theta)}\\prod_{c \\in \\mathcal C}\\psi_c(x_c | \\theta) where \\mathcal C \\mathcal C is the set of all (maximal) clique of G G , and Z(\\theta) Z(\\theta) the partition function , defined as Z(\\theta)= \\sum_x \\prod_{c \\in \\mathcal C} \\psi_c(x_c|\\theta_c) Z(\\theta)= \\sum_x \\prod_{c \\in \\mathcal C} \\psi_c(x_c|\\theta_c) The factored structure of the distribution makes it possible to more efficiently do the sums/integrals needed to compute it. Lets see how to factorize the undirected graph of our running example: p(x) \\propto \\psi_{1, 2, 3}(x_1, x_2, x_3) \\psi_{2, 3, 5}(x_2, x_3, x_5) \\psi_{2, 4, 5}(x_2, x_4, x_5) \\psi_{3, 5, 6}(x_3, x_5, x_6) \\psi_{4, 5, 6, 7}(x_4, x_5, x_6, x_7) p(x) \\propto \\psi_{1, 2, 3}(x_1, x_2, x_3) \\psi_{2, 3, 5}(x_2, x_3, x_5) \\psi_{2, 4, 5}(x_2, x_4, x_5) \\psi_{3, 5, 6}(x_3, x_5, x_6) \\psi_{4, 5, 6, 7}(x_4, x_5, x_6, x_7) If the variables are discrete, we can represent the potential or energy functions as tables of (non-negative) numbers p(A, B, C, D) = \\frac{1}{Z} \\psi_{a, b}(A, B) \\psi_{b, c}(B, C) \\psi_{c, d}(C, D) \\psi_{a, d}(A, D) p(A, B, C, D) = \\frac{1}{Z} \\psi_{a, b}(A, B) \\psi_{b, c}(B, C) \\psi_{c, d}(C, D) \\psi_{a, d}(A, D) Error Why the switch from \\psi \\psi to \\phi \\phi here? It is important to note that these potential are not probabilities, but represent compatibilities between the different assignments. Factor product Given 3 disjoint sets of variables X, Y, Z X, Y, Z and factors \\psi_1(X, Y) \\psi_1(X, Y) , \\psi_2(Y, Z) \\psi_2(Y, Z) the factor product is defined as: \\psi_{X, Y, Z}(X, Y, Z) = \\psi_{X, Y}(X, Y)\\phi_{Y, Z}(Y, Z) \\psi_{X, Y, Z}(X, Y, Z) = \\psi_{X, Y}(X, Y)\\phi_{Y, Z}(Y, Z) Error Again, is the the switch from \\psi \\psi to \\phi \\phi a typo? Deliberate?","title":"Week 4"},{"location":"lectures/week_4/#week-4-undirected-graphical-models","text":"","title":"Week 4: Undirected Graphical Models"},{"location":"lectures/week_4/#assigned-reading","text":"Murphy: Chapters 19-19.5","title":"Assigned Reading"},{"location":"lectures/week_4/#overview","text":"Will fill out when I get my notebook back.","title":"Overview"},{"location":"lectures/week_4/#directed-graphical-models-a-review","text":"So far, we have seen directed acyclic graph models (DAGMs) . These models represent large joint distributions using local relationships specified by the graph, where each random variable is a node and the edges specify the conditional dependence between random variables (and therefore missing edges imply conditional independence). Graphically, these models looked like The graph factorized according to the local conditional probabilities p(x_{1, ..., N}) = \\prod_i^Np(x_i | x_{\\pi_i}) p(x_{1, ..., N}) = \\prod_i^Np(x_i | x_{\\pi_i}) where x_{\\pi_i} x_{\\pi_i} are the parents of node x_i x_i . Each node is conditionally independent of its non-descendents given its parents \\ \\{x_i \\bot x_{\\tilde\\pi_i} | x_{\\pi_i}\\} \\quad \\forall_i \\ \\{x_i \\bot x_{\\tilde\\pi_i} | x_{\\pi_i}\\} \\quad \\forall_i Note Recall that this is simply a topological ordering of the graph (i.e. parents have lower numbers than their children) For discrete variables, each node stores a conditional probability table (CPT) of size k^n k^n , where k k is the number of discrete states and n n the number of conditionally dependent nodes.","title":"Directed Graphical Models (a Review)"},{"location":"lectures/week_4/#are-dagms-always-useful","text":"For some problems, it is not always clear how to choose the direction for the edges in our DAGMs. Take the example of modeling dependencies in an image our assumptions lead to unatural conditional independence between random variables. Take for example, the Markov blanket of node X_8 X_8 mb(8) = \\{3, 7\\} \\cup \\{9, 13\\} \\cup \\{12, 4\\} mb(8) = \\{3, 7\\} \\cup \\{9, 13\\} \\cup \\{12, 4\\} Note The Markov blanket contains the parents, children and co-parents of a node. More generally, it is the set of all variables that shield the node from the rest of the network. An alternative to DAGMs, is undirected graphical models (UGMs).","title":"Are DAGMs always useful?"},{"location":"lectures/week_4/#undirected-graphical-models","text":"Undirected graphical models (UDGMs), also called Markov random fields (MRFs) or Markov networks, is a set of random variables described by an undirected graph. As in DAGMs, the nodes in the graph represent random variables . However, in contrast to DAGMs, edges represent probabilistic interactions between neighboring variables (as opposed to conditional dependence).","title":"Undirected Graphical Models"},{"location":"lectures/week_4/#dependencies-in-ugms","text":"In DGMs, we used conditional probabilities to represent the distribution of nodes given their parents. In UGMs, we use a more symmetric parameterization that captures the affinities between related variables: def . Global Markov Property : X_A \\bot X_B | X_C X_A \\bot X_B | X_C iff C separates A from B (i.e. there is no path in the graph between A and B that doesn't go through C). def . Markov Blanket (local property) : The set of nodes that renders a node t t conditionally independent of all the other nodes in the graph t \\bot \\mathcal V \\setminus cl(t) | mb(t) t \\bot \\mathcal V \\setminus cl(t) | mb(t) def . Pairwise (Markov) Property : The set of nodes that renders a node t t conditionally independent of all the other nodes in the graph s \\bot t | \\mathcal V \\setminus \\{s, t\\} \\Leftrightarrow G_{st} = 0 s \\bot t | \\mathcal V \\setminus \\{s, t\\} \\Leftrightarrow G_{st} = 0 where G \\Rightarrow L \\Rightarrow P \\Rightarrow P \\quad p(x) 0 G \\Rightarrow L \\Rightarrow P \\Rightarrow P \\quad p(x) > 0","title":"Dependencies in UGMs"},{"location":"lectures/week_4/#simple-example","text":"Global: \\{1, 2\\} \\bot \\{6, 7\\} | \\{3, 4, 5\\} \\{1, 2\\} \\bot \\{6, 7\\} | \\{3, 4, 5\\} Local: 1 \\bot \\text{rest} | \\{2, 3\\} 1 \\bot \\text{rest} | \\{2, 3\\} Pairwise: 1 \\bot 7 | \\text{rest} 1 \\bot 7 | \\text{rest}","title":"Simple example"},{"location":"lectures/week_4/#image-example","text":"Error I don't know how to solve the examples on the lecture slides (slide 11). Leaving blank for now.","title":"Image example"},{"location":"lectures/week_4/#not-all-ugms-can-be-represented-as-dgms","text":"Take the follow UGM for example (a) and our attempts at encoding this as a DGM (b, c). First, note the two conditional independencies of our UGM in (a): A \\bot C|D,B A \\bot C|D,B B \\bot D|A,C B \\bot D|A,C In (b), we are able to encode the first independence, but not the second (i.e., our DGM implies that B is dependent on D given A and C). In (c), we are again able to encode the first independence, but our model also implies that B and D are marginally independent.","title":"Not all UGMs can be represented as DGMs"},{"location":"lectures/week_4/#not-all-dgms-can-be-represented-as-ugms","text":"It is also true that not all DGMs can be represented as UGMs. One such example is the 'V-structure' that we saw in the explaining away case in lecture 3 . An undirected model is unable to capture the marginal independence, X \\bot Y X \\bot Y that holds at the same time as \\neg (X \\bot Y | Z ) \\neg (X \\bot Y | Z ) .","title":"Not all DGMs can be represented as UGMs"},{"location":"lectures/week_4/#cliques","text":"A clique in an undirected graph is a subset of its vertices such that every two vertices in the subset are connected by an edge (i.e., the subgraph induced by the clique is complete ). def . The maximal clique is a clique that cannot be extended by including one more adjacent vertex. def . The maximum clique is a clique of the largest possible size in a given graph. For example, in the following graph a maximal clique is show in blue, while the maximum clique is shown in green.","title":"Cliques"},{"location":"lectures/week_4/#parameterization-of-an-ugm","text":"Let x = (x_1, ..., x_m) x = (x_1, ..., x_m) be the set of all random variables in our graph. Unlike in DGMs, there is no topological ordering associated with an undirected graph, and so we cannot use the chain rule to represent p(x) p(x) . Therefore, instead of associating conditional probabilities to each node, we associate potential functions or factors with each maximal clique in the graph. For a given clique c c , we define the potential function or factor \\psi_c(x_c | \\theta_c) \\psi_c(x_c | \\theta_c) to be any non-negative function, where x_c x_c is some subset of variables in x x . The joint distribution is the proportional to the product of clique potentials . Note Any positive distribution whose conditional independencies are represented with an UGM can be represented this way. More formally , A positive distribution p(x) 0 p(x) > 0 satisfies the conditional independence properties of an undirected graph G G iff p p can be represented as a product of factors, one per maximal clique, i.e., p(x | \\theta) = \\frac{1}{Z(\\theta)}\\prod_{c \\in \\mathcal C}\\psi_c(x_c | \\theta) p(x | \\theta) = \\frac{1}{Z(\\theta)}\\prod_{c \\in \\mathcal C}\\psi_c(x_c | \\theta) where \\mathcal C \\mathcal C is the set of all (maximal) clique of G G , and Z(\\theta) Z(\\theta) the partition function , defined as Z(\\theta)= \\sum_x \\prod_{c \\in \\mathcal C} \\psi_c(x_c|\\theta_c) Z(\\theta)= \\sum_x \\prod_{c \\in \\mathcal C} \\psi_c(x_c|\\theta_c) The factored structure of the distribution makes it possible to more efficiently do the sums/integrals needed to compute it. Lets see how to factorize the undirected graph of our running example: p(x) \\propto \\psi_{1, 2, 3}(x_1, x_2, x_3) \\psi_{2, 3, 5}(x_2, x_3, x_5) \\psi_{2, 4, 5}(x_2, x_4, x_5) \\psi_{3, 5, 6}(x_3, x_5, x_6) \\psi_{4, 5, 6, 7}(x_4, x_5, x_6, x_7) p(x) \\propto \\psi_{1, 2, 3}(x_1, x_2, x_3) \\psi_{2, 3, 5}(x_2, x_3, x_5) \\psi_{2, 4, 5}(x_2, x_4, x_5) \\psi_{3, 5, 6}(x_3, x_5, x_6) \\psi_{4, 5, 6, 7}(x_4, x_5, x_6, x_7) If the variables are discrete, we can represent the potential or energy functions as tables of (non-negative) numbers p(A, B, C, D) = \\frac{1}{Z} \\psi_{a, b}(A, B) \\psi_{b, c}(B, C) \\psi_{c, d}(C, D) \\psi_{a, d}(A, D) p(A, B, C, D) = \\frac{1}{Z} \\psi_{a, b}(A, B) \\psi_{b, c}(B, C) \\psi_{c, d}(C, D) \\psi_{a, d}(A, D) Error Why the switch from \\psi \\psi to \\phi \\phi here? It is important to note that these potential are not probabilities, but represent compatibilities between the different assignments.","title":"Parameterization of an UGM"},{"location":"lectures/week_4/#factor-product","text":"Given 3 disjoint sets of variables X, Y, Z X, Y, Z and factors \\psi_1(X, Y) \\psi_1(X, Y) , \\psi_2(Y, Z) \\psi_2(Y, Z) the factor product is defined as: \\psi_{X, Y, Z}(X, Y, Z) = \\psi_{X, Y}(X, Y)\\phi_{Y, Z}(Y, Z) \\psi_{X, Y, Z}(X, Y, Z) = \\psi_{X, Y}(X, Y)\\phi_{Y, Z}(Y, Z) Error Again, is the the switch from \\psi \\psi to \\phi \\phi a typo? Deliberate?","title":"Factor product"},{"location":"tutorials/week_1/","text":"","title":"Week 1"}]}