{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning Course Website Course Information Sheet When and Where Lectures: Tuesdays 15:00-17:00 in SS 2117 Tutorials: Thursday 13:00-14:00 in SS 2117","title":"About"},{"location":"#csc4122506-winter-2019-probabilistic-learning-and-reasoning","text":"Course Website Course Information Sheet","title":"CSC412/2506 Winter 2019: Probabilistic Learning and Reasoning"},{"location":"#when-and-where","text":"Lectures: Tuesdays 15:00-17:00 in SS 2117 Tutorials: Thursday 13:00-14:00 in SS 2117","title":"When and Where"},{"location":"lectures/week_1/","text":"Week 1: Introduction Assigned Reading Murphy: Chapters 1 and 2 Chapter 2 of David Mackay's textbook Overview Course information Overview of ML with examples Ungraded, anonymous background quiz Ungraded, anonymous background quiz Thursday (tutorial): Basics of ML vocabulary (cross-validation, objective functions, overfitting, regularization) and basics of probability manipulation. Textbook and Resources There is no required textbook, but optional reading will be assigned each week Kevin Murphy (2012), Machine Learning: A Probabilistic Perspective . David MacKay (2003) Information Theory, Inference, and Learning Algorithms . Stats vs Machine Learning Statisticians look at the data, consider the problem, and design a model we can understand. They Analyze methods to give guarantees Want to make few assumptions In machine learning , We only care about making good predictions! The basic idea is to learn a general procedure that works for lots of datasets. Often, there is no way around making assumptions, so we make our model large enough to hopefully learn something close to the truth. We can't use bounds in practice, so we evaluate and empirically choose model details. Sometimes, we end up with interpretable models anyways! Types of Learning Unsupervised Learning: Given unlabeled data instances x_1 x_1 , x_2 x_2 , x_3 x_3 ... build a statistical model of x x , which can be used for making predictions, decisions. Supervised Learning: Given input-output pairs (x,y) (x,y) the goal is to predict correct output given a new input. Semi-supervised Learning: We are given only a limited amount of (x, y) (x, y) pairs, but lots of unlabeled x x 's Active learning and RL: Also get to choose actions that influence (x, y) (x, y) pairs, but lots of unlabeled x x \u2019s. future information + reward. Can just use basic decision theory. All just special cases of estimating distributions from data: p(y | x) p(y | x) , p(x) p(x) , p(x, y) p(x, y) . Finding Structure in Data Page 9. Not a lot of information, fill this in after attending the lecture. Matrix Factorization Page 10-11. Again, not a lot of information on this slide. Blog + tutorial on matrix factorization for movie recommendation. Multiple Kinds of Data in One Model","title":"Week 1"},{"location":"lectures/week_1/#week-1-introduction","text":"","title":"Week 1: Introduction"},{"location":"lectures/week_1/#assigned-reading","text":"Murphy: Chapters 1 and 2 Chapter 2 of David Mackay's textbook","title":"Assigned Reading"},{"location":"lectures/week_1/#overview","text":"Course information Overview of ML with examples Ungraded, anonymous background quiz Ungraded, anonymous background quiz Thursday (tutorial): Basics of ML vocabulary (cross-validation, objective functions, overfitting, regularization) and basics of probability manipulation.","title":"Overview"},{"location":"lectures/week_1/#textbook-and-resources","text":"There is no required textbook, but optional reading will be assigned each week Kevin Murphy (2012), Machine Learning: A Probabilistic Perspective . David MacKay (2003) Information Theory, Inference, and Learning Algorithms .","title":"Textbook and Resources"},{"location":"lectures/week_1/#stats-vs-machine-learning","text":"Statisticians look at the data, consider the problem, and design a model we can understand. They Analyze methods to give guarantees Want to make few assumptions In machine learning , We only care about making good predictions! The basic idea is to learn a general procedure that works for lots of datasets. Often, there is no way around making assumptions, so we make our model large enough to hopefully learn something close to the truth. We can't use bounds in practice, so we evaluate and empirically choose model details. Sometimes, we end up with interpretable models anyways!","title":"Stats vs Machine Learning"},{"location":"lectures/week_1/#types-of-learning","text":"Unsupervised Learning: Given unlabeled data instances x_1 x_1 , x_2 x_2 , x_3 x_3 ... build a statistical model of x x , which can be used for making predictions, decisions. Supervised Learning: Given input-output pairs (x,y) (x,y) the goal is to predict correct output given a new input. Semi-supervised Learning: We are given only a limited amount of (x, y) (x, y) pairs, but lots of unlabeled x x 's Active learning and RL: Also get to choose actions that influence (x, y) (x, y) pairs, but lots of unlabeled x x \u2019s. future information + reward. Can just use basic decision theory. All just special cases of estimating distributions from data: p(y | x) p(y | x) , p(x) p(x) , p(x, y) p(x, y) .","title":"Types of Learning"},{"location":"lectures/week_1/#finding-structure-in-data","text":"Page 9. Not a lot of information, fill this in after attending the lecture.","title":"Finding Structure in Data"},{"location":"lectures/week_1/#matrix-factorization","text":"Page 10-11. Again, not a lot of information on this slide. Blog + tutorial on matrix factorization for movie recommendation.","title":"Matrix Factorization"},{"location":"lectures/week_1/#multiple-kinds-of-data-in-one-model","text":"","title":"Multiple Kinds of Data in One Model"},{"location":"tutorials/week_1/","text":"","title":"Week 1"}]}